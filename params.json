{
  "name": "ParallelRNN",
  "tagline": "Speeding up Recurrent Neural Networks",
  "body": "# Team Members\r\n\r\nEsha Uboweja, Bojian Han\r\n\r\n# Summary\r\n\r\nWe are going to implement a fast, parallel version of Recurrent Neural Networks (RNNs).\r\nWe will optimize RNN training performance on multi-core CPUs.\r\n\r\n# Background\r\n\r\nRNNs are neural networks that make use of memory to store certain pieces of information, so that outputs at a time-step t are dependent on inputs at time-step t, as well as inputs at time-steps t-1, t-2, etc. While the duration of this \"memory\" is short, i.e. output at t is dependent only up to T steps back in time, such recurrent dependency pattern requires the network to be iterative in at least 2 ways. Firstly, as in many neural networks, an epoch of training can happen only after a previous epoch of training is completely. This iterative dependency is most likely unavoidable. Further, training the RNN's hidden units for each time-step happens iteratively, so that the update to the hidden unit at time t depends on the update to the hidden unit at time (t-1). This is the core idea implemented as \"Back-propagation Through Time\" or BPTT for RNNs.\r\n\r\nFigure 1: An RNN architecture\r\n![RNN](images/rnn.jpg)\r\n\r\nAn example of an RNN architecture is shown in Figure 1 (from (1)). As described in (1), the network is described as follows:\r\n\r\n* For a given input x, hidden unit set s, the output unit set is o. U, V and W represent the weight matrices for weights from input to hidden units, hidden units to output units and hidden to hidden units respectively.\r\n* We \"unroll\" the RNN to better understand its representation over time. Note that the weight matrices for each connection type are the same for each input, hidden and output units triplet. \r\n* _x<sub>t</sub>_ is the input at time-step t, _s<sub>t</sub>_ is the hidden unit at time-step t, _o<sub>t</sub>_ is the output at time-step t\r\n* _s<sub>t</sub> = f(Ux<sub>t</sub> + Ws<sub>t-1</sub>)_ where _f_ is a non-linear function such as _tanh_ or the sigmoid function.\r\n* _o<sub>t</sub> = softmax(Vs<sub>t</sub>)_\r\n* Note that even though in this figure the number of outputs is equal to the number of inputs, in general this may not be the case.\r\n\r\n# Challenges\r\n\r\n* As discussed in the Background section, there are a lot of iterative dependencies in RNN, which makes it challenging to implement them in parallel\r\n* Further, for different hidden units, there may be different amounts of work because a hidden unit at time-step t has to iteratively use the computation of the hidden unit at time-step (t-1), and so on.\r\n* On CPUs, an interesting problem happens when the training data for 1 epoch is too large to fit in cache. Training many epochs iteratively in such a case will be slow, and a speed-up is highly desired for this simple issue as well.\r\n* The activation functions such as _sigmoid_ and _tanh_ are non-linear, so the hidden unit computations are non-linear as well. This makes the problem of parallelizing the hidden unit updates even more challenging.\r\n\r\nA fast parallel implementation of RNN should use training data of arbitrary sizes efficiently and also try to optimize over dependencies of the gradient descent updates to the 3 weight matrices for multiple hidden units.\r\n\r\nWe believe one way to solve this problem is to implement a data-parallel RNN training scheme, that batches up sets of data (or the entire set of data) and feeds it directly to the network for training. We are also investing some other approaches, such as those described in (2).\r\n\r\n# Goals/Deliverables\r\n\r\n* Plan: \r\n  - Successfully implement a fast, parallel implementation of RNN on multi-core CPUs, using one of OpenMP, ISPC and Halide.\r\n  - Present results and benchmarks on our parallel RNN framework's correctness and speed as compared to a sequential version on a few datasets (either language based or image based).\r\n\r\n* Hope:\r\n - Implement a parallel RNN framework in more than one of Halide, ISPC or OpenMP and compare and analyze these implementations.\r\n\r\n# Resources\r\n\r\nWe will write code in C++ (or perhaps Halide). C++ is our first choice because we have used it throughout the semester for class assignments and we know our code doesn't require any additional setup on gates machines or on the \"latedays\" machine cluster. We are new to using Halide, which is a Domain Specific Language (DSL) for image processing. In Halide, the matrix computations could be faster, but we will finalize our choice of language in our first project week.\r\n\r\nWe are programming a CPU-based implementation because while GPUs are very common, a multi-core CPU is a more common platform, and perhaps even cheaper. If time permits, we may look into a GPU based implementation (and modify our \"Hope to achieve\" set of goals!).\r\n\r\n# Schedule\r\n\r\n* April 02 - April 08: Implement serial RNN from scratch in C++\r\n* April 09 - April 15: Implement a parallel RNN in C++\r\n* April 16 - April 22: Train RNN on some small datasets AND parameter tuning for parallel RNN implementation\r\n* April 23 - April 29: Implement clever data decomposition and training schemes for large datasets\r\n* April 30 - May 6: Finalize implementation + benchmarking AND report\r\n* May 6 - May 9: Work on presentation\r\n\r\n# References\r\n1. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\r\n2. [Accelerating Neural Network training via two-stage classes and parallelization](http://research.microsoft.com/pubs/249888/Accelerating%20RNN%20training.pdf)",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}