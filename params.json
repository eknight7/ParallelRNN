{
  "name": "ParallelRNN",
  "tagline": "Speeding up Recurrent Neural Networks",
  "body": "### Team Members \r\nEsha Uboweja, Bojian Han\r\n\r\n# Final Report\r\n\r\n![Parallel RNN Training on multi-core CPU](parallelRNNReport.pdf)\r\n\r\n\r\n# Project Update\r\n\r\nIn our project, we implemented a parallel RNN that trains using backpropagation through time (BPTT) in Halide and benchmarked our Halide implementation against our implementation of Theano RNN and NumPy RNN which are highly optimized for the task.\r\n\r\nWe have made algorithmic improvements to the backpropagation algorithm shown in the ([WildML tutorial](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/))  which has an O(N^2) algorithm to implement BPTT, w.r.t. The number of input, hidden and output nodes in the RNN. Our version uses an O(N) algorithm for BPTT and this runs much faster. We have also implemented an RNN in NumPy which produces numerically equivalent updates and results as the original Theano RNN implementation, with some variation in floating point accuracy. Our NumPy implementation achieved over 2x speedup for small inputs and is slightly  faster than Theano for larger inputs.\r\n\r\nIn Halide, we have utilized parallelism and scheduling techniques that we learnt in the first 3 assignments of the class as well as techniques for matrix multiplication and inspirations from Spark RDD lecture. This is demonstrated via vectorization of code using Halide scheduling optimizations, parallelization using multiple threads using the Halide scheduler, tiling for matrix operations, loop fusing via both of Halide JIT and Halide AOT pipelines, as well as  manual loop fusing. Additionally, we have also explored the usage of AOT to remove the excessive compilation time due to JIT for functions that are repeatedly compiled and reused.\r\n\r\nOur current Halide implementation scales well for large inputs, runs only at 50% of the speed of Numpy version for large inputs of 1024 by 1024 images. Take note however that this is a HUGE improvement from same Halide code without the optimizations we described above, as the naive serial Halide implementation is almost 100~300 times slower than the NumPy version. For example, 1 epoch of training the RNN takes 366 seconds in the naive Halide implementation, 8 seconds in our optimized Halide implementation and 5 seconds in our NumPy implementation. \r\n\r\n---\r\n\r\n# Project Checkpoint\r\n\r\n## Needs attention\r\n\r\nWe need the following setup on \"latedays\" machines so that we can test our implementation on the Xeon Phi:\r\n* Theano for Python\r\n* Halide (requires a new LLVM version)\r\n\r\n## Progress\r\n\r\n1. We have code for training RNNs using Theano and Python with Numpy.\r\n\r\n2. We have a better understanding of the Backpropagation Through Time (BPTT) for RNNs, and how some parts of it can depend\r\non our choice of the non-linear activation. Our Python code presently uses `tanh`, a popular set of choices is [`tanh`, `sigmoid`, `ReLU`]. \r\n\r\n3. We have gone through Halide tutorials and have decided to code our RNN implementation using Halide. We decided this\r\nbecause of the efficiency of matrix operations in Halide. Another reason is that while we can use OpenBLAS or similar BLAS\r\nlibraries in C++ for efficient matrix algebra, we don't have to reimplement our RNN if we want to test it on GPU.\r\n\r\n## Evaluation Plan\r\n\r\n1. We will compare our parallel RNN implementation speedup against a Theano implementation. We are familiar with Theano\r\nand we know Theano compiles optimized code for different platforms. Our implementation will be benchmarked against similar performance in Theano with the exact same network structure (this means that if our network does not use bias, the Theano one will not use bias either). When our implementation uses mini-batch data-parallelism for training the network, the Theano implementation will use mini-batch training using Matrix-Matrix multiplication (Weights Matrices and Batches of Training Vectors concatenated into a matrix) and Theano's API for calculating gradients.\r\n\r\n2. It is a fact that simple, 1 hidden layer RNNs are not widely used at this time because of more interesting networks like multi-layer RNNs and LSTMs. This implies a difficulty in finding good problems and datasets to test our implementation. However, our goal is to parallelize and efficiently train a 1 hidden layer RNN. We therefore have come up with 2 types of problems for testing our RNNs:\r\n\r\n   i. Learning simple convolutional filters - RNNs are designed to learn about temporal dependencies for a given data sequence of values at different time-steps. We can treat images as sequences of pixels, so that a row of pixels `[0 0 100 200]` can be thought of as a sequence of pixels observed at time-steps `0,1,2,3`. This is similar to \"walking\" along the columns of an image from left to right. Treating images in this manner allows us to learn simple filters like a 1D Sobel filter `[-1 0 1]` for example (we have code in Theano for this). Further, data generation and scalability work into this problem well, because we can simply generate a lot of images or use a popular dataset like MNIST, and train and test our implementation.\r\n\r\n   ii. Dealing with image sequences, videos - We can setup an RNN to generate sequences of spatial differences between frames. Problem setup:\r\n     1. Input to the RNN model: A sequence of consecutive frames\r\n     2. Output of the RNN model: A sequence of temporal differences between frame at time t-1 and time t\r\n     3. Size of dataset: Unlimited since we can generate dataset from lots of images by applying transformations to it/Or we can get videos and train on these videos\r\n     4. Size of input/output: Depends on size of images which can vary\r\n     5. Final evaluation in terms of functionalities: This RNN model can be easily trained for small inputs. However, it is uncertain if the RNN can learn well for large images because the network might not be able to find the correct optima when the input is too large because the layers are fully connected. In order to evaluate this, we may need to check size of images from small (20x20) to large (200x200).\r\n     6. Final evaluation in terms of parallelism testing: Since our project is primarily focused on using Halide to schedule the RNN computation, it is important to test the scheduling against multiple parameters to identify the various machine factors involved such as caching, RAM or even disk memory access speed as input increases. \r\n\r\nWe aim to present graphs showing performance and speed up on these two applications compared to a baseline implementation in Theano, and various scheduled implementations in Halide, on a multi-core CPU. Since we have chosen some interesting image based problems, we will also try to visualize results obtained by testing the RNN v/s expected output where possible.\r\n\r\n## Revised Schedule\r\n\r\n* April 20 - April 23 \r\n    * Implement and test a \"serial\" RNN in Halide on Application 1 (Esha)\r\n    * Explore 3D matrix operations and their optimizations in Halide (Bojian)\r\n* April 26 - April 30\r\n    * Test \"serial\" RNN on Application 2 (Esha)\r\n    * Work with different scheduling algorithms and cache level optimizations for data-parallel RNNs (Esha + Bojian)\r\n* April 30 - May 6: \r\n    * Finalize benchmarks on Applications 1 and 2 (Esha + Bojian)\r\n    * Work on report (Esha + Bojian)\r\n    * Use this time to possibly do a 3rd application (either text/video based)\r\n* May 6 - May 9: Work on presentation (Esha + Bojian)\r\n\r\n\r\n\r\n---\r\n\r\n# Project Proposal\r\n\r\n## Team Members\r\nEsha Uboweja, Bojian Han\r\n\r\n## Summary\r\n\r\nWe are going to implement a fast, parallel version of Recurrent Neural Networks (RNNs).\r\nWe will optimize RNN training performance on multi-core CPUs.\r\n\r\n## Background\r\n\r\nRNNs are neural networks that make use of memory to store certain pieces of information, so that outputs at a time-step t are dependent on inputs at time-step t, as well as inputs at time-steps t-1, t-2, etc. While the duration of this \"memory\" is short, i.e. output at t is dependent only up to T steps back in time, such recurrent dependency pattern requires the network to be iterative in at least 2 ways. Firstly, as in many neural networks, an epoch of training can happen only after a previous epoch of training is completely. This iterative dependency is most likely unavoidable. Further, training the RNN's hidden units for each time-step happens iteratively, so that the update to the hidden unit at time t depends on the update to the hidden unit at time (t-1). This is the core idea implemented as \"Back-propagation Through Time\" or BPTT for RNNs.\r\n\r\nFigure 1: An RNN architecture\r\n![RNN](images/rnn.jpg)\r\n\r\nAn example of an RNN architecture is shown in Figure 1 (from (1)). As described in (1), the network is described as follows:\r\n\r\n* For a given input x, hidden unit set s, the output unit set is o. U, V and W represent the weight matrices for weights from input to hidden units, hidden units to output units and hidden to hidden units respectively.\r\n* We \"unroll\" the RNN to better understand its representation over time. Note that the weight matrices for each connection type are the same for each input, hidden and output units triplet. \r\n* _x<sub>t</sub>_ is the input at time-step t, _s<sub>t</sub>_ is the hidden unit at time-step t, _o<sub>t</sub>_ is the output at time-step t\r\n* _s<sub>t</sub> = f(Ux<sub>t</sub> + Ws<sub>t-1</sub>)_ where _f_ is a non-linear function such as _tanh_ or the sigmoid function.\r\n* _o<sub>t</sub> = softmax(Vs<sub>t</sub>)_\r\n* Note that even though in this figure the number of outputs is equal to the number of inputs, in general this may not be the case.\r\n\r\n## Challenges\r\n\r\n* As discussed in the Background section, there are a lot of iterative dependencies in RNN, which makes it challenging to implement them in parallel\r\n* Further, for different hidden units, there may be different amounts of work because a hidden unit at time-step t has to iteratively use the computation of the hidden unit at time-step (t-1), and so on.\r\n* On CPUs, an interesting problem happens when the training data for 1 epoch is too large to fit in cache. Training many epochs iteratively in such a case will be slow, and a speed-up is highly desired for this simple issue as well.\r\n* The activation functions such as _sigmoid_ and _tanh_ are non-linear, so the hidden unit computations are non-linear as well. This makes the problem of parallelizing the hidden unit updates even more challenging.\r\n\r\nA fast parallel implementation of RNN should use training data of arbitrary sizes efficiently and also try to optimize over dependencies of the gradient descent updates to the 3 weight matrices for multiple hidden units.\r\n\r\nWe believe one way to solve this problem is to implement a data-parallel RNN training scheme, that batches up sets of data (or the entire set of data) and feeds it directly to the network for training. We are also investing some other approaches, such as those described in (2).\r\n\r\n## Goals/Deliverables\r\n\r\n* Plan: \r\n  - Successfully implement a fast, parallel implementation of RNN on multi-core CPUs, using one of OpenMP, ISPC and Halide.\r\n  - Present results and benchmarks on our parallel RNN framework's correctness and speed as compared to a sequential version on a few datasets (either language based or image based).\r\n\r\n* Hope:\r\n - Implement a parallel RNN framework in more than one of Halide, ISPC or OpenMP and compare and analyze these implementations.\r\n\r\n## Resources\r\n\r\nWe will write code in C++ (or perhaps Halide). C++ is our first choice because we have used it throughout the semester for class assignments and we know our code doesn't require any additional setup on gates machines or on the \"latedays\" machine cluster. We are new to using Halide, which is a Domain Specific Language (DSL) for image processing. In Halide, the matrix computations could be faster, but we will finalize our choice of language in our first project week.\r\n\r\nWe are programming a CPU-based implementation because while GPUs are very common, a multi-core CPU is a more common platform, and perhaps even cheaper. If time permits, we may look into a GPU based implementation (and modify our \"Hope to achieve\" set of goals!).\r\n\r\n## Schedule\r\n\r\n* April 02 - April 08: Implement serial RNN from scratch in C++\r\n* April 09 - April 15: Implement a parallel RNN in C++\r\n* April 16 - April 22: Train RNN on some small datasets AND parameter tuning for parallel RNN implementation\r\n* April 23 - April 29: Implement clever data decomposition and training schemes for large datasets\r\n* April 30 - May 6: Finalize implementation + benchmarking AND report\r\n* May 6 - May 9: Work on presentation\r\n\r\n## References\r\n1. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\r\n2. [Accelerating Neural Network training via two-stage classes and parallelization](http://research.microsoft.com/pubs/249888/Accelerating%20RNN%20training.pdf)",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}