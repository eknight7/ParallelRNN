{
  "name": "ParallelRNN",
  "tagline": "Speeding up Recurrent Neural Networks",
  "body": "### Team Members Esha Uboweja, Bojian Han\r\n\r\n# Project Checkpoint\r\n\r\n## Needs attention\r\n\r\nWe need the following setup on \"latedays\" machines so that we can test our implementation on the Xeon Phi:\r\n* Theano for Python\r\n* Halide (requires a new LLVM version)\r\n\r\n## Progress\r\n\r\n1. We have code for training RNNs using Theano and Python with Numpy.\r\n2. We have a better understanding of the Backpropagation Through Time (BPTT) for RNNs, and how some parts of it can depend\r\non our choice of the non-linear activation. Our Python code presently uses `tanh`, a popular set of choices is [`tanh`, `sigmoid`, `ReLU`]. \r\n3. We have gone through Halide tutorials and have decided to code our RNN implementation using Halide. We decided this\r\nbecause of the efficiency of matrix operations in Halide. Another reason is that while we can use OpenBLAS or similar BLAS\r\nlibraries in C++ for efficient matrix algebra, we don't have to reimplement our RNN if we want to test it on GPU.\r\n\r\n## Evaluation Plan\r\n\r\n1. We will compare our parallel RNN implementation speedup against a Theano implementation. We are familiar with Theano\r\nand we know Theano compiles optimized code for different platforms.\r\n2. It is a fact that simple, 1 hidden layer RNNs are not widely used at this time because of more interesting networks like multi-layer RNNs and LSTMs. This implies a difficulty in finding good problems and datasets to test our implementation. However, our goal is to parallelize and efficiently train a 1 hidden layer RNN. We therefore have come up with 2 types of problems for testing our RNNs:\r\n   i. Learning simple convolutional filters - RNNs are designed to learn about temporal dependencies for a given data sequence of values at different time-steps. We can treat images as sequences of pixels, so that a row of pixels `[0 0 100 200]` can be thought of as a sequence of pixels observed at time-steps 0,1,2,3. This is similar to \"walking\" along the columns of an image from left to right. Treating images in this manner allows us to learn simple filters like a 1D Sobel filter `[-1 0 1]` for example (we have code in Theano for this). Further, data generation and scalability work into this problem well, because we can simply generate a lot of images or use a popular dataset like MNIST, and train and test our implementation.\r\n   ii. Dealing with image sequences, videos - We can setup an RNN to generate sequences of spatial differences between frames. Problem setup:\r\n        1. Input to the RNN model: A sequence of consecutive frames\r\n        2. Output of the RNN model: A sequence of temporal differences between frame at time t-1 and time t\r\n        3. Size of dataset: Unlimited since we can generate dataset from lots of images by applying transformations to it/Or we can get videos and train on these videos\r\n        4. Size of input/output: Depends on size of images which can vary\r\n        5. Final evaluation in terms of functionalities: This RNN model can be easily trained for small inputs. However, it is uncertain if the RNN can learn well for large images because the network might not be able to find the correct optima when the input is too large because the layers are fully connected. In order to evaluate this, we may need to check size of images from small (20x20) to large (200x200).\r\n        6. Final evaluation in terms of parallelism testing: Since our project is primarily focused on using Halide to schedule the RNN computation, it is important to test the scheduling against multiple parameters to identify the various machine factors involved such as caching, RAM or even disk memory access speed as input increases. \r\n\r\n\r\n\r\n\r\n## Revised Schedule\r\n\r\n\r\n---\r\n\r\n# Project Proposal\r\n\r\n## Team Members\r\nEsha Uboweja, Bojian Han\r\n\r\n## Summary\r\n\r\nWe are going to implement a fast, parallel version of Recurrent Neural Networks (RNNs).\r\nWe will optimize RNN training performance on multi-core CPUs.\r\n\r\n## Background\r\n\r\nRNNs are neural networks that make use of memory to store certain pieces of information, so that outputs at a time-step t are dependent on inputs at time-step t, as well as inputs at time-steps t-1, t-2, etc. While the duration of this \"memory\" is short, i.e. output at t is dependent only up to T steps back in time, such recurrent dependency pattern requires the network to be iterative in at least 2 ways. Firstly, as in many neural networks, an epoch of training can happen only after a previous epoch of training is completely. This iterative dependency is most likely unavoidable. Further, training the RNN's hidden units for each time-step happens iteratively, so that the update to the hidden unit at time t depends on the update to the hidden unit at time (t-1). This is the core idea implemented as \"Back-propagation Through Time\" or BPTT for RNNs.\r\n\r\nFigure 1: An RNN architecture\r\n![RNN](images/rnn.jpg)\r\n\r\nAn example of an RNN architecture is shown in Figure 1 (from (1)). As described in (1), the network is described as follows:\r\n\r\n* For a given input x, hidden unit set s, the output unit set is o. U, V and W represent the weight matrices for weights from input to hidden units, hidden units to output units and hidden to hidden units respectively.\r\n* We \"unroll\" the RNN to better understand its representation over time. Note that the weight matrices for each connection type are the same for each input, hidden and output units triplet. \r\n* _x<sub>t</sub>_ is the input at time-step t, _s<sub>t</sub>_ is the hidden unit at time-step t, _o<sub>t</sub>_ is the output at time-step t\r\n* _s<sub>t</sub> = f(Ux<sub>t</sub> + Ws<sub>t-1</sub>)_ where _f_ is a non-linear function such as _tanh_ or the sigmoid function.\r\n* _o<sub>t</sub> = softmax(Vs<sub>t</sub>)_\r\n* Note that even though in this figure the number of outputs is equal to the number of inputs, in general this may not be the case.\r\n\r\n## Challenges\r\n\r\n* As discussed in the Background section, there are a lot of iterative dependencies in RNN, which makes it challenging to implement them in parallel\r\n* Further, for different hidden units, there may be different amounts of work because a hidden unit at time-step t has to iteratively use the computation of the hidden unit at time-step (t-1), and so on.\r\n* On CPUs, an interesting problem happens when the training data for 1 epoch is too large to fit in cache. Training many epochs iteratively in such a case will be slow, and a speed-up is highly desired for this simple issue as well.\r\n* The activation functions such as _sigmoid_ and _tanh_ are non-linear, so the hidden unit computations are non-linear as well. This makes the problem of parallelizing the hidden unit updates even more challenging.\r\n\r\nA fast parallel implementation of RNN should use training data of arbitrary sizes efficiently and also try to optimize over dependencies of the gradient descent updates to the 3 weight matrices for multiple hidden units.\r\n\r\nWe believe one way to solve this problem is to implement a data-parallel RNN training scheme, that batches up sets of data (or the entire set of data) and feeds it directly to the network for training. We are also investing some other approaches, such as those described in (2).\r\n\r\n## Goals/Deliverables\r\n\r\n* Plan: \r\n  - Successfully implement a fast, parallel implementation of RNN on multi-core CPUs, using one of OpenMP, ISPC and Halide.\r\n  - Present results and benchmarks on our parallel RNN framework's correctness and speed as compared to a sequential version on a few datasets (either language based or image based).\r\n\r\n* Hope:\r\n - Implement a parallel RNN framework in more than one of Halide, ISPC or OpenMP and compare and analyze these implementations.\r\n\r\n## Resources\r\n\r\nWe will write code in C++ (or perhaps Halide). C++ is our first choice because we have used it throughout the semester for class assignments and we know our code doesn't require any additional setup on gates machines or on the \"latedays\" machine cluster. We are new to using Halide, which is a Domain Specific Language (DSL) for image processing. In Halide, the matrix computations could be faster, but we will finalize our choice of language in our first project week.\r\n\r\nWe are programming a CPU-based implementation because while GPUs are very common, a multi-core CPU is a more common platform, and perhaps even cheaper. If time permits, we may look into a GPU based implementation (and modify our \"Hope to achieve\" set of goals!).\r\n\r\n## Schedule\r\n\r\n* April 02 - April 08: Implement serial RNN from scratch in C++\r\n* April 09 - April 15: Implement a parallel RNN in C++\r\n* April 16 - April 22: Train RNN on some small datasets AND parameter tuning for parallel RNN implementation\r\n* April 23 - April 29: Implement clever data decomposition and training schemes for large datasets\r\n* April 30 - May 6: Finalize implementation + benchmarking AND report\r\n* May 6 - May 9: Work on presentation\r\n\r\n## References\r\n1. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\r\n2. [Accelerating Neural Network training via two-stage classes and parallelization](http://research.microsoft.com/pubs/249888/Accelerating%20RNN%20training.pdf)",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}