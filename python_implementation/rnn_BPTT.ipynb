{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as TT\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients for RNN on 1 example at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients with simple functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of input units\n",
    "nin = 1\n",
    "# Number of hidden units\n",
    "nh = 4\n",
    "# Number of output units\n",
    "nout = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "th_W_xh = TT.matrix()\n",
    "th_W_hh = TT.matrix()\n",
    "th_W_hy = TT.matrix()\n",
    "\n",
    "th_x = TT.matrix()\n",
    "th_y = TT.matrix()\n",
    "th_h0 = TT.vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 #define _CUDA_NDARRAY_C\n",
      "2 \n",
      "3 #include <Python.h>\n",
      "4 #include <structmember.h>\n",
      "5 #include \"theano_mod_helper.h\"\n",
      "6 \n",
      "7 #include <numpy/arrayobject.h>\n",
      "8 #include <iostream>\n",
      "9 \n",
      "10 #include \"cuda_ndarray.cuh\"\n",
      "11 \n",
      "12 #ifndef CNMEM_DLLEXPORT\n",
      "13 #define CNMEM_DLLEXPORT\n",
      "14 #endif\n",
      "15 \n",
      "16 #include \"cnmem.h\"\n",
      "17 #include \"cnmem.cpp\"\n",
      "18 \n",
      "19 //If true, when there is a gpu malloc or free error, we print the size of allocated memory on the device.\n",
      "20 #define COMPUTE_GPU_MEM_USED 0\n",
      "21 \n",
      "22 //If true, we fill with NAN allocated device memory.\n",
      "23 #define ALLOC_MEMSET 0\n",
      "24 \n",
      "25 //If true, we print out when we free a device pointer, uninitialize a\n",
      "26 //CudaNdarray, or allocate a device pointer\n",
      "27 #define PRINT_FREE_MALLOC 0\n",
      "28 \n",
      "29 //If true, we do error checking at the start of functions, to make sure there\n",
      "30 //is not a pre-existing error when the function is called.\n",
      "31 //You probably need to set the environment variable\n",
      "32 //CUDA_LAUNCH_BLOCKING=1, and/or modify the CNDA_THREAD_SYNC\n",
      "33 //preprocessor macro in cuda_ndarray.cuh\n",
      "34 //if you want this to work.\n",
      "35 #define PRECHECK_ERROR 0\n",
      "36 \n",
      "37 cublasHandle_t handle = NULL;\n",
      "38 int* err_var = NULL;\n",
      "39 \n",
      "40 /////////////////////////\n",
      "41 // Alloc and Free\n",
      "42 /////////////////////////\n",
      "43 \n",
      "44 static int g_gpu_context_active = 0;\n",
      "45 \n",
      "46 \n",
      "47 PyObject *\n",
      "48 CudaNdarray_Dimshuffle(PyObject* _unused, PyObject* args);\n",
      "49 static PyObject *CudaNdarray_get_shape(CudaNdarray *self, void *closure);\n",
      "50 \n",
      "51 \n",
      "52 /**\n",
      "53  *\n",
      "54  * In the test program I'm using, the _outstanding_mallocs decreases with every call.\n",
      "55  * This suggests there are more free() calls being made than alloc(), but I can't figure out why.\n",
      "56  *\n",
      "57  */\n",
      "58 int _outstanding_mallocs[] = {0,0};\n",
      "59 \n",
      "60 #if COMPUTE_GPU_MEM_USED\n",
      "61 size_t _allocated_size = 0;\n",
      "62 size_t _max_allocated_size = 0;\n",
      "63 \n",
      "64 const int TABLE_SIZE = 10000;\n",
      "65 struct table_struct{\n",
      "66     void* ptr;\n",
      "67     size_t size;\n",
      "68 };\n",
      "69 table_struct _alloc_size_table[TABLE_SIZE];\n",
      "70 #endif\n",
      "71 \n",
      "72 void * device_malloc(size_t size)\n",
      "73 {\n",
      "74     return device_malloc(size, VERBOSE_DEVICE_MALLOC);\n",
      "75 }\n",
      "76 \n",
      "77 ///@TODO: thejaswi: link this option to a theano config variable?\n",
      "78 static bool g_use_cnmem = false;\n",
      "79 static const int g_max_devices = 8;\n",
      "80 int initCnmem(int card_number_provided, int card_nb, size_t mem) {\n",
      "81     static bool cnmemInitialized = false;\n",
      "82     if(cnmemInitialized) {\n",
      "83         return 0;\n",
      "84     }\n",
      "85     // On stderr to be at the same place as \"Using gpu device...\"\n",
      "86     int numDevices = 0;\n",
      "87     cnmemDevice_t devices[g_max_devices];\n",
      "88     if(cudaGetDeviceCount(&numDevices) != cudaSuccess) {\n",
      "89         PyErr_Format(PyExc_RuntimeError,\n",
      "90                      \"initCnmem: 'cudaGetDeviceCount' failed! Reason=%s\\n\",\n",
      "91                      cudaGetErrorString(cudaGetLastError()));\n",
      "92         return -1;\n",
      "93     }\n",
      "94     if(card_number_provided){\n",
      "95         numDevices = 1;\n",
      "96         int i = 0;\n",
      "97         devices[i].device = card_nb;\n",
      "98         devices[i].size = mem;\n",
      "99         ///@TODO: thejaswi: add support for multiple streams\n",
      "100         devices[i].numStreams = 0;\n",
      "101         devices[i].streams = NULL;\n",
      "102         devices[i].streamSizes = NULL;\n",
      "103     }else{\n",
      "104         for(int i=0;i<numDevices;++i) {\n",
      "105             devices[i].device = i;\n",
      "106             devices[i].size = mem;\n",
      "107             ///@TODO: thejaswi: add support for multiple streams\n",
      "108             devices[i].numStreams = 0;\n",
      "109             devices[i].streams = NULL;\n",
      "110         }\n",
      "111     }\n",
      "112 \n",
      "113     ///@TODO: thejaswi: passing custom cnmem flags?\n",
      "114     cnmemStatus_t status = cnmemInit(numDevices, devices, CNMEM_FLAGS_DEFAULT);\n",
      "115     if(status != CNMEM_STATUS_SUCCESS) {\n",
      "116         PyErr_Format(PyExc_RuntimeError,\n",
      "117                      \"initCnmem: cnmemInit call failed! Reason=%s. numdev=%d\\n\",\n",
      "118                      cnmemGetErrorString(status), numDevices);\n",
      "119         return -1;\n",
      "120     }\n",
      "121     cnmemInitialized = true;\n",
      "122     return 0;\n",
      "123 }\n",
      "124 \n",
      "125 void * device_malloc(size_t size, int verbose)\n",
      "126 {\n",
      "127     #if PRECHECK_ERROR\n",
      "128         cudaThreadSynchronize();\n",
      "129         cudaError_t prevError = cudaGetLastError();\n",
      "130         if (cudaSuccess != prevError)\n",
      "131         {\n",
      "132             fprintf(stderr,\n",
      "133                     \"Error existed before calling device_malloc. %s\\n\",\n",
      "134                     cudaGetErrorString(prevError)\n",
      "135                     );\n",
      "136         }\n",
      "137     #endif\n",
      "138     void * rval=NULL;\n",
      "139     ///@TODO: thejaswi: support for multiple-streams?\n",
      "140     if(g_use_cnmem) {\n",
      "141         cnmemStatus_t status = CNMEM_STATUS_SUCCESS;\n",
      "142         status = cnmemMalloc(&rval, size, NULL);\n",
      "143         if(status != CNMEM_STATUS_SUCCESS) {\n",
      "144             PyErr_Format(PyExc_MemoryError,\n",
      "145                          \"Error allocating %llu bytes of device memory (%s).\",\n",
      "146                          (unsigned long long)size, cnmemGetErrorString(status));\n",
      "147             return NULL;\n",
      "148         }\n",
      "149     }\n",
      "150     else {\n",
      "151         cudaError_t err = cudaMalloc(&rval, size);\n",
      "152         if (cudaSuccess != err)\n",
      "153         {\n",
      "154             // Clear the error flag, cudaMalloc doesn't do it.\n",
      "155             // Currently this returns the same thing as err, but if in future\n",
      "156             // it returns something else I still don't see why we should ignore\n",
      "157             // it.  All we want to do here is reset the flag.\n",
      "158             cudaGetLastError();\n",
      "159             if (verbose)\n",
      "160             {\n",
      "161                 size_t free = 0, total = 0;\n",
      "162                 cudaError_t err2 = cudaMemGetInfo(&free, &total);\n",
      "163                 if (err2 != cudaSuccess){\n",
      "164                     cudaGetLastError();\n",
      "165                     fprintf(stderr,\n",
      "166                             \"Error when trying to find the memory information\"\n",
      "167                             \" on the GPU: %s\\n\", cudaGetErrorString(err2));\n",
      "168                 }\n",
      "169                 #if COMPUTE_GPU_MEM_USED\n",
      "170                     fprintf(stderr,\n",
      "171                             \"Error allocating %llu bytes of device memory (%s).\"\n",
      "172                             \" new total bytes allocated: %llu.\"\n",
      "173                             \" Driver report %llu bytes free and %llu bytes total \\n\",\n",
      "174                             (unsigned long long)size, cudaGetErrorString(err), (unsigned long long)_allocated_size,\n",
      "175                             (unsigned long long)free, (unsigned long long)total);\n",
      "176                 #else\n",
      "177                     fprintf(stderr,\n",
      "178                             \"Error allocating %llu bytes of device memory (%s).\"\n",
      "179                             \" Driver report %llu bytes free and %llu bytes total \\n\",\n",
      "180                             (unsigned long long)size, cudaGetErrorString(err), (unsigned long long)free, (unsigned long long)total);\n",
      "181                 #endif\n",
      "182             }\n",
      "183             PyErr_Format(PyExc_MemoryError,\n",
      "184                          \"Error allocating %llu bytes of device memory (%s).\",\n",
      "185                          (unsigned long long)size, cudaGetErrorString(err));\n",
      "186             return NULL;\n",
      "187         }\n",
      "188     }\n",
      "189     if (rval != NULL){\n",
      "190         // Can it happen that cudaMalloc return cudaSuccess, but return a NULL ptr?\n",
      "191         // Could this be what happen if size is 0?\n",
      "192         _outstanding_mallocs[0] += 1;\n",
      "193 \n",
      "194 #if COMPUTE_GPU_MEM_USED\n",
      "195         _allocated_size += size;\n",
      "196         _max_allocated_size = std::max(_max_allocated_size, _allocated_size);\n",
      "197         int i = 0;\n",
      "198         for(;i<TABLE_SIZE;i++){\n",
      "199             if(NULL==_alloc_size_table[i].ptr){\n",
      "200                 _alloc_size_table[i].ptr=rval;\n",
      "201                 _alloc_size_table[i].size=size;\n",
      "202                 break;\n",
      "203             }\n",
      "204         }\n",
      "205         if (i == TABLE_SIZE){\n",
      "206             fprintf(stderr,\n",
      "207                     \"When tracking GPU malloc, our table size wasn't big enough.\"\n",
      "208                     \" So we loose some tracking. Raise the value of TABLE_SIZE in the file cuda_ndarra.cu\");\n",
      "209         }\n",
      "210 #endif\n",
      "211     }\n",
      "212     //fprintf(stderr,\n",
      "213     //\"allocated %li bytes of device memory (%s). new total bytes allocated: %d. ptr: %p\\n\",\n",
      "214     //(long)size, cudaGetErrorString(err),_allocated_size,rval);\n",
      "215 \n",
      "216     if(ALLOC_MEMSET){\n",
      "217         //We init them to nan to make sure we catch more debug case.\n",
      "218         cudaMemset(rval, 0xFF, size);\n",
      "219         //printf(\"MEMSET\\n\");\n",
      "220     }\n",
      "221     #if PRINT_FREE_MALLOC\n",
      "222         fprintf(stderr, \"device malloc %p of size %d\\n\", rval, size);\n",
      "223     #endif\n",
      "224     return rval;\n",
      "225 }\n",
      "226 \n",
      "227 int device_free(void *ptr)\n",
      "228 {\n",
      "229     #if PRECHECK_ERROR\n",
      "230         cudaThreadSynchronize();\n",
      "231         cudaError_t prevError = cudaGetLastError();\n",
      "232         if (cudaSuccess != prevError)\n",
      "233         {\n",
      "234             fprintf(stderr,\n",
      "235                     \"Error existed before calling device_free. %s\\n\",\n",
      "236                     cudaGetErrorString(prevError)\n",
      "237                     );\n",
      "238         }\n",
      "239     #endif\n",
      "240     #if PRINT_FREE_MALLOC\n",
      "241         size_t free = 0, total = 0;\n",
      "242         cudaError_t err2 = cudaMemGetInfo(&free, &total);\n",
      "243         if (err2 != cudaSuccess){\n",
      "244             cudaGetLastError();\n",
      "245             fprintf(stderr,\n",
      "246                     \"Error when tring to find the memory information\"\n",
      "247                     \" on the GPU: %s\\n\", cudaGetErrorString(err2));\n",
      "248         }\n",
      "249         #if COMPUTE_GPU_MEM_USED\n",
      "250         {\n",
      "251             int i = 0;\n",
      "252             for(;i<TABLE_SIZE;i++)\n",
      "253                 if(_alloc_size_table[i].ptr==ptr){\n",
      "254                     break;\n",
      "255                 }\n",
      "256             assert(i<TABLE_SIZE);\n",
      "257             fprintf(stderr, \"device_free %p of size %d.\"\n",
      "258                     \" Driver report %d bytes free and %d bytes total \\n\",\n",
      "259                     ptr, _alloc_size_table[i].size, free, total);\n",
      "260         }\n",
      "261         #else\n",
      "262             fprintf(stderr, \"device_free %p.\"\n",
      "263                     \" Driver report %d bytes free and %d bytes total \\n\",\n",
      "264                     ptr, free, total);\n",
      "265         #endif\n",
      "266     #endif\n",
      "267 \n",
      "268     // if there is no gpu context, the call to cudaFree will fail; skip it entirely\n",
      "269     if(!g_gpu_context_active) {\n",
      "270         return 0;\n",
      "271     }\n",
      "272 \n",
      "273     ///@TODO: thejaswi: multi-stream support\n",
      "274     if(g_use_cnmem) {\n",
      "275         cnmemStatus_t status = cnmemFree(ptr, NULL);\n",
      "276         if(status != CNMEM_STATUS_SUCCESS) {\n",
      "277             fprintf(stderr, \"device_free: cnmemFree call failed! Reason=%s\\n\",\n",
      "278                     cnmemGetErrorString(status));\n",
      "279         }\n",
      "280     }\n",
      "281     else {\n",
      "282         // We need sync as the Theano's GC could remove intermediate variable that\n",
      "283         // are still needed as the gpu kernel are running or in the queue.\n",
      "284         CNDA_BEGIN_ALLOW_THREADS\n",
      "285         cudaThreadSynchronize();\n",
      "286         CNDA_END_ALLOW_THREADS\n",
      "287 \n",
      "288         cudaError_t err =  cudaFree(ptr);\n",
      "289         if (cudaSuccess != err)\n",
      "290         {\n",
      "291             // Clear the error flag, cudaFree doesn't do it.\n",
      "292             // Currently this returns the same thing as err, but if in future\n",
      "293             // it returns something else I still don't see why we should ignore\n",
      "294             // it.  All we want to do here is reset the flag.\n",
      "295             cudaGetLastError();\n",
      "296             size_t free = 0, total = 0;\n",
      "297             cudaError_t err2 = cudaMemGetInfo(&free, &total);\n",
      "298             if (err2 != cudaSuccess){\n",
      "299                 cudaGetLastError();\n",
      "300                 fprintf(stderr,\n",
      "301                         \"Error when tring to find the memory information\"\n",
      "302                         \" on the GPU: %s\\n\", cudaGetErrorString(err2));\n",
      "303             }\n",
      "304             #if COMPUTE_GPU_MEM_USED\n",
      "305             {\n",
      "306                 int i = 0;\n",
      "307                 for(;i<TABLE_SIZE;i++)\n",
      "308                     if(_alloc_size_table[i].ptr==ptr){\n",
      "309                         break;\n",
      "310                     }\n",
      "311                 assert(i<TABLE_SIZE);\n",
      "312                 fprintf(stderr,\n",
      "313                         \"Error freeing device pointer %p (%s) of size %llu. %llu byte already allocated.\"\n",
      "314                         \" Driver report %llu bytes free and %llu bytes total \\n\",\n",
      "315                         ptr, cudaGetErrorString(err),\n",
      "316                         (unsigned long long)_alloc_size_table[i].size, (unsigned long long)_allocated_size, (unsigned long long)free, (unsigned long long)total);\n",
      "317             }\n",
      "318             #else\n",
      "319                 fprintf(stderr,\n",
      "320                         \"Error freeing device pointer %p (%s).\"\n",
      "321                         \" Driver report %llu bytes free and %llu bytes total \\n\",\n",
      "322                         ptr,\n",
      "323                         cudaGetErrorString(err), (unsigned long long)free, (unsigned long long)total);\n",
      "324             #endif\n",
      "325             if (NULL != PyErr_Occurred()){\n",
      "326                 fprintf(stderr,\n",
      "327                         \"device_free: cudaFree() returned an error, but there is already an\"\n",
      "328                         \" Python error set. This happen during the clean up when there is a\"\n",
      "329                         \" first error and the CUDA driver is in a so bad state that it don't\"\n",
      "330                         \" work anymore. We keep the previous error set to help debugging it.\");\n",
      "331                 return -1;\n",
      "332             }\n",
      "333             PyErr_Format(PyExc_MemoryError,\n",
      "334                     \"error freeing device pointer %p (%s)\",\n",
      "335                     ptr,\n",
      "336                     cudaGetErrorString(err));\n",
      "337             return -1;\n",
      "338         }\n",
      "339     }\n",
      "340     _outstanding_mallocs[0] -= (ptr != NULL);\n",
      "341     #if COMPUTE_GPU_MEM_USED\n",
      "342         int i=0;\n",
      "343         size_t total_freed = 0;\n",
      "344         for(;i<TABLE_SIZE;i++)\n",
      "345             if(_alloc_size_table[i].ptr==ptr){\n",
      "346                 _allocated_size -= _alloc_size_table[i].size;\n",
      "347                 total_freed += _alloc_size_table[i].size;\n",
      "348                 _alloc_size_table[i].ptr=0;\n",
      "349                 _alloc_size_table[i].size=0;\n",
      "350 \n",
      "351                 break;\n",
      "352             }\n",
      "353         //if(i==TABLE_SIZE)\n",
      "354         //    printf(\"Unallocated unknow size!\\n\");\n",
      "355         //fprintf(stderr, \"freed %li bytes of device memory (%s). %d already allocated, ptr=%p\\n\", (long)total_freed, cudaGetErrorString(err),_allocated_size,ptr);\n",
      "356     #endif\n",
      "357     return 0;\n",
      "358 }\n",
      "359 \n",
      "360 static PyObject *\n",
      "361 outstanding_mallocs(PyObject* self, PyObject * args)\n",
      "362 {\n",
      "363     return PyInt_FromLong(_outstanding_mallocs[0]);\n",
      "364 }\n",
      "365 \n",
      "366 \n",
      "367 static void *work_mem = NULL;\n",
      "368 static size_t work_size = 0;\n",
      "369 \n",
      "370 /*\n",
      "371  * Returns a chunk of memory for temporary work inside of an op. You can only\n",
      "372  * request a single chunk of memory at a time since it is reused.\n",
      "373  */\n",
      "374 void *get_work_mem(size_t sz) {\n",
      "375     if (sz <= work_size)\n",
      "376         return work_mem;\n",
      "377     device_free(work_mem);\n",
      "378     work_mem = device_malloc(sz);\n",
      "379     work_size = sz;\n",
      "380     if (work_mem == NULL)\n",
      "381         work_size = 0;\n",
      "382     return work_mem;\n",
      "383 }\n",
      "384 \n",
      "385 /////////////////////////\n",
      "386 // Static helper methods\n",
      "387 /////////////////////////\n",
      "388 \n",
      "389 static void\n",
      "390 CudaNdarray_null_init(CudaNdarray*self)\n",
      "391 {\n",
      "392     self->base = NULL;\n",
      "393     self->nd = -1;\n",
      "394     self->host_structure = NULL;\n",
      "395     self->data_allocated = 0;\n",
      "396     self->dev_structure_fresh = 1;\n",
      "397     self->dev_structure = NULL;\n",
      "398     self->devdata = NULL;\n",
      "399 }\n",
      "400 \n",
      "401 static int\n",
      "402 CudaNdarray_uninit(CudaNdarray*self)\n",
      "403 {\n",
      "404     #if PRINT_FREE_MALLOC\n",
      "405         fprintf(stderr, \"CudaNdarray_uninit %p\\n\", self);\n",
      "406     #endif\n",
      "407     int rval = 0;\n",
      "408     if (self->data_allocated) {\n",
      "409         assert(self->devdata);\n",
      "410         if (device_free(self->devdata))\n",
      "411         {\n",
      "412             fprintf(stderr,\n",
      "413                     \"CudaNdarray_uninit: error freeing self->devdata. (self=%p, self->devata=%p)\\n\",\n",
      "414                     self, self->devdata);\n",
      "415             rval = -1;\n",
      "416         }\n",
      "417         self->devdata = NULL;\n",
      "418         self->data_allocated = 0;\n",
      "419     }\n",
      "420     if (self->dev_structure)\n",
      "421     {\n",
      "422         if (device_free(self->dev_structure))\n",
      "423         {\n",
      "424             fprintf(stderr,\n",
      "425                     \"CudaNdarray_uninit: error freeing dev_structure memory %p (self=%p)\\n\",\n",
      "426                     self->dev_structure, self);\n",
      "427             rval = -1;\n",
      "428         }\n",
      "429         self->dev_structure = NULL;\n",
      "430     }\n",
      "431     if (self->host_structure)\n",
      "432     {\n",
      "433         free(self->host_structure);\n",
      "434         self->host_structure = NULL;\n",
      "435     }\n",
      "436     self->nd = -1;\n",
      "437     Py_XDECREF(self->base);\n",
      "438     self->base = NULL;\n",
      "439     return rval;\n",
      "440 }\n",
      "441 \n",
      "442 \n",
      "443 //make the rightmost coords change fastest\n",
      "444 //TODO: why does a downward for-loop not work????\n",
      "445 //TODO: use the log2_dims and driver code to remove / and %\n",
      "446 //TODO: skip the last division (when d == 0)\n",
      "447 #define decl_k_elemwise_unary_rowmajor(name, F) \\\n",
      "448 __global__ void name (unsigned int numEls,  \\\n",
      "449         unsigned int nd, \\\n",
      "450         const int * dim,  \\\n",
      "451         const float * a_data, const int * a_str, \\\n",
      "452         float * z_data, const int * z_str) \\\n",
      "453 { \\\n",
      "454     const unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x; \\\n",
      "455     const unsigned int numThreads = blockDim.x * gridDim.x; \\\n",
      "456  \\\n",
      "457     for (unsigned int i = idx; i < numEls; i += numThreads) \\\n",
      "458     { \\\n",
      "459         unsigned int ii = i; \\\n",
      "460         const float * a_i = a_data; \\\n",
      "461         float * z_i = z_data; \\\n",
      "462         for (unsigned int _d = 0; _d < nd; ++_d) \\\n",
      "463         { \\\n",
      "464             unsigned int d = nd - _d-1;  \\\n",
      "465             int i_d = ii % dim[d]; /* i_d is our position in the d'th dimension   */ \\\n",
      "466             ii = ii / dim[d]; \\\n",
      "467             a_i += i_d * a_str[d]; /* increment our a and z pointers by i_d elements */ \\\n",
      "468             z_i += i_d * z_str[d]; \\\n",
      "469         } \\\n",
      "470         z_i[0] = F(a_i[0]); \\\n",
      "471     } \\\n",
      "472 }\n",
      "473 \n",
      "474 template<typename T> __device__ T unary_copy(T a) { return a; }\n",
      "475 decl_k_elemwise_unary_rowmajor(k_elemwise_unary_rowmajor_copy, unary_copy<float>)\n",
      "476 \n",
      "477 template<typename T> __device__ T unary_exp(T a) { return exp(a); }\n",
      "478 decl_k_elemwise_unary_rowmajor(k_elemwise_unary_rowmajor_exp, unary_exp<float>)\n",
      "479 \n",
      "480 /////////////////////////////\n",
      "481 // Satisfying reqs to be Type\n",
      "482 /////////////////////////////\n",
      "483 \n",
      "484 //DON'T use directly(if their is other CudaNdarray that point to it, it will cause problem)! use Py_DECREF() instead\n",
      "485 static void\n",
      "486 CudaNdarray_dealloc(CudaNdarray* self)\n",
      "487 {\n",
      "488     if (0) std::cerr << \"CudaNdarray dealloc \" << self << \" \" << self->devdata << '\\n';\n",
      "489     if(Py_REFCNT(self) > 1)\n",
      "490       printf(\"WARNING:CudaNdarray_dealloc called when there is still active reference to it.\\n\");\n",
      "491     CudaNdarray_uninit(self);\n",
      "492     Py_TYPE(self)->tp_free((PyObject*)self);\n",
      "493     --_outstanding_mallocs[1];\n",
      "494     if (0)\n",
      "495     {\n",
      "496         fprintf(stderr, \"device_malloc_counts: (device) %i (obj) %i\\n\",\n",
      "497                 _outstanding_mallocs[0],\n",
      "498                 _outstanding_mallocs[1]);\n",
      "499     }\n",
      "500 }\n",
      "501 \n",
      "502 static PyObject *\n",
      "503 CudaNdarray_new(PyTypeObject *type, PyObject *args, PyObject *kwds)\n",
      "504 {\n",
      "505     CudaNdarray *self;\n",
      "506 \n",
      "507     self = (CudaNdarray *)type->tp_alloc(type, 0);\n",
      "508     if (self != NULL)\n",
      "509     {\n",
      "510         CudaNdarray_null_init(self);\n",
      "511         ++_outstanding_mallocs[1];\n",
      "512     }\n",
      "513     return (PyObject *)self;\n",
      "514 }\n",
      "515 static int\n",
      "516 CudaNdarray_init(CudaNdarray *self, PyObject *args, PyObject *kwds)\n",
      "517 {\n",
      "518     PyObject *arr=NULL;\n",
      "519 \n",
      "520     if (! PyArg_ParseTuple(args, \"O\", &arr))\n",
      "521         return -1;\n",
      "522     if (! PyArray_Check(arr))\n",
      "523     {\n",
      "524         PyErr_SetString(PyExc_TypeError, \"PyArray arg required\");\n",
      "525         return -1;\n",
      "526     }\n",
      "527     int rval = CudaNdarray_CopyFromArray(self, (PyArrayObject*)arr);\n",
      "528     return rval;\n",
      "529 }\n",
      "530 static PyMemberDef CudaNdarray_members[] =\n",
      "531 {\n",
      "532     /*\n",
      "533     {\"first\", T_OBJECT_EX, offsetof(CudaNdarray, first), 0,\n",
      "534      \"first name\"},\n",
      "535     {\"last\", T_OBJECT_EX, offsetof(CudaNdarray, last), 0,\n",
      "536      \"last name\"},\n",
      "537     {\"number\", T_INT, offsetof(CudaNdarray, number), 0,\n",
      "538      \"noddy number\"},\n",
      "539      */\n",
      "540     {NULL}  /* Sentinel */\n",
      "541 };\n",
      "542 \n",
      "543 PyObject * CudaNdarray_CreateArrayObj(CudaNdarray * self, PyObject *args)\n",
      "544 {\n",
      "545     PyObject * dtype = NULL;\n",
      "546     if (args && !PyArg_ParseTuple(args, \"|O\", &dtype))\n",
      "547         return NULL;\n",
      "548     if (dtype) {\n",
      "549         PyArray_Descr* dtype2;\n",
      "550         // PyArray_DescrConverter try to convert anything to a PyArray_Descr.\n",
      "551         if(!PyArray_DescrConverter(dtype, &dtype2))\n",
      "552         {\n",
      "553             PyObject * str = PyObject_Repr(dtype);\n",
      "554             PyErr_Format(PyExc_TypeError,\n",
      "555                          \"CudaNdarray dtype parameter not understood: %s\",\n",
      "556                          PyString_AsString(str)\n",
      "557                          );\n",
      "558             Py_CLEAR(str);\n",
      "559             return NULL;\n",
      "560         }\n",
      "561         int typeNum = dtype2->type_num;\n",
      "562         Py_DECREF(dtype2);\n",
      "563         if (typeNum != NPY_FLOAT32)\n",
      "564         {\n",
      "565             PyObject * str = PyObject_Repr(dtype);\n",
      "566             PyErr_Format(PyExc_TypeError,\n",
      "567                          \"CudaNdarray support only support float32 dtype, provided: %d\",\n",
      "568                          typeNum\n",
      "569                          );\n",
      "570             Py_CLEAR(str);\n",
      "571             return NULL;\n",
      "572         }\n",
      "573     }\n",
      "574 \n",
      "575     int verbose = 0;\n",
      "576     if(self->nd>=0 && CudaNdarray_SIZE(self)==0){\n",
      "577         npy_intp * npydims = (npy_intp*)malloc(self->nd * sizeof(npy_intp));\n",
      "578         assert (npydims);\n",
      "579         for (int i = 0; i < self->nd; ++i) npydims[i] = (npy_intp)(CudaNdarray_HOST_DIMS(self)[i]);\n",
      "580         PyObject * rval = PyArray_SimpleNew(self->nd, npydims, REAL_TYPENUM);\n",
      "581         free(npydims);\n",
      "582         if (!rval){\n",
      "583             return NULL;\n",
      "584         }\n",
      "585         assert (PyArray_ITEMSIZE((PyArrayObject *)rval) == sizeof(real));\n",
      "586         return rval;\n",
      "587     }\n",
      "588     if ((self->nd < 0) || (self->devdata == 0))\n",
      "589     {\n",
      "590         PyErr_SetString(PyExc_ValueError, \"can't copy from un-initialized CudaNdarray\");\n",
      "591         return NULL;\n",
      "592     }\n",
      "593     CudaNdarray * contiguous_self = NULL;\n",
      "594     if (CudaNdarray_is_c_contiguous(self))\n",
      "595     {\n",
      "596         contiguous_self = self;\n",
      "597         Py_INCREF(contiguous_self);\n",
      "598         if (verbose) std::cerr << \"CreateArrayObj already contiguous\" << contiguous_self << '\\n';\n",
      "599     }\n",
      "600     else\n",
      "601     {\n",
      "602         contiguous_self = (CudaNdarray*)CudaNdarray_Copy(self);\n",
      "603         if (verbose) std::cerr << \"CreateArrayObj created contiguous\" << contiguous_self << '\\n';\n",
      "604     }\n",
      "605     if (!contiguous_self)\n",
      "606     {\n",
      "607         return NULL;\n",
      "608     }\n",
      "609 \n",
      "610     npy_intp * npydims = (npy_intp*)malloc(self->nd * sizeof(npy_intp));\n",
      "611     assert (npydims);\n",
      "612     for (int i = 0; i < self->nd; ++i)\n",
      "613         npydims[i] = (npy_intp)(CudaNdarray_HOST_DIMS(self)[i]);\n",
      "614     PyArrayObject * rval = (PyArrayObject *) PyArray_SimpleNew(self->nd,\n",
      "615                                                                npydims,\n",
      "616                                                                REAL_TYPENUM);\n",
      "617     free(npydims);\n",
      "618     if (!rval)\n",
      "619     {\n",
      "620         Py_DECREF(contiguous_self);\n",
      "621         return NULL;\n",
      "622     }\n",
      "623 \n",
      "624     assert (PyArray_ITEMSIZE(rval) == sizeof(real));\n",
      "625 \n",
      "626     npy_intp rval_size = PyArray_SIZE(rval);\n",
      "627     void *rval_data = PyArray_DATA(rval);\n",
      "628     cudaError_t err;\n",
      "629     CNDA_BEGIN_ALLOW_THREADS;\n",
      "630 \n",
      "631     err = cudaMemcpy(rval_data, contiguous_self->devdata,\n",
      "632                      rval_size * sizeof(real),\n",
      "633                      cudaMemcpyDeviceToHost\n",
      "634                      );\n",
      "635     //CNDA_THREAD_SYNC;  // unneeded because cudaMemcpy is blocking anyway\n",
      "636     CNDA_END_ALLOW_THREADS;\n",
      "637 \n",
      "638     if (cudaSuccess != err)\n",
      "639     {\n",
      "640         PyErr_Format(PyExc_RuntimeError, \"error (%s)copying data to host\",\n",
      "641                      cudaGetErrorString(err));\n",
      "642         Py_DECREF(rval);\n",
      "643         rval = NULL;\n",
      "644     }\n",
      "645 \n",
      "646     Py_DECREF(contiguous_self);\n",
      "647     return (PyObject *)rval;\n",
      "648 }\n",
      "649 \n",
      "650 // TODO-- we have two functions here, ZEROS and Zeros.\n",
      "651 // ZEROS is meant to be called just from C code (you don't need to pass it PyObject * s)\n",
      "652 // but this naming is very weird, makes it look like a macro\n",
      "653 // we should figure out the correct convention and change to that\n",
      "654 PyObject* CudaNdarray_ZEROS(int n, int * dims)\n",
      "655 {\n",
      "656 \n",
      "657     size_t total_elements = 1;\n",
      "658 \n",
      "659     for(size_t i=0;i<n;i++){\n",
      "660         // Detect overflow on unsigned integer\n",
      "661         if (dims[i] != 0 && total_elements > (SIZE_MAX / dims[i])) {\n",
      "662             PyErr_Format(PyExc_RuntimeError,\n",
      "663                          \"Can't store in size_t for the bytes requested %llu * %llu\",\n",
      "664                          (unsigned long long)total_elements,\n",
      "665                          (unsigned long long)dims[i]);\n",
      "666             return NULL;\n",
      "667         }\n",
      "668         total_elements*=dims[i];\n",
      "669     }\n",
      "670 \n",
      "671     // total_elements now contains the size of the array, in reals\n",
      "672     if (total_elements > (SIZE_MAX / sizeof(real))){\n",
      "673         PyErr_Format(PyExc_RuntimeError,\n",
      "674                      \"Can't store in size_t for the bytes requested %llu * 4\",\n",
      "675                      (unsigned long long)total_elements);\n",
      "676         return NULL;\n",
      "677     }\n",
      "678     size_t total_size = total_elements * sizeof(real);\n",
      "679 \n",
      "680     CudaNdarray* rval = (CudaNdarray*)CudaNdarray_New();\n",
      "681     if (!rval)\n",
      "682     {\n",
      "683         PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_ZEROS: call to New failed\");\n",
      "684         return NULL;\n",
      "685     }\n",
      "686 \n",
      "687     if (CudaNdarray_alloc_contiguous(rval, n, dims))\n",
      "688     {\n",
      "689         PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_ZEROS: allocation failed.\");\n",
      "690         Py_DECREF(rval);\n",
      "691         return NULL;\n",
      "692     }\n",
      "693 \n",
      "694     // Fill with zeros\n",
      "695     //fprintf(stdout, \"Sizeof: %d\\n\", total_size);\n",
      "696     if (cudaSuccess != cudaMemset(rval->devdata, 0, total_size))\n",
      "697     {\n",
      "698         PyErr_Format(PyExc_MemoryError,\n",
      "699                      \"CudaNdarray_ZEROS: Error memsetting %llu bytes of device memory.\",\n",
      "700                      (unsigned long long)total_size);\n",
      "701         Py_DECREF(rval);\n",
      "702         return NULL;\n",
      "703     }\n",
      "704 \n",
      "705     if (cnda_copy_structure_to_device(rval))\n",
      "706     {\n",
      "707         PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_ZEROS: syncing structure to device failed\");\n",
      "708         Py_DECREF(rval);\n",
      "709         return NULL;\n",
      "710     }\n",
      "711     return (PyObject*) rval;\n",
      "712 }\n",
      "713 \n",
      "714 // declared as a static method (hence 1st parameter is not used)\n",
      "715 // Based on _Copy and _dimshuffle\n",
      "716 PyObject* CudaNdarray_Zeros(PyObject* _unused, PyObject* shape)\n",
      "717 {\n",
      "718     if(!shape)\n",
      "719     {\n",
      "720         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_Zeros: function takes at least 1 argument (0 given)\");\n",
      "721         return NULL;\n",
      "722     }\n",
      "723     if(!PySequence_Check(shape))\n",
      "724     {\n",
      "725         PyErr_SetString(PyExc_TypeError, \"shape argument must be a sequence\");\n",
      "726         return NULL;\n",
      "727     }\n",
      "728 \n",
      "729     int shplen = PySequence_Length(shape);\n",
      "730 \n",
      "731     if (shplen == 0)\n",
      "732     {\n",
      "733         return CudaNdarray_ZEROS(0, NULL);\n",
      "734     }\n",
      "735 \n",
      "736     int* newdims = (int *)malloc(sizeof(int) * shplen);\n",
      "737 \n",
      "738     if (!newdims)\n",
      "739     {\n",
      "740         PyErr_SetString(PyExc_MemoryError,\n",
      "741             \"CudaNdarray_Zeros: Failed to allocate temporary space\");\n",
      "742         return NULL;\n",
      "743     }\n",
      "744 \n",
      "745     // start from the end to compute strides\n",
      "746     for (int i = shplen-1; i >= 0; --i)\n",
      "747     {\n",
      "748         PyObject* shp_el_obj = PySequence_GetItem(shape, i);\n",
      "749         if(shp_el_obj == NULL)\n",
      "750         {\n",
      "751             // shouldn't happen since we checked length before...\n",
      "752             PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_Zeros: Index out of bound in sequence\");\n",
      "753             free(newdims);\n",
      "754             return NULL;\n",
      "755         }\n",
      "756 \n",
      "757         int shp_el = PyInt_AsLong(shp_el_obj);\n",
      "758         Py_DECREF(shp_el_obj);\n",
      "759 \n",
      "760         if (shp_el < 0)\n",
      "761         {\n",
      "762             PyErr_SetString(PyExc_ValueError, \"CudaNdarray_Zeros: shape must contain only non-negative values for size of a dimension\");\n",
      "763             free(newdims);\n",
      "764             return NULL;\n",
      "765         }\n",
      "766 \n",
      "767         newdims[i] = shp_el;\n",
      "768     }\n",
      "769 \n",
      "770     PyObject* rval = CudaNdarray_ZEROS(shplen,newdims);\n",
      "771 \n",
      "772     free(newdims);\n",
      "773 \n",
      "774     return (PyObject*)rval;\n",
      "775 }\n",
      "776 \n",
      "777 \n",
      "778 \n",
      "779 \n",
      "780 \n",
      "781 PyObject * CudaNdarray_Copy(const CudaNdarray * self)\n",
      "782 {\n",
      "783     PyObject * rval = CudaNdarray_New();\n",
      "784     if ((!rval) || (-1 == self->nd))\n",
      "785     {\n",
      "786         return rval;\n",
      "787     }\n",
      "788     if (CudaNdarray_alloc_contiguous((CudaNdarray*)rval, self->nd, CudaNdarray_HOST_DIMS(self)))\n",
      "789     {\n",
      "790         Py_DECREF(rval);\n",
      "791         return NULL;\n",
      "792     }\n",
      "793     if (CudaNdarray_CopyFromCudaNdarray((CudaNdarray*)rval, self))\n",
      "794     {\n",
      "795         Py_DECREF(rval);\n",
      "796         return NULL;\n",
      "797     }\n",
      "798     return rval;\n",
      "799 }\n",
      "800 PyObject * CudaNdarray_DeepCopy(CudaNdarray * self, PyObject * memo)\n",
      "801 {\n",
      "802     assert(PyDict_Check(memo));\n",
      "803     PyObject * selfkey = PyInt_FromLong((long)self);\n",
      "804     assert(selfkey);\n",
      "805     if (PyDict_Contains(memo, selfkey))\n",
      "806     {\n",
      "807         PyObject * rval = PyDict_GetItem(memo, selfkey);\n",
      "808         Py_DECREF(selfkey);\n",
      "809         Py_XINCREF(rval);\n",
      "810         return rval;\n",
      "811     }\n",
      "812     else\n",
      "813     {\n",
      "814         PyObject * rval = CudaNdarray_Copy(self);\n",
      "815         if (0) std::cerr << \"DeepCopy created \" << rval << \" devdata \" << ((CudaNdarray*)rval)->devdata << \"\\n\";\n",
      "816         if (NULL == rval)\n",
      "817         {\n",
      "818             Py_DECREF(selfkey);\n",
      "819             return NULL;\n",
      "820         }\n",
      "821         if (PyDict_SetItem(memo, selfkey, rval))\n",
      "822         {\n",
      "823             Py_DECREF(rval);\n",
      "824             Py_DECREF(selfkey);\n",
      "825             return NULL;\n",
      "826         }\n",
      "827         Py_DECREF(selfkey);\n",
      "828         return rval;\n",
      "829     }\n",
      "830 }\n",
      "831 PyObject * CudaNdarray_ReduceSum(CudaNdarray * self, PyObject * py_reduce_mask)\n",
      "832 {\n",
      "833     if (!PySequence_Check(py_reduce_mask))\n",
      "834     {\n",
      "835         PyErr_SetString(PyExc_TypeError, \"reduce_mask must be sequence of ints\");\n",
      "836         return NULL;\n",
      "837     }\n",
      "838     int len = PySequence_Length(py_reduce_mask);\n",
      "839     if (len != self->nd)\n",
      "840     {\n",
      "841         PyErr_SetString(PyExc_TypeError, \"length of reduce_mask must match self->nd\");\n",
      "842         return NULL;\n",
      "843     }\n",
      "844     CudaNdarray * self_sum = (CudaNdarray*)CudaNdarray_New();\n",
      "845     if (!self_sum)\n",
      "846     {\n",
      "847         return NULL;\n",
      "848     }\n",
      "849     //TODO: allocate a fixed size dimshuffle_pattern_cache on the stack,\n",
      "850     //      and use it if it is big enough.\n",
      "851     int * dimshuffle_pattern = (int*)malloc(len * 2 * sizeof(int));\n",
      "852     int * sum_dims = dimshuffle_pattern + len;\n",
      "853     int n_remaining_dims = 0;\n",
      "854     if (!dimshuffle_pattern)\n",
      "855     {\n",
      "856         Py_DECREF(self_sum);\n",
      "857         PyErr_SetString(PyExc_MemoryError, \"failed to alloc internal storage\");\n",
      "858         return NULL;\n",
      "859     }\n",
      "860     for (int i = 0; i < len; ++i)\n",
      "861     {\n",
      "862         PyObject *o_i = PySequence_GetItem(py_reduce_mask, i);\n",
      "863         int o_i_int = PyInt_AsLong(o_i);\n",
      "864         Py_XDECREF(o_i);\n",
      "865         if (PyErr_Occurred())\n",
      "866         {\n",
      "867             Py_DECREF(self_sum);\n",
      "868             free(dimshuffle_pattern);\n",
      "869             return NULL;\n",
      "870         }\n",
      "871         if (o_i_int) // this is a dimension over which we are reducing\n",
      "872         {\n",
      "873             sum_dims[i] = 1;\n",
      "874         }\n",
      "875         else\n",
      "876         {\n",
      "877             sum_dims[i] = CudaNdarray_HOST_DIMS(self)[i];\n",
      "878             dimshuffle_pattern[n_remaining_dims++] = i;\n",
      "879         }\n",
      "880     }\n",
      "881     if (0   || CudaNdarray_alloc_contiguous(self_sum, len, sum_dims)\n",
      "882             || CudaNdarray_reduce_sum(self_sum, self)\n",
      "883             || CudaNdarray_dimshuffle(self_sum, n_remaining_dims, dimshuffle_pattern))\n",
      "884     {\n",
      "885         Py_DECREF(self_sum);\n",
      "886         free(dimshuffle_pattern);\n",
      "887         return NULL;\n",
      "888     }\n",
      "889     free(dimshuffle_pattern);\n",
      "890     return (PyObject*)self_sum;\n",
      "891 }\n",
      "892 \n",
      "893 // Reshape self to the new shape gived by the tuple shape.\n",
      "894 //\n",
      "895 // If self is c contiguous, it return a view. Otherwise it always do a copy.\n",
      "896 // TODO: make it return a view when the strides allow it even if it is not\n",
      "897 //       c contiguous\n",
      "898 PyObject * CudaNdarray_Reshape(CudaNdarray * self, PyObject * shape)\n",
      "899 {\n",
      "900     if(!CudaNdarray_is_c_contiguous(self))\n",
      "901     {\n",
      "902         // allocate new space\n",
      "903         //TODO: test to see if we can re-use old one and take a new param to\n",
      "904         //  use this\n",
      "905         CudaNdarray* rval = (CudaNdarray*) CudaNdarray_Copy(self);\n",
      "906         if (!rval)\n",
      "907         {\n",
      "908             return NULL;\n",
      "909         }\n",
      "910 \n",
      "911         CudaNdarray* ret = (CudaNdarray*) CudaNdarray_Reshape(rval, shape);\n",
      "912         Py_XDECREF(rval);\n",
      "913         return (PyObject*)ret;\n",
      "914     }\n",
      "915 \n",
      "916     // check shape tuple\n",
      "917     unsigned int rval_nd;\n",
      "918     unsigned int * rval_dims;\n",
      "919     size_t rval_size = 1;\n",
      "920 \n",
      "921     if (PyTuple_Check(shape)){\n",
      "922         // copy shape to integer array\n",
      "923         rval_nd = PyTuple_Size(shape);\n",
      "924     }else if (PyInt_Check(shape)){\n",
      "925         rval_nd = 1;\n",
      "926     }else{\n",
      "927         PyErr_SetString(PyExc_TypeError, \"shape must be tuple of integers or an integer\");\n",
      "928         return NULL;\n",
      "929     }\n",
      "930     rval_dims = (unsigned int*)malloc(rval_nd * sizeof(int));\n",
      "931 \n",
      "932     if(PyTuple_Check(shape)){\n",
      "933         for (int i = 0; i < rval_nd; ++i)\n",
      "934         {\n",
      "935             rval_dims[i] = PyInt_AsLong(PyTuple_GetItem(shape, i)); //GetItem returns borrowed reference\n",
      "936             if (PyErr_Occurred()) //error in AsLong\n",
      "937             {\n",
      "938                 free(rval_dims);\n",
      "939                 return NULL;\n",
      "940             }\n",
      "941             if(rval_dims[i]<0){\n",
      "942                 PyErr_Format(PyExc_ValueError, \"Reshape has invalid dimension %i (must be >=0)\",rval_dims[i]);\n",
      "943                 free(rval_dims);\n",
      "944                 return NULL;\n",
      "945             }\n",
      "946             rval_size = rval_size * rval_dims[i];\n",
      "947         }\n",
      "948     }else{\n",
      "949         rval_size = PyInt_AsLong(shape);\n",
      "950         rval_dims[0] = rval_size;\n",
      "951     }\n",
      "952     // calculate new size, assert same as old size\n",
      "953     if (rval_size != CudaNdarray_SIZE(self))\n",
      "954     {\n",
      "955         PyErr_Format(PyExc_ValueError, \"size must remain unchanged, changed from %lld to %lld\", CudaNdarray_SIZE(self), rval_size);\n",
      "956         free(rval_dims);\n",
      "957         return NULL;\n",
      "958     }\n",
      "959     if (rval_size==0)\n",
      "960     {\n",
      "961         PyObject * rval = CudaNdarray_NewDims(rval_nd, rval_dims);\n",
      "962         free(rval_dims);\n",
      "963         return rval;\n",
      "964     }\n",
      "965 \n",
      "966     //return a view, not a copy\n",
      "967     //we can do this as we checked self is c_contiguous\n",
      "968     CudaNdarray * rval = (CudaNdarray * )CudaNdarray_New(rval_nd);\n",
      "969 \n",
      "970     if (!rval || 0 != rval->data_allocated\n",
      "971         ||CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self), self))\n",
      "972     {\n",
      "973         Py_XDECREF(rval);\n",
      "974         free(rval_dims);\n",
      "975         return NULL;\n",
      "976     }\n",
      "977     //set dim and stride\n",
      "978     int size = 1;\n",
      "979     for (int i = rval_nd-1; i >= 0; --i)\n",
      "980     {\n",
      "981         CudaNdarray_set_stride(rval, i, (rval_dims[i] == 1) ? 0 : size);\n",
      "982         CudaNdarray_set_dim(rval, i, rval_dims[i]);\n",
      "983         size = size * rval_dims[i];\n",
      "984     }\n",
      "985     free(rval_dims);\n",
      "986     return (PyObject*)rval;\n",
      "987 }\n",
      "988 \n",
      "989 PyObject * CudaNdarray_View(const CudaNdarray * self)\n",
      "990 {\n",
      "991     CudaNdarray * rval = (CudaNdarray*)CudaNdarray_New(self->nd);\n",
      "992     if (!rval || CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self), self))\n",
      "993     {\n",
      "994         Py_XDECREF(rval);\n",
      "995         rval = NULL;\n",
      "996     }\n",
      "997     else\n",
      "998     {\n",
      "999         for (int i = 0; i < self->nd; ++i)\n",
      "1000         {\n",
      "1001             CudaNdarray_set_dim(rval, i, CudaNdarray_HOST_DIMS(self)[i]);\n",
      "1002             CudaNdarray_set_stride(rval, i, CudaNdarray_HOST_STRIDES(self)[i]);\n",
      "1003         }\n",
      "1004     }\n",
      "1005     return (PyObject*)rval;\n",
      "1006 }\n",
      "1007 \n",
      "1008 /*\n",
      "1009  * d0,... are the output dims\n",
      "1010  * indices are a list of index to operate on\n",
      "1011  *         They are int32 viewed as float32.\n",
      "1012  * a is the output\n",
      "1013  * b is the input\n",
      "1014  * dB0, the source leading dimensions size\n",
      "1015  */\n",
      "1016 template <int operator_num>\n",
      "1017 __global__ void k_take_3(const int d0, const int d1, const int d2,\n",
      "1018                          const npy_int64* indices,\n",
      "1019                          float* a,\n",
      "1020                          const int sA0, const int sA1, const int sA2,\n",
      "1021                          const float* b, const int dB0,\n",
      "1022                          const int sB0, const int sB1, const int sB2,\n",
      "1023                          int* err){\n",
      "1024     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){\n",
      "1025         npy_int64 idx = indices[i0];\n",
      "1026         if (idx<0)\n",
      "1027             idx += dB0; // To allow negative indexing.\n",
      "1028         if ((idx < 0) || (idx >= dB0)){\n",
      "1029             // Any value other the 0 probably work. But to be more safe, I want\n",
      "1030             // to change all bits to prevent problem with concurrent write that\n",
      "1031             // could cross cache line. But this should not happen with the\n",
      "1032             // current code and driver.\n",
      "1033             *err = 0xFFFF;\n",
      "1034             continue;\n",
      "1035         }\n",
      "1036         for (int i1 = threadIdx.x; i1 < d1; i1 += blockDim.x){\n",
      "1037             for (int i2 = threadIdx.y; i2 < d2; i2 += blockDim.y){\n",
      "1038                 int a_idx = i0*sA0 + i1*sA1 + i2*sA2;\n",
      "1039                 int b_idx = idx*sB0 + i1*sB1 + i2*sB2;\n",
      "1040                 a[a_idx] = b[b_idx];\n",
      "1041             }\n",
      "1042         }\n",
      "1043     }\n",
      "1044 }\n",
      "1045 \n",
      "1046 // We try to be similar to the PyArray_TakeFrom function\n",
      "1047 //http://docs.scipy.org/doc/numpy/reference/c-api.array.html\n",
      "1048 //TODO: support other clip mode then raise(clip, wrap)\n",
      "1049 //self is the input that we copy data from.\n",
      "1050 //The indices that we receive MUST be an CudaNdarray(float32)\n",
      "1051 //    that is in fact a view to int64 indices\n",
      "1052 PyObject*\n",
      "1053 CudaNdarray_TakeFrom(CudaNdarray * self, PyObject *args){\n",
      "1054     int verbose = 0;\n",
      "1055     PyObject * indices_obj = NULL;\n",
      "1056     //int axis; Default None, that mean the flattened array.\n",
      "1057     PyObject * axis_obj = Py_None;\n",
      "1058     PyObject * out_obj = Py_None;\n",
      "1059     PyObject * clipmode_obj = NULL;\n",
      "1060     int max_threads = 1; // max threads per blocks\n",
      "1061 \n",
      "1062     if (! PyArg_ParseTuple(args, \"O|OOOi\", &indices_obj, &axis_obj,\n",
      "1063                            &out_obj, &clipmode_obj, &max_threads))\n",
      "1064         return NULL;\n",
      "1065 \n",
      "1066     //Check argument indices\n",
      "1067     //TODO: if not a numpy.ndarray, convert to numpy.ndarray\n",
      "1068     //TODO: If a CudaNdarray, accept it and suppose the data is int32? is float32 number of int?\n",
      "1069     //TODO: Support ndarray of other dtype then int32\n",
      "1070     //TODO: support list of indices that are not c_contiguous\n",
      "1071     CudaNdarray * indices = NULL;\n",
      "1072     if (CudaNdarray_Check(indices_obj)) {\n",
      "1073         if (verbose) printf(\"cudandarray indices\\n\");\n",
      "1074         indices = (CudaNdarray*) indices_obj;\n",
      "1075         Py_INCREF(indices);\n",
      "1076     } else if (PyArray_Check(indices_obj)) {\n",
      "1077         if (verbose) printf(\"ndarray indices\\n\");\n",
      "1078         if (PyArray_TYPE((PyArrayObject *)indices_obj) != NPY_INT64) {\n",
      "1079             PyErr_SetString(PyExc_TypeError,\n",
      "1080                             \"CudaNdarray_TakeFrom: need a ndarray for indices\"\n",
      "1081                             \" with dtype int64\");\n",
      "1082             return NULL;\n",
      "1083         }\n",
      "1084         if (PyArray_NDIM(((PyArrayObject*)indices_obj)) != 1) {\n",
      "1085             PyErr_SetString(PyExc_TypeError,\n",
      "1086                             \"CudaNdarray_TakeFrom: need a CudaNdarray of\"\n",
      "1087                             \" indices with only 1 dimensions\");\n",
      "1088             return NULL;\n",
      "1089         }\n",
      "1090         // We need indices_obj to be contiguous, in order to take a view\n",
      "1091         // with a different dtype.\n",
      "1092         if (!PyArray_IS_C_CONTIGUOUS((PyArrayObject*) indices_obj)) {\n",
      "1093             PyObject* indices_obj_contig = PyArray_NewCopy((PyArrayObject*) indices_obj, NPY_CORDER);\n",
      "1094             if (!indices_obj_contig)\n",
      "1095                 return NULL;\n",
      "1096             indices_obj = indices_obj_contig;\n",
      "1097         } else {\n",
      "1098             // Keep the refcount consistent\n",
      "1099             Py_INCREF(indices_obj);\n",
      "1100         }\n",
      "1101         PyArray_Descr* float32_descr = PyArray_DescrFromType(NPY_FLOAT32);\n",
      "1102         PyObject * indices_float32 = NULL;\n",
      "1103         indices_float32 = PyArray_View((PyArrayObject*)indices_obj,\n",
      "1104                                                   float32_descr, NULL);\n",
      "1105         if (verbose) printf(\"ndarray indices\\n\");\n",
      "1106         if (!indices_float32) {\n",
      "1107             Py_DECREF(indices_obj);\n",
      "1108             return NULL;\n",
      "1109         }\n",
      "1110 \n",
      "1111         indices = (CudaNdarray*) CudaNdarray_New();\n",
      "1112         if (verbose) printf(\"\\nndarray after new\\n\");\n",
      "1113         if (! indices){\n",
      "1114             Py_DECREF(indices_obj);\n",
      "1115             Py_DECREF(indices_float32);\n",
      "1116             return NULL;\n",
      "1117         }\n",
      "1118         if (CudaNdarray_CopyFromArray(indices,\n",
      "1119                                       (PyArrayObject *)indices_float32)){\n",
      "1120             Py_DECREF(indices_obj);\n",
      "1121             Py_DECREF(indices_float32);\n",
      "1122             return NULL;\n",
      "1123         }\n",
      "1124         Py_DECREF(indices_obj);\n",
      "1125         Py_DECREF(indices_float32);\n",
      "1126     } else {\n",
      "1127         PyObject* py_s = PyObject_Str(indices_obj);\n",
      "1128         const char* s = PyString_AsString(py_s);\n",
      "1129         Py_DECREF(py_s);\n",
      "1130         PyErr_Format(PyExc_TypeError,\n",
      "1131                      \"CudaNdarray_TakeFrom: need an ndarray of int64 or a\"\n",
      "1132                      \" CudaNdarray(float32) that is a view from int64 data\"\n",
      "1133                      \" for indices. Got %s\", s);\n",
      "1134         return NULL;\n",
      "1135     }\n",
      "1136 \n",
      "1137     if (verbose) {\n",
      "1138         printf(\"indices used on the gpu\\n\");\n",
      "1139         fprint_CudaNdarray(stdout, indices);\n",
      "1140         PyObject * used_indices = CudaNdarray_CreateArrayObj(indices);\n",
      "1141         PyObject_Print(used_indices, stdout, 0);\n",
      "1142         Py_DECREF(used_indices);\n",
      "1143     }\n",
      "1144     if (verbose) printf(\"after print of object\\n\");\n",
      "1145     if(!CudaNdarray_is_c_contiguous(indices) != 0) {\n",
      "1146         PyErr_SetString(PyExc_NotImplementedError,\n",
      "1147                         \"CudaNdarray_TakeFrom: The indices must be contiguous in memory.\");\n",
      "1148         Py_DECREF(indices);\n",
      "1149         return NULL;\n",
      "1150     }\n",
      "1151     int nb_indices = CudaNdarray_SIZE((CudaNdarray *)indices) / 2;// int64 are 8 bytes, float32 are 4 bytes\n",
      "1152 \n",
      "1153     //Check argument axis\n",
      "1154     //TODO: implement the default and other axis\n",
      "1155     long axis = PyInt_AsLong(axis_obj);\n",
      "1156 \n",
      "1157     if (axis != 0) {\n",
      "1158         PyErr_Format(PyExc_NotImplementedError,\n",
      "1159                      \"CudaNdarray_TakeFrom: only axis=0 is currently supported.\"\n",
      "1160                      \" Got %ld.\", axis);\n",
      "1161         Py_DECREF(indices);\n",
      "1162         return NULL;\n",
      "1163     }\n",
      "1164 \n",
      "1165     //Check argument out_obj\n",
      "1166     CudaNdarray * out = NULL;\n",
      "1167     if (out_obj && CudaNdarray_Check(out_obj))\n",
      "1168         out = (CudaNdarray*) out_obj;\n",
      "1169     if (out && (out->nd != self->nd ||\n",
      "1170                 CudaNdarray_HOST_DIMS(out)[0] != nb_indices))\n",
      "1171         out = NULL;\n",
      "1172     int * dims = (int *)malloc(sizeof(int) * self->nd);\n",
      "1173     dims[0] = nb_indices;\n",
      "1174 \n",
      "1175     for (int i=1 ; i<self->nd ; i++) {\n",
      "1176         dims[i] = CudaNdarray_HOST_DIMS(self)[i];\n",
      "1177         if (out && CudaNdarray_HOST_DIMS(out)[i] != dims[i]) {\n",
      "1178             out = NULL;\n",
      "1179         }\n",
      "1180     }\n",
      "1181     if (!out) {\n",
      "1182         out = (CudaNdarray*)CudaNdarray_New();\n",
      "1183         if (!out){\n",
      "1184             Py_DECREF(indices);\n",
      "1185             free(dims);\n",
      "1186             return NULL;\n",
      "1187         }\n",
      "1188         if (CudaNdarray_alloc_contiguous(out, self->nd, dims)) {\n",
      "1189             Py_DECREF(out);\n",
      "1190             Py_DECREF(indices);\n",
      "1191             free(dims);\n",
      "1192             return NULL;\n",
      "1193         }\n",
      "1194     }else {\n",
      "1195         Py_INCREF(out);\n",
      "1196     }\n",
      "1197 \n",
      "1198     //Check argument clipmode\n",
      "1199     if (clipmode_obj) {\n",
      "1200         char * clipmode = PyString_AsString(clipmode_obj);\n",
      "1201         if (! clipmode){\n",
      "1202             Py_DECREF(indices);\n",
      "1203             Py_DECREF(out);\n",
      "1204             free(dims);\n",
      "1205             return NULL;\n",
      "1206         }\n",
      "1207         if (strcmp(clipmode, \"raise\") != 0) {\n",
      "1208             PyErr_Format(PyExc_NotImplementedError,\n",
      "1209                          \"CudaNdarray_TakeFrom: only the raise mode is currently supported. Got '%s'\",\n",
      "1210                          clipmode);\n",
      "1211             Py_DECREF(indices);\n",
      "1212             Py_DECREF(out);\n",
      "1213             free(dims);\n",
      "1214             return NULL;\n",
      "1215         }\n",
      "1216     }\n",
      "1217     void (*k3)(const int, const int, const int,\n",
      "1218                const npy_int64*,\n",
      "1219                float*, const int, const int, const int,\n",
      "1220                const float*, const int,\n",
      "1221                const int, const int, const int,\n",
      "1222                int*);\n",
      "1223     k3 = k_take_3<CPY>;\n",
      "1224 \n",
      "1225     // Create the memory place that will store the error information.\n",
      "1226     if(init_err_var() != 0) return NULL;\n",
      "1227 \n",
      "1228     dim3 n_blocks(std::min(CudaNdarray_HOST_DIMS(out)[0],65535),1,1);\n",
      "1229     if(CudaNdarray_HOST_DIMS(out)[0] == 0){\n",
      "1230         // We take 0 elements, so no need for the rest of the code.\n",
      "1231         // This speed up that case AND fix crash otherwise.\n",
      "1232         free(dims);\n",
      "1233         Py_DECREF(indices);\n",
      "1234         return (PyObject *)out;\n",
      "1235     }\n",
      "1236 \n",
      "1237     switch (self->nd) {\n",
      "1238         case 1:\n",
      "1239             {\n",
      "1240                 dim3 n_threads(1, 1, 1);\n",
      "1241                 if (verbose)\n",
      "1242                     printf(\"cudaGetLastError=%d, nd=%d\"\n",
      "1243                            \" kernel config: (n_blocks.x=%d, n_blocks.y=%d,\"\n",
      "1244                            \" n_threads.x=%i, n_threads.y=%i)\\n\",\n",
      "1245                            cudaGetLastError(), self->nd,\n",
      "1246                            n_blocks.x, n_blocks.y, n_threads.x, n_threads.y);\n",
      "1247                 k3<<<n_blocks, n_threads>>>(\n",
      "1248                         dims[0],\n",
      "1249                         1,\n",
      "1250                         1,\n",
      "1251                         (npy_int64*) CudaNdarray_DEV_DATA(indices),\n",
      "1252                         CudaNdarray_DEV_DATA(out),\n",
      "1253                         CudaNdarray_HOST_STRIDES(out)[0], //strides\n",
      "1254                         1,\n",
      "1255                         1,\n",
      "1256                         CudaNdarray_DEV_DATA(self),\n",
      "1257                         CudaNdarray_HOST_DIMS(self)[0], //For indices check\n",
      "1258                         CudaNdarray_HOST_STRIDES(self)[0], //strides\n",
      "1259                         1,\n",
      "1260                         1,\n",
      "1261                         err_var);\n",
      "1262             }\n",
      "1263             break;\n",
      "1264         case 2:\n",
      "1265             {\n",
      "1266                 dim3 n_threads(std::min(CudaNdarray_HOST_DIMS(out)[1], max_threads), 1, 1);\n",
      "1267 \n",
      "1268                 if (verbose)\n",
      "1269                     printf(\"cudaGetLastError=%d, nd=%d\"\n",
      "1270                            \" kernel config: (n_blocks.x=%d, n_blocks.y=%d,\"\n",
      "1271                            \" n_threads.x=%i, n_threads.y=%i)\\n\",\n",
      "1272                            cudaGetLastError(), self->nd,\n",
      "1273                            n_blocks.x, n_blocks.y, n_threads.x, n_threads.y);\n",
      "1274 \n",
      "1275                 k3<<<n_blocks, n_threads>>>(\n",
      "1276                         dims[0], //dimensions\n",
      "1277                         dims[1],\n",
      "1278                         1,\n",
      "1279                         (npy_int64*) CudaNdarray_DEV_DATA(indices),\n",
      "1280                         CudaNdarray_DEV_DATA(out),\n",
      "1281                         CudaNdarray_HOST_STRIDES(out)[0], //strides\n",
      "1282                         CudaNdarray_HOST_STRIDES(out)[1],\n",
      "1283                         1,\n",
      "1284                         CudaNdarray_DEV_DATA(self),\n",
      "1285                         CudaNdarray_HOST_DIMS(self)[0], //For indices check\n",
      "1286                         CudaNdarray_HOST_STRIDES(self)[0], //strides\n",
      "1287                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1288                         1,\n",
      "1289                         err_var);\n",
      "1290             }\n",
      "1291             break;\n",
      "1292         case 3:\n",
      "1293             {\n",
      "1294                 int ty = std::min(CudaNdarray_HOST_DIMS(out)[2], max_threads);\n",
      "1295                 int tx = std::min(CudaNdarray_HOST_DIMS(out)[1], max_threads / ty);\n",
      "1296                 dim3 n_threads(tx, ty, 1);\n",
      "1297                 if (verbose)\n",
      "1298                     printf(\"cudaGetLastError=%d, nd=%d\"\n",
      "1299                            \" kernel config: (n_blocks.x=%d, n_blocks.y=%d,\"\n",
      "1300                            \" n_threads.x=%i, n_threads.y=%i)\\n\",\n",
      "1301                            cudaGetLastError(), self->nd,\n",
      "1302                            n_blocks.x, n_blocks.y, n_threads.x, n_threads.y);\n",
      "1303                 k3<<<n_blocks, n_threads>>>(\n",
      "1304                         dims[0], //dimensions\n",
      "1305                         dims[1],\n",
      "1306                         dims[2],\n",
      "1307                         (npy_int64*) CudaNdarray_DEV_DATA(indices),\n",
      "1308                         CudaNdarray_DEV_DATA(out),\n",
      "1309                         CudaNdarray_HOST_STRIDES(out)[0], //strides\n",
      "1310                         CudaNdarray_HOST_STRIDES(out)[1],\n",
      "1311                         CudaNdarray_HOST_STRIDES(out)[2],\n",
      "1312                         CudaNdarray_DEV_DATA(self),\n",
      "1313                         CudaNdarray_HOST_DIMS(self)[0], //For indices check\n",
      "1314                         CudaNdarray_HOST_STRIDES(self)[0], //strides\n",
      "1315                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1316                         CudaNdarray_HOST_STRIDES(self)[2],\n",
      "1317                         err_var);\n",
      "1318             }\n",
      "1319             break;\n",
      "1320     default:\n",
      "1321         PyErr_SetString(PyExc_NotImplementedError,\n",
      "1322                         \"CudaNdarray_TakeFrom: only input with 1, 2 or 3\"\n",
      "1323                         \" dimensions are currently supported\");\n",
      "1324 \n",
      "1325     }\n",
      "1326     free(dims);\n",
      "1327     CNDA_THREAD_SYNC;\n",
      "1328     cudaError_t err = cudaGetLastError();\n",
      "1329     if (cudaSuccess != err) {\n",
      "1330         PyErr_Format(PyExc_RuntimeError,\n",
      "1331                      \"Cuda error: %s: %s.\\n\",\n",
      "1332                      \"CudaNdarray_TakeFrom\",\n",
      "1333                      cudaGetErrorString(err));\n",
      "1334         Py_DECREF(indices);\n",
      "1335         Py_DECREF(out);\n",
      "1336         return NULL;\n",
      "1337     }\n",
      "1338 \n",
      "1339     int index_err = check_err_var();\n",
      "1340     Py_DECREF(indices);\n",
      "1341     if (index_err != 0) {\n",
      "1342         Py_DECREF(out);\n",
      "1343         return NULL;\n",
      "1344     }\n",
      "1345 \n",
      "1346     if (verbose) printf(\"TAKE SUCCEDED\\n\");\n",
      "1347     return (PyObject *)out;\n",
      "1348 }\n",
      "1349 \n",
      "1350 \n",
      "1351 PyObject * CudaNdarray_SetStride(CudaNdarray * self, PyObject *args)\n",
      "1352 {\n",
      "1353     int pos, stride;\n",
      "1354     if (! PyArg_ParseTuple(args, \"ii\", &pos, &stride))\n",
      "1355         return NULL;\n",
      "1356     if ((pos < 0) || (pos >= self->nd))\n",
      "1357     {\n",
      "1358         PyErr_Format(PyExc_ValueError, \"position argument out of legal range [0, %i)\", self->nd);\n",
      "1359         return NULL;\n",
      "1360     }\n",
      "1361     CudaNdarray_set_stride(self, pos, stride);\n",
      "1362     if (cnda_copy_structure_to_device(self))\n",
      "1363     {\n",
      "1364         return NULL;\n",
      "1365     }\n",
      "1366     Py_INCREF(Py_None);\n",
      "1367     return Py_None;\n",
      "1368 }\n",
      "1369 PyObject * CudaNdarray_SetShapeI(CudaNdarray * self, PyObject *args)\n",
      "1370 {\n",
      "1371     int pos, dim;\n",
      "1372     if (! PyArg_ParseTuple(args, \"ii\", &pos, &dim))\n",
      "1373         return NULL;\n",
      "1374     if ((pos < 0) || (pos >= self->nd))\n",
      "1375     {\n",
      "1376         PyErr_Format(PyExc_ValueError, \"position argument out of legal range [0, %i)\", self->nd);\n",
      "1377         return NULL;\n",
      "1378     }\n",
      "1379     CudaNdarray_set_dim(self, pos, dim);\n",
      "1380     if (cnda_copy_structure_to_device(self))\n",
      "1381     {\n",
      "1382         return NULL;\n",
      "1383     }\n",
      "1384     Py_INCREF(Py_None);\n",
      "1385     return Py_None;\n",
      "1386 }\n",
      "1387 \n",
      "1388 static PyObject *\n",
      "1389 CudaNdarray_exp(CudaNdarray* self)\n",
      "1390 {\n",
      "1391     CudaNdarray * rval = (CudaNdarray *)CudaNdarray_New();\n",
      "1392     if ((NULL == rval) || CudaNdarray_alloc_contiguous(rval, self->nd, CudaNdarray_HOST_DIMS(self)))\n",
      "1393     {\n",
      "1394         Py_XDECREF(rval);\n",
      "1395         return NULL;\n",
      "1396     }\n",
      "1397     unsigned int size = 1;\n",
      "1398     for (int i = 0; i < self->nd; i++)\n",
      "1399     {\n",
      "1400         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];\n",
      "1401     }\n",
      "1402     unsigned int threads_per_block = std::min(size, (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
      "1403     unsigned int n_blocks = std::min(ceil_intdiv(size,threads_per_block), (unsigned int)NUM_VECTOR_OP_BLOCKS);\n",
      "1404     k_elemwise_unary_rowmajor_exp<<<n_blocks,threads_per_block>>>(size, self->nd, CudaNdarray_DEV_DIMS(self),\n",
      "1405             CudaNdarray_DEV_DATA(self), CudaNdarray_DEV_STRIDES(self),\n",
      "1406             CudaNdarray_DEV_DATA(rval), CudaNdarray_DEV_STRIDES(rval));\n",
      "1407 \n",
      "1408     //TODO: don't do this right away, do it when we need the result\n",
      "1409     CNDA_THREAD_SYNC;\n",
      "1410     cudaError_t err = cudaGetLastError();\n",
      "1411     if( cudaSuccess != err)\n",
      "1412     {\n",
      "1413         Py_DECREF(rval);\n",
      "1414         PyErr_Format(PyExc_RuntimeError, \"Cuda error: %s: %s.\\n\", \"kExp\", cudaGetErrorString(err));\n",
      "1415         return NULL;\n",
      "1416     }\n",
      "1417 \n",
      "1418     return (PyObject*)rval;\n",
      "1419 }\n",
      "1420 \n",
      "1421 static PyMethodDef CudaNdarray_methods[] =\n",
      "1422 {\n",
      "1423     {\"__array__\",\n",
      "1424         (PyCFunction)CudaNdarray_CreateArrayObj, METH_VARARGS,\n",
      "1425         \"Copy from the device to a numpy ndarray\"},\n",
      "1426     {\"__copy__\",\n",
      "1427         (PyCFunction)CudaNdarray_View, METH_NOARGS,\n",
      "1428         \"Create a shallow copy of this object. used by module copy\"},\n",
      "1429     {\"__deepcopy__\",\n",
      "1430         (PyCFunction)CudaNdarray_DeepCopy, METH_O,\n",
      "1431         \"Create a copy of this object\"},\n",
      "1432     {\"zeros\",\n",
      "1433         (PyCFunction)CudaNdarray_Zeros, METH_STATIC | METH_O,\n",
      "1434         \"Create a new CudaNdarray with specified shape, filled with zeros.\"},\n",
      "1435     {\"copy\",\n",
      "1436         (PyCFunction)CudaNdarray_Copy, METH_NOARGS,\n",
      "1437         \"Create a copy of this object\"},\n",
      "1438     {\"is_c_contiguous\",\n",
      "1439         (PyCFunction)CudaNdarray_IS_C_Contiguous, METH_NOARGS,\n",
      "1440         \"Return True is the object is c contiguous. False otherwise.\"},\n",
      "1441     {\"reduce_sum\",\n",
      "1442         (PyCFunction)CudaNdarray_ReduceSum, METH_O,\n",
      "1443         \"Reduce over the given dimensions by summation\"},\n",
      "1444     {\"exp\",\n",
      "1445         (PyCFunction)CudaNdarray_exp, METH_NOARGS,\n",
      "1446         \"Return the exponential of all elements\"},\n",
      "1447     {\"reshape\",\n",
      "1448         (PyCFunction)CudaNdarray_Reshape, METH_O,\n",
      "1449         \"Return a reshaped view (or copy) of this ndarray\\n\\\n",
      "1450             The required argument is a tuple of integers specifying the shape of the new ndarray.\"},\n",
      "1451     {\"view\",\n",
      "1452         (PyCFunction)CudaNdarray_View, METH_NOARGS,\n",
      "1453         \"Return an alias of this ndarray\"},\n",
      "1454     {\"_set_stride\",\n",
      "1455         (PyCFunction)CudaNdarray_SetStride, METH_VARARGS,\n",
      "1456         \"For integer arguments (i, s), set the 'i'th stride to 's'\"},\n",
      "1457     {\"take\",\n",
      "1458         (PyCFunction)CudaNdarray_TakeFrom, METH_VARARGS,\n",
      "1459         \"Equivalent of numpy.take\"},\n",
      "1460     {\"_set_shape_i\",\n",
      "1461         (PyCFunction)CudaNdarray_SetShapeI, METH_VARARGS,\n",
      "1462         \"For integer arguments (i, s), set the 'i'th shape to 's'\"},\n",
      "1463     {NULL, NULL, NULL, NULL}  /* Sentinel */\n",
      "1464 };\n",
      "1465 \n",
      "1466 \n",
      "1467 ////////////////////\n",
      "1468 // Number protocol\n",
      "1469 ////////////////////\n",
      "1470 \n",
      "1471 __global__ void kAdd_contiguous(float* a, float* b, float* dest, unsigned int numEls) {\n",
      "1472     const unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "1473     const unsigned int numThreads = blockDim.x * gridDim.x;\n",
      "1474 \n",
      "1475     for (unsigned int i = idx; i < numEls; i += numThreads) {\n",
      "1476         dest[i] = a[i] + b[i];\n",
      "1477     }\n",
      "1478 }\n",
      "1479 \n",
      "1480 // Will be called by __add__ in Python\n",
      "1481 static PyObject *\n",
      "1482 CudaNdarray_add(PyObject* py_self, PyObject * py_other)\n",
      "1483 {\n",
      "1484     if (! CudaNdarray_Check(py_self)) {\n",
      "1485         PyErr_SetString(PyExc_TypeError, \"need a CudaNdarray on left\");\n",
      "1486         return NULL;\n",
      "1487     }\n",
      "1488     if (! CudaNdarray_Check(py_other)) {\n",
      "1489         PyErr_SetString(PyExc_TypeError, \"need a CudaNdarray on right\");\n",
      "1490         return NULL;\n",
      "1491     }\n",
      "1492     CudaNdarray * self = (CudaNdarray *)py_self;\n",
      "1493     CudaNdarray * other = (CudaNdarray *)py_other;\n",
      "1494     if(!CudaNdarray_is_c_contiguous(self) || !CudaNdarray_is_c_contiguous(other)){\n",
      "1495         PyErr_SetString(PyExc_TypeError, \"We have implementet only the c_contiguous version for now.\");\n",
      "1496         return NULL;\n",
      "1497     }\n",
      "1498 \n",
      "1499     //standard elemwise size checks\n",
      "1500     if (self->nd != other->nd)\n",
      "1501     {\n",
      "1502         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_add: need same number of dims\");\n",
      "1503         return NULL;\n",
      "1504     }\n",
      "1505     //standard elemwise dim checks\n",
      "1506     unsigned int size = 1;\n",
      "1507     for (int i = 0; i< self->nd; ++i)\n",
      "1508     {\n",
      "1509         if (CudaNdarray_HOST_DIMS(self)[i] != CudaNdarray_HOST_DIMS(other)[i])\n",
      "1510         {\n",
      "1511             PyErr_SetString(PyExc_TypeError, \"need same dimensions\");\n",
      "1512             return NULL;\n",
      "1513         }\n",
      "1514         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];\n",
      "1515     }\n",
      "1516     CudaNdarray * rval = (CudaNdarray *)CudaNdarray_New();\n",
      "1517     if (!rval || CudaNdarray_alloc_contiguous(rval, self->nd, CudaNdarray_HOST_DIMS(self)))\n",
      "1518     {\n",
      "1519         Py_XDECREF(rval);\n",
      "1520         return NULL;\n",
      "1521     }\n",
      "1522 \n",
      "1523     if(CudaNdarray_SIZE((CudaNdarray *)py_self)==0 && CudaNdarray_SIZE((CudaNdarray *)py_other)==0){\n",
      "1524       return (PyObject *) rval;\n",
      "1525     }\n",
      "1526 \n",
      "1527     int threads_per_block = std::min(size, (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
      "1528     int n_blocks = std::min(ceil_intdiv(size,(unsigned int)threads_per_block), (unsigned int)NUM_VECTOR_OP_BLOCKS);\n",
      "1529     kAdd_contiguous<<<n_blocks,threads_per_block>>>(\n",
      "1530             self->devdata, other->devdata, rval->devdata, size);\n",
      "1531     CNDA_THREAD_SYNC;\n",
      "1532     cudaError_t err = cudaGetLastError();\n",
      "1533     if( cudaSuccess != err)\n",
      "1534     {\n",
      "1535         PyErr_Format(PyExc_RuntimeError, \"Cuda error: %s: %s.\\n\", \"kAdd\", cudaGetErrorString(err));\n",
      "1536         Py_DECREF(rval);\n",
      "1537         return NULL;\n",
      "1538     }\n",
      "1539     return (PyObject *) rval;\n",
      "1540 }\n",
      "1541 \n",
      "1542 template <int operator_num>\n",
      "1543 __global__ void k_ielem_3(const int d0, const int d1, const int d2,\n",
      "1544         float* a, const int sA0, const int sA1, const int sA2,\n",
      "1545         const float* b, const int sB0, const int sB1, const int sB2){\n",
      "1546     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){\n",
      "1547         for (int i1 = blockIdx.y; i1 < d1; i1 += gridDim.y){\n",
      "1548             for (int i2 = threadIdx.x; i2 < d2; i2 += blockDim.x){\n",
      "1549                 switch (operator_num)\n",
      "1550                 {\n",
      "1551                   case IADD:\n",
      "1552                     a[i0*sA0 + i1*sA1 + i2*sA2] += b[i0*sB0 + i1*sB1 + i2*sB2];\n",
      "1553                     break;\n",
      "1554                   case IDIV:\n",
      "1555                     a[i0*sA0 + i1*sA1 + i2*sA2] /= b[i0*sB0 + i1*sB1 + i2*sB2];\n",
      "1556                     break;\n",
      "1557                   case CPY:\n",
      "1558                     a[i0*sA0 + i1*sA1 + i2*sA2] = b[i0*sB0 + i1*sB1 + i2*sB2];\n",
      "1559                     break;\n",
      "1560                 }\n",
      "1561             }\n",
      "1562         }\n",
      "1563     }\n",
      "1564 }\n",
      "1565 \n",
      "1566 template <int operator_num>\n",
      "1567 __global__ void k_ielem_4(const int d0, const int d1, const int d2, const int d3,\n",
      "1568                          float* a, const int sA0, const int sA1,\n",
      "1569                          const int sA2, const int sA3,\n",
      "1570                          const float* b, const int sB0, const int sB1,\n",
      "1571                          const int sB2, const int sB3){\n",
      "1572     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){\n",
      "1573         for (int i1 = blockIdx.y; i1 < d1; i1 += gridDim.y){\n",
      "1574             for (int i2 = threadIdx.x; i2 < d2; i2 += blockDim.x){\n",
      "1575                 for (int i3 = threadIdx.y; i3 < d3; i3 += blockDim.y){\n",
      "1576                     switch (operator_num) {\n",
      "1577                         case IADD:\n",
      "1578                             a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3]\n",
      "1579                             += b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3];\n",
      "1580                             break;\n",
      "1581                         case IDIV:\n",
      "1582                             a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3]\n",
      "1583                             /= b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3];\n",
      "1584                             break;\n",
      "1585                         case CPY:\n",
      "1586                             a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3]\n",
      "1587                             = b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3];\n",
      "1588                             break;\n",
      "1589                     }\n",
      "1590                 }\n",
      "1591             }\n",
      "1592         }\n",
      "1593     }\n",
      "1594 }\n",
      "1595 \n",
      "1596 template <int operator_num>\n",
      "1597 __global__ void k_ielem_6(const int d0, const int d1,\n",
      "1598                           const int d2, const int d3,\n",
      "1599                           const int d4, const int d5,\n",
      "1600                           float* a, const int sA0, const int sA1,\n",
      "1601                           const int sA2, const int sA3,\n",
      "1602                           const int sA4, const int sA5,\n",
      "1603                           const float* b, const int sB0, const int sB1,\n",
      "1604                           const int sB2, const int sB3,\n",
      "1605                           const int sB4, const int sB5\n",
      "1606                           ){\n",
      "1607     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){\n",
      "1608         for (int i1 = blockIdx.y; i1 < d1; i1 += gridDim.y){\n",
      "1609             for (int i2 = blockIdx.z; i2 < d2; i2 += gridDim.z){\n",
      "1610                 for (int i3 = threadIdx.x; i3 < d3; i3 += blockDim.x){\n",
      "1611                     for (int i4 = threadIdx.y; i4 < d4; i4 += blockDim.y){\n",
      "1612                         for (int i5 = threadIdx.z; i5 < d5; i5 += blockDim.z){\n",
      "1613                             switch (operator_num) {\n",
      "1614                             case IADD:\n",
      "1615                                 a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3 + i4*sA4 + i5*sA5]\n",
      "1616                                     += b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3 + i4*sB4 + i5*sB5];\n",
      "1617                                 break;\n",
      "1618                             case IDIV:\n",
      "1619                                 a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3 + i4*sA4 + i5*sA5]\n",
      "1620                                     /= b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3 + i4*sB4 + i5*sB5];\n",
      "1621                                 break;\n",
      "1622                             case CPY:\n",
      "1623                                 a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3 + i4*sA4 + i5*sA5]\n",
      "1624                                     = b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3 + i4*sB4 + i5*sB5];\n",
      "1625                                 break;\n",
      "1626                             }\n",
      "1627                         }\n",
      "1628                     }\n",
      "1629                 }\n",
      "1630             }\n",
      "1631         }\n",
      "1632     }\n",
      "1633 }\n",
      "1634 \n",
      "1635 /*\n",
      "1636 CudaNdarray_inplace_elemwise\n",
      "1637 Compute elemwise, working inplace on A.\n",
      "1638 Currently implemented A / B, A + B and A = B\n",
      "1639 (the last is not tested and not used!)\n",
      "1640 \n",
      "1641 py_self - the CudaNdarray that we'll modify (A)\n",
      "1642 py_other - the other argument (B)\n",
      "1643 fct_nb - which operation to perform (operator_t)\n",
      "1644 \n",
      "1645 Returns 0 on success.\n",
      "1646 Returns -1 on failure, and sets Python exception.\n",
      "1647 \n",
      "1648 */\n",
      "1649 int\n",
      "1650 CudaNdarray_inplace_elemwise(PyObject* py_self, PyObject * py_other, operator_t fct_nb)\n",
      "1651 {\n",
      "1652     int verbose = 0;\n",
      "1653     void (*k3)(const int, const int, const int,\n",
      "1654                     float*, const int, const int, const int,\n",
      "1655                     const float*, const int, const int, const int);\n",
      "1656     void (*k4)(const int, const int, const int, const int,\n",
      "1657                     float*, const int, const int,\n",
      "1658                     const int, const int,\n",
      "1659                     const float*, const int, const int,\n",
      "1660                     const int, const int);\n",
      "1661     void (*k6)(const int, const int,\n",
      "1662                const int, const int,\n",
      "1663                const int, const int,\n",
      "1664                float*, const int, const int,\n",
      "1665                const int, const int,\n",
      "1666                const int, const int,\n",
      "1667                const float*, const int, const int,\n",
      "1668                const int, const int,\n",
      "1669                const int, const int);\n",
      "1670     switch (fct_nb)\n",
      "1671     {\n",
      "1672         case IADD:\n",
      "1673             k3 = k_ielem_3<IADD>;\n",
      "1674             k4 = k_ielem_4<IADD>;\n",
      "1675             k6 = k_ielem_6<IADD>;\n",
      "1676             break;\n",
      "1677         case IDIV:\n",
      "1678             k3 = k_ielem_3<IDIV>;\n",
      "1679             k4 = k_ielem_4<IDIV>;\n",
      "1680             k6 = k_ielem_6<IDIV>;\n",
      "1681             break;\n",
      "1682         case CPY:\n",
      "1683             k3 = k_ielem_3<CPY>;\n",
      "1684             k4 = k_ielem_4<CPY>;\n",
      "1685             k6 = k_ielem_6<CPY>;\n",
      "1686             break;\n",
      "1687         default:\n",
      "1688             assert (0);\n",
      "1689             PyErr_Format(\n",
      "1690                 PyExc_TypeError,\n",
      "1691                 \"CudaNdarray_inplace_elemwise invalid fct_nb (%i).\",\n",
      "1692                 (int)fct_nb);\n",
      "1693             return -1;\n",
      "1694     }\n",
      "1695     if (!CudaNdarray_Check(py_self)) {\n",
      "1696         PyErr_SetString(\n",
      "1697             PyExc_TypeError,\n",
      "1698             \"CudaNdarray_inplace_elemwise need a CudaNdarray on left\");\n",
      "1699         return -1;\n",
      "1700     }\n",
      "1701     CudaNdarray * new_other = NULL;\n",
      "1702     if (!CudaNdarray_Check(py_other)) {\n",
      "1703         new_other = (CudaNdarray*) CudaNdarray_New();\n",
      "1704         if(!new_other)\n",
      "1705         {\n",
      "1706             return -1;\n",
      "1707         }\n",
      "1708         if(CudaNdarray_CopyFromArray(new_other, (PyArrayObject *) py_other))\n",
      "1709         {\n",
      "1710             Py_XDECREF(new_other);\n",
      "1711             return -1;\n",
      "1712         }\n",
      "1713         py_other = (PyObject *) new_other;\n",
      "1714     }\n",
      "1715 \n",
      "1716     CudaNdarray * self = (CudaNdarray *)py_self;\n",
      "1717     CudaNdarray * other = (CudaNdarray *)py_other;\n",
      "1718 \n",
      "1719     if (verbose)\n",
      "1720     {\n",
      "1721         fprintf(stderr,\n",
      "1722             \"INPLACE ADD/DIV for self->nd=%d other->nd=%d\\n\",\n",
      "1723             self->nd, other->nd);\n",
      "1724     }\n",
      "1725 \n",
      "1726     //standard elemwise nb dim checks\n",
      "1727     if (self->nd < other->nd)\n",
      "1728     {\n",
      "1729         PyErr_Format(\n",
      "1730             PyExc_TypeError,\n",
      "1731             \"CudaNdarray_inplace_elemwise: The destination need more or the\"\n",
      "1732             \" same number of dimensions then the source. Got %d and %d.\",\n",
      "1733             self->nd, other->nd);\n",
      "1734         Py_XDECREF(new_other);\n",
      "1735         return -1;\n",
      "1736     }\n",
      "1737 \n",
      "1738     //broadcast to the same number of dimensions.\n",
      "1739     int* other_dims = (int*) alloca(self->nd * sizeof(int));\n",
      "1740     int* other_strides = (int*) alloca(self->nd * sizeof(int));\n",
      "1741     int added_dims = self->nd - other->nd;\n",
      "1742     // Add the added broadcasted dimensions\n",
      "1743     for (int i = 0; i< added_dims; ++i)\n",
      "1744     {\n",
      "1745         other_dims[i] = 1;\n",
      "1746         other_strides[i] = 0;\n",
      "1747     }\n",
      "1748     // Copy the existing dimensions\n",
      "1749     for (int i = 0; i< other->nd; ++i)\n",
      "1750     {\n",
      "1751         other_dims[i+added_dims] = CudaNdarray_HOST_DIMS(other)[i];\n",
      "1752         other_strides[i+added_dims] = CudaNdarray_HOST_STRIDES(other)[i];\n",
      "1753     }\n",
      "1754 \n",
      "1755     //standard elemwise dim checks\n",
      "1756     unsigned int size = 1;\n",
      "1757     for (int i = 0; i< self->nd; ++i)\n",
      "1758     {\n",
      "1759         if ((CudaNdarray_HOST_DIMS(self)[i] != other_dims[i])\n",
      "1760             && (other_dims[i] != 1))\n",
      "1761         {\n",
      "1762             PyErr_SetString(\n",
      "1763                 PyExc_ValueError,\n",
      "1764                 \"CudaNdarray_inplace_elemwise need same dimensions (or broadcastable dimension)\");\n",
      "1765             Py_XDECREF(new_other);\n",
      "1766             return -1;\n",
      "1767         }\n",
      "1768         // if we're broadcasting other, then make sure it has stride 0\n",
      "1769         assert ((CudaNdarray_HOST_DIMS(self)[i] == other_dims[i])\n",
      "1770             || (other_strides[i] == 0));\n",
      "1771         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];\n",
      "1772     }\n",
      "1773 \n",
      "1774     if (size==0)\n",
      "1775     {\n",
      "1776         int other_size = CudaNdarray_SIZE((CudaNdarray *)py_other);\n",
      "1777         if (!(other_size == 0 || other_size == 1))\n",
      "1778         {\n",
      "1779             PyErr_SetString(\n",
      "1780                 PyExc_ValueError,\n",
      "1781                 \"CudaNdarray_inplace_elemwise cannot work inplace on\"\n",
      "1782                 \" un-initialized array when the new value have more than\"\n",
      "1783                 \" 0 or 1 broadcastable dimensions\");\n",
      "1784             Py_XDECREF(new_other);\n",
      "1785             return 0;\n",
      "1786         }\n",
      "1787         Py_XDECREF(new_other);\n",
      "1788         return 0;\n",
      "1789     }\n",
      "1790 \n",
      "1791     switch(self->nd)\n",
      "1792     {\n",
      "1793         case 0:\n",
      "1794             {\n",
      "1795                 dim3 n_blocks(1, 1, 1);\n",
      "1796                 dim3 n_threads(1);\n",
      "1797                 k3<<<n_blocks, n_threads>>>(\n",
      "1798                         1, //d0\n",
      "1799                         1, //d1\n",
      "1800                         1, //d2\n",
      "1801                         CudaNdarray_DEV_DATA(self),\n",
      "1802                         1, //strides\n",
      "1803                         1,\n",
      "1804                         1,\n",
      "1805                         CudaNdarray_DEV_DATA(other),\n",
      "1806                         1, //strides\n",
      "1807                         1,\n",
      "1808                         1);\n",
      "1809                 CNDA_THREAD_SYNC;\n",
      "1810                 cudaError_t err = cudaGetLastError();\n",
      "1811                 if (cudaSuccess != err)\n",
      "1812                 {\n",
      "1813                     PyErr_Format(\n",
      "1814                         PyExc_RuntimeError,\n",
      "1815                         \"CudaNdarray_inplace_elemwise case0: Cuda error: %s: %s.\\n\",\n",
      "1816                         \"k3\",\n",
      "1817                         cudaGetErrorString(err));\n",
      "1818                     Py_XDECREF(new_other);\n",
      "1819                     return -1;\n",
      "1820                 }\n",
      "1821             }\n",
      "1822             break;\n",
      "1823         case 1:\n",
      "1824             {\n",
      "1825                 dim3 n_blocks(1, 1, 1);\n",
      "1826                 dim3 n_threads(\n",
      "1827                         std::min(\n",
      "1828                             CudaNdarray_HOST_DIMS(self)[0],\n",
      "1829                             NUM_VECTOR_OP_THREADS_PER_BLOCK));\n",
      "1830                 k3<<<n_blocks, n_threads>>>(\n",
      "1831                         1, //dimensions\n",
      "1832                         1,\n",
      "1833                         CudaNdarray_HOST_DIMS(self)[0],\n",
      "1834                         CudaNdarray_DEV_DATA(self),\n",
      "1835                         1, //strides\n",
      "1836                         1,\n",
      "1837                         CudaNdarray_HOST_STRIDES(self)[0],\n",
      "1838                         CudaNdarray_DEV_DATA(other),\n",
      "1839                         1, //strides\n",
      "1840                         1,\n",
      "1841                         other_strides[0]);\n",
      "1842                 CNDA_THREAD_SYNC;\n",
      "1843                 cudaError_t err = cudaGetLastError();\n",
      "1844                 if (cudaSuccess != err)\n",
      "1845                 {\n",
      "1846                     PyErr_Format(\n",
      "1847                         PyExc_RuntimeError,\n",
      "1848                         \"CudaNdarray_inplace_elemwise case1: Cuda error: %s: %s.\\n\",\n",
      "1849                         \"k3\",\n",
      "1850                         cudaGetErrorString(err));\n",
      "1851                     Py_XDECREF(new_other);\n",
      "1852                     return -1;\n",
      "1853                 }\n",
      "1854             }\n",
      "1855             break;\n",
      "1856         case 2:\n",
      "1857             {\n",
      "1858                 //TODO:  if both self and other are f-contiguous\n",
      "1859                 //       Then flip the block and thread dimensions\n",
      "1860                 //       to make contiguous reads & writes\n",
      "1861                 dim3 n_blocks(1,\n",
      "1862                         std::min(\n",
      "1863                             CudaNdarray_HOST_DIMS(self)[0],\n",
      "1864                             NUM_VECTOR_OP_BLOCKS));\n",
      "1865                 dim3 n_threads(\n",
      "1866                         std::min(\n",
      "1867                             CudaNdarray_HOST_DIMS(self)[1],\n",
      "1868                             NUM_VECTOR_OP_THREADS_PER_BLOCK));\n",
      "1869                 k3<<<n_blocks, n_threads>>>(1,\n",
      "1870                         CudaNdarray_HOST_DIMS(self)[0],\n",
      "1871                         CudaNdarray_HOST_DIMS(self)[1],\n",
      "1872                         CudaNdarray_DEV_DATA(self),\n",
      "1873                         1,\n",
      "1874                         CudaNdarray_HOST_STRIDES(self)[0],\n",
      "1875                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1876                         CudaNdarray_DEV_DATA(other),\n",
      "1877                         1,\n",
      "1878                         other_strides[0],\n",
      "1879                         other_strides[1]);\n",
      "1880                 CNDA_THREAD_SYNC;\n",
      "1881                 cudaError_t err = cudaGetLastError();\n",
      "1882                 if (cudaSuccess != err)\n",
      "1883                 {\n",
      "1884                     PyErr_Format(\n",
      "1885                         PyExc_RuntimeError,\n",
      "1886                         \"CudaNdarray_inplace_elemwise case2: Cuda error: %s: %s.\\n\",\n",
      "1887                         \"k3\",\n",
      "1888                         cudaGetErrorString(err));\n",
      "1889                     Py_XDECREF(new_other);\n",
      "1890                     return -1;\n",
      "1891                 }\n",
      "1892             }\n",
      "1893             break;\n",
      "1894         case 3:\n",
      "1895             {\n",
      "1896                 //TODO:  Dimshuffle so that at least one of the arrays\n",
      "1897                 //       has a contiguous dimension on the thread idx.\n",
      "1898                 dim3 n_blocks(\n",
      "1899                         std::min(\n",
      "1900                             CudaNdarray_HOST_DIMS(self)[0],\n",
      "1901                             NUM_VECTOR_OP_BLOCKS),\n",
      "1902                         CudaNdarray_HOST_DIMS(self)[1]);\n",
      "1903                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)\n",
      "1904                     n_blocks.y /= 2;\n",
      "1905                 dim3 n_threads(\n",
      "1906                         std::min(\n",
      "1907                             CudaNdarray_HOST_DIMS(self)[2],\n",
      "1908                             NUM_VECTOR_OP_THREADS_PER_BLOCK));\n",
      "1909                 k3<<<n_blocks, n_threads>>>(\n",
      "1910                         CudaNdarray_HOST_DIMS(self)[0],\n",
      "1911                         CudaNdarray_HOST_DIMS(self)[1],\n",
      "1912                         CudaNdarray_HOST_DIMS(self)[2],\n",
      "1913                         CudaNdarray_DEV_DATA(self),\n",
      "1914                         CudaNdarray_HOST_STRIDES(self)[0],\n",
      "1915                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1916                         CudaNdarray_HOST_STRIDES(self)[2],\n",
      "1917                         CudaNdarray_DEV_DATA(other),\n",
      "1918                         other_strides[0],\n",
      "1919                         other_strides[1],\n",
      "1920                         other_strides[2]);\n",
      "1921                 CNDA_THREAD_SYNC;\n",
      "1922                 cudaError_t err = cudaGetLastError();\n",
      "1923                 if (cudaSuccess != err)\n",
      "1924                 {\n",
      "1925                     PyErr_Format(\n",
      "1926                         PyExc_RuntimeError,\n",
      "1927                         \"CudaNdarray_inplace_elemwise case3: Cuda error: %s: %s.\\n\",\n",
      "1928                         \"k3\",\n",
      "1929                         cudaGetErrorString(err));\n",
      "1930                     Py_XDECREF(new_other);\n",
      "1931                     return -1;\n",
      "1932                 }\n",
      "1933             }\n",
      "1934             break;\n",
      "1935         case 4:\n",
      "1936             {\n",
      "1937                 dim3 n_blocks(\n",
      "1938                         std::min(\n",
      "1939                             CudaNdarray_HOST_DIMS(self)[0],\n",
      "1940                             NUM_VECTOR_OP_BLOCKS),\n",
      "1941                         CudaNdarray_HOST_DIMS(self)[1]\n",
      "1942                         );\n",
      "1943                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)\n",
      "1944                     n_blocks.y /= 2;\n",
      "1945                 dim3 n_threads(\n",
      "1946                         std::min(\n",
      "1947                             CudaNdarray_HOST_DIMS(self)[2],\n",
      "1948                             NUM_VECTOR_OP_THREADS_PER_BLOCK)\n",
      "1949                     //TODO: DON\"T YOU NEED OT PUT DIMS[3] in here???\n",
      "1950                             );\n",
      "1951                 k4<<<n_blocks, n_threads>>>(\n",
      "1952                         CudaNdarray_HOST_DIMS(self)[0],\n",
      "1953                         CudaNdarray_HOST_DIMS(self)[1],\n",
      "1954                         CudaNdarray_HOST_DIMS(self)[2],\n",
      "1955                         CudaNdarray_HOST_DIMS(self)[3],\n",
      "1956                         CudaNdarray_DEV_DATA(self),\n",
      "1957                         CudaNdarray_HOST_STRIDES(self)[0],\n",
      "1958                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1959                         CudaNdarray_HOST_STRIDES(self)[2],\n",
      "1960                         CudaNdarray_HOST_STRIDES(self)[3],\n",
      "1961                         CudaNdarray_DEV_DATA(other),\n",
      "1962                         other_strides[0],\n",
      "1963                         other_strides[1],\n",
      "1964                         other_strides[2],\n",
      "1965                         other_strides[3]);\n",
      "1966                 CNDA_THREAD_SYNC;\n",
      "1967                 cudaError_t err = cudaGetLastError();\n",
      "1968                 if (cudaSuccess != err)\n",
      "1969                 {\n",
      "1970                     PyErr_Format(\n",
      "1971                         PyExc_RuntimeError,\n",
      "1972                         \"CudaNdarray_inplace_elemwise case4: Cuda error: %s: %s.\\n\",\n",
      "1973                         \"k4\",\n",
      "1974                         cudaGetErrorString(err));\n",
      "1975                     Py_XDECREF(new_other);\n",
      "1976                     return -1;\n",
      "1977                 }\n",
      "1978             }\n",
      "1979             break;\n",
      "1980         case 5:\n",
      "1981             {\n",
      "1982                 dim3 n_blocks(\n",
      "1983                         std::min(\n",
      "1984                             CudaNdarray_HOST_DIMS(self)[1],\n",
      "1985                             NUM_VECTOR_OP_BLOCKS),\n",
      "1986                         CudaNdarray_HOST_DIMS(self)[2]);\n",
      "1987                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)\n",
      "1988                     n_blocks.y /= 2;\n",
      "1989                 dim3 n_threads(\n",
      "1990                         std::min(\n",
      "1991                             CudaNdarray_HOST_DIMS(self)[3],\n",
      "1992                             NUM_VECTOR_OP_THREADS_PER_BLOCK)\n",
      "1993                     //TODO: DON\"T YOU NEED OT PUT DIMS[3] in here???\n",
      "1994                     );\n",
      "1995                 for (int i = 0; i < CudaNdarray_HOST_DIMS(self)[0]; ++i)\n",
      "1996                 {\n",
      "1997                      k4<<<n_blocks, n_threads>>>(\n",
      "1998                             CudaNdarray_HOST_DIMS(self)[1],\n",
      "1999                             CudaNdarray_HOST_DIMS(self)[2],\n",
      "2000                             CudaNdarray_HOST_DIMS(self)[3],\n",
      "2001                             CudaNdarray_HOST_DIMS(self)[4],\n",
      "2002                             CudaNdarray_DEV_DATA(self) + i * CudaNdarray_HOST_STRIDES(self)[0],\n",
      "2003                             CudaNdarray_HOST_STRIDES(self)[1],\n",
      "2004                             CudaNdarray_HOST_STRIDES(self)[2],\n",
      "2005                             CudaNdarray_HOST_STRIDES(self)[3],\n",
      "2006                             CudaNdarray_HOST_STRIDES(self)[4],\n",
      "2007                             CudaNdarray_DEV_DATA(other) + i * other_strides[0],\n",
      "2008                             other_strides[1],\n",
      "2009                             other_strides[2],\n",
      "2010                             other_strides[3],\n",
      "2011                             other_strides[4]);\n",
      "2012                     CNDA_THREAD_SYNC;\n",
      "2013                     cudaError_t err = cudaGetLastError();\n",
      "2014                     if( cudaSuccess != err)\n",
      "2015                     {\n",
      "2016                         PyErr_Format(\n",
      "2017                             PyExc_RuntimeError,\n",
      "2018                             \"CudaNdarray_inplace_elemwise case5: Cuda error: %s: %s. n_block=(%ld,%ld) n_threads=%ld\\n\",\n",
      "2019                             \"k5 with loop over k4\",\n",
      "2020                             cudaGetErrorString(err),\n",
      "2021                             (long) n_blocks.x, (long) n_blocks.y, (long) n_threads.x);\n",
      "2022                         Py_XDECREF(new_other);\n",
      "2023                         return -1;\n",
      "2024                     }\n",
      "2025                 }\n",
      "2026             }\n",
      "2027             break;\n",
      "2028         case 6:\n",
      "2029             {\n",
      "2030                 dim3 n_blocks(\n",
      "2031                         std::min(\n",
      "2032                             CudaNdarray_HOST_DIMS(self)[0],\n",
      "2033                             NUM_VECTOR_OP_BLOCKS),\n",
      "2034                         CudaNdarray_HOST_DIMS(self)[1],\n",
      "2035                         CudaNdarray_HOST_DIMS(self)[2]\n",
      "2036                         );\n",
      "2037                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)\n",
      "2038                     n_blocks.y /= 2;\n",
      "2039                 // GTX285(compute capabilities 1.3) don't support n_blocks.z > 1\n",
      "2040                 // (compute capabilities 2.0) support 65535 for n_blocks.z\n",
      "2041                 //while (n_blocks.x * n_blocks.y * n_blocks.z > NUM_VECTOR_OP_BLOCKS)\n",
      "2042                 //    n_blocks.z /= 2;\n",
      "2043                 n_blocks.z = 1;\n",
      "2044                 dim3 n_threads(\n",
      "2045                         std::min(\n",
      "2046                             CudaNdarray_HOST_DIMS(self)[3],\n",
      "2047                             NUM_VECTOR_OP_THREADS_PER_BLOCK)\n",
      "2048                     //TODO: DON'T YOU NEED TO PUT DIMS[4] in here???\n",
      "2049                     //TODO: DON'T YOU NEED TO PUT DIMS[5] in here???\n",
      "2050                             );\n",
      "2051                 k6<<<n_blocks, n_threads>>>(\n",
      "2052                         CudaNdarray_HOST_DIMS(self)[0],\n",
      "2053                         CudaNdarray_HOST_DIMS(self)[1],\n",
      "2054                         CudaNdarray_HOST_DIMS(self)[2],\n",
      "2055                         CudaNdarray_HOST_DIMS(self)[3],\n",
      "2056                         CudaNdarray_HOST_DIMS(self)[4],\n",
      "2057                         CudaNdarray_HOST_DIMS(self)[5],\n",
      "2058                         CudaNdarray_DEV_DATA(self),\n",
      "2059                         CudaNdarray_HOST_STRIDES(self)[0],\n",
      "2060                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "2061                         CudaNdarray_HOST_STRIDES(self)[2],\n",
      "2062                         CudaNdarray_HOST_STRIDES(self)[3],\n",
      "2063                         CudaNdarray_HOST_STRIDES(self)[4],\n",
      "2064                         CudaNdarray_HOST_STRIDES(self)[5],\n",
      "2065                         CudaNdarray_DEV_DATA(other),\n",
      "2066                         other_strides[0],\n",
      "2067                         other_strides[1],\n",
      "2068                         other_strides[2],\n",
      "2069                         other_strides[3],\n",
      "2070                         other_strides[4],\n",
      "2071                         other_strides[5]);\n",
      "2072                 CNDA_THREAD_SYNC;\n",
      "2073                 cudaError_t err = cudaGetLastError();\n",
      "2074                 if (cudaSuccess != err)\n",
      "2075                 {\n",
      "2076                     PyErr_Format(\n",
      "2077                         PyExc_RuntimeError,\n",
      "2078                         \"CudaNdarray_inplace_elemwise case6: Cuda error: %s: %s. n_blocks=(%ld, %ld, %ld) n_threads=(%ld)\\n\",\n",
      "2079                         \"k6\",\n",
      "2080                         cudaGetErrorString(err),\n",
      "2081                         (long) n_blocks.x, (long) n_blocks.y, (long) n_blocks.z,\n",
      "2082                         (long) n_threads.x);\n",
      "2083                     Py_XDECREF(new_other);\n",
      "2084                     return -1;\n",
      "2085                 }\n",
      "2086             }\n",
      "2087             break;\n",
      "2088         default:\n",
      "2089         {\n",
      "2090             PyErr_Format(\n",
      "2091                 PyExc_NotImplementedError,\n",
      "2092                 \"inplace_elemwise w nd=%i\\n\",\n",
      "2093                 self->nd);\n",
      "2094             Py_XDECREF(new_other);\n",
      "2095             return -1;\n",
      "2096         }\n",
      "2097     }\n",
      "2098     if (verbose)\n",
      "2099         fprintf(stderr, \"INPLACE ADD/DIV end\\n\");\n",
      "2100     Py_XDECREF(new_other);\n",
      "2101     return 0;\n",
      "2102 }\n",
      "2103 \n",
      "2104 /*\n",
      "2105  * We need this inplace Add to support IncSubTensor\n",
      "2106  * It returns py_self on success with an additional reference. Else NULL.\n",
      "2107  */\n",
      "2108 // Will be called by __iadd__ in Python\n",
      "2109 PyObject *\n",
      "2110 CudaNdarray_inplace_add(PyObject* py_self, PyObject * py_other)\n",
      "2111 {\n",
      "2112     if (CudaNdarray_inplace_elemwise(py_self, py_other, IADD))\n",
      "2113     {\n",
      "2114         return NULL;\n",
      "2115     }\n",
      "2116     Py_INCREF(py_self);\n",
      "2117     return py_self;\n",
      "2118 }\n",
      "2119 \n",
      "2120 /*\n",
      "2121  * We need this inplace div for cuda/tests/test_basic_ops.py:test_shared_options\n",
      "2122  * It returns py_self on success with an additional reference. Else NULL.\n",
      "2123  */\n",
      "2124 // Will be called by __idiv__ in Python\n",
      "2125 static PyObject *\n",
      "2126 CudaNdarray_inplace_div(PyObject* py_self, PyObject * py_other)\n",
      "2127 {\n",
      "2128     if (CudaNdarray_inplace_elemwise(py_self, py_other, IDIV))\n",
      "2129     {\n",
      "2130         return NULL;\n",
      "2131     }\n",
      "2132     Py_INCREF(py_self);\n",
      "2133     return py_self;\n",
      "2134 }\n",
      "2135 \n",
      "2136 // The PyNumberMethods struct layout changed in a non-trivial way from 2 to 3.\n",
      "2137 #if PY_MAJOR_VERSION == 3\n",
      "2138 static PyNumberMethods CudaNdarrayNumberMethods =\n",
      "2139 {\n",
      "2140     (binaryfunc)CudaNdarray_add,  //binaryfunc nb_add;  __add__\n",
      "2141     0,  //binaryfunc nb_subtract;\n",
      "2142     0,  //binaryfunc nb_multiply;\n",
      "2143     0,  //binaryfunc nb_remainder;\n",
      "2144     0,  //binaryfunc nb_divmod;\n",
      "2145     0,  //ternaryfunc nb_power;\n",
      "2146     0,  //unaryfunc nb_negative;\n",
      "2147     0,  //unaryfunc nb_positive;\n",
      "2148     0,  //unaryfunc nb_absolute;\n",
      "2149     0,  //inquiry nb_bool;\n",
      "2150     0,  //unaryfunc nb_invert;\n",
      "2151     0,  //binaryfunc nb_lshift;\n",
      "2152     0,  //binaryfunc nb_rshift;\n",
      "2153     0,  //binaryfunc nb_and;\n",
      "2154     0,  //binaryfunc nb_xor;\n",
      "2155     0,  //binaryfunc nb_or;\n",
      "2156     0,  //unaryfunc nb_int;\n",
      "2157     0,  //void *nb_reserved;\n",
      "2158     0,  //unaryfunc nb_float;\n",
      "2159 \n",
      "2160     (binaryfunc)CudaNdarray_inplace_add,  //binaryfunc nb_inplace_add;  __iadd__\n",
      "2161     0,  //binaryfunc nb_inplace_subtract;\n",
      "2162     0,  //binaryfunc nb_inplace_multiply;\n",
      "2163     0,  //binaryfunc nb_inplace_remainder;\n",
      "2164     0,  //ternaryfunc nb_inplace_power;\n",
      "2165     0,  //binaryfunc nb_inplace_lshift;\n",
      "2166     0,  //binaryfunc nb_inplace_rshift;\n",
      "2167     0,  //binaryfunc nb_inplace_and;\n",
      "2168     0,  //binaryfunc nb_inplace_xor;\n",
      "2169     0,  //binaryfunc nb_inplace_or;\n",
      "2170 \n",
      "2171     0,  //binaryfunc nb_floor_divide;\n",
      "2172     0,  //binaryfunc nb_true_divide;\n",
      "2173     0,  //binaryfunc nb_inplace_floor_divide;\n",
      "2174     (binaryfunc)CudaNdarray_inplace_div,  //binaryfunc nb_inplace_true_divide;        __idiv__\n",
      "2175 \n",
      "2176     0,  //unaryfunc nb_index\n",
      "2177 };\n",
      "2178 #else\n",
      "2179 static PyNumberMethods CudaNdarrayNumberMethods =\n",
      "2180 {\n",
      "2181     (binaryfunc)CudaNdarray_add,  //binaryfunc nb_add;  __add__\n",
      "2182     0,  //binaryfunc nb_subtract;      __sub__\n",
      "2183     0,  //binaryfunc nb_multiply;      __mul__\n",
      "2184     0,  //binaryfunc nb_divide;        __div__\n",
      "2185     0,  //binaryfunc nb_remainder;     __mod__\n",
      "2186     0,  //binaryfunc nb_divmod;        __divmod__\n",
      "2187     0,  //ternaryfunc nb_power;        __pow__\n",
      "2188     0,  //unaryfunc nb_negative;       __neg__\n",
      "2189     0,  //unaryfunc nb_positive;       __pos__\n",
      "2190     0,  //unaryfunc nb_absolute;       __abs__\n",
      "2191     0,  //inquiry nb_nonzero;          __nonzero__     /* Used by PyObject_IsTrue */\n",
      "2192     0,  //unaryfunc nb_invert;         __invert__\n",
      "2193     0,  //binaryfunc nb_lshift;        __lshift__\n",
      "2194     0,  //binaryfunc nb_rshift;        __rshift__\n",
      "2195     0,  //binaryfunc nb_and;           __and__\n",
      "2196     0,  //binaryfunc nb_xor;           __xor__\n",
      "2197     0,  //binaryfunc nb_or;            __or__\n",
      "2198     0,  //coercion nb_coerce;          __coerce__     /* Used by the coerce() function */\n",
      "2199     0,  //unaryfunc nb_int;            __int__\n",
      "2200     0,  //unaryfunc nb_long;           __long__\n",
      "2201     0,  //unaryfunc nb_float;          __float__\n",
      "2202     0,  //unaryfunc nb_oct;            __oct__\n",
      "2203     0,  //unaryfunc nb_hex;            __hex__\n",
      "2204 \n",
      "2205     /* Added in release 2.0 */\n",
      "2206     (binaryfunc)CudaNdarray_inplace_add,  //binaryfunc nb_inplace_add;  __iadd__\n",
      "2207     0,  //binaryfunc nb_inplace_subtract;      __isub__\n",
      "2208     0,  //binaryfunc nb_inplace_multiply;      __imul__\n",
      "2209     (binaryfunc)CudaNdarray_inplace_div,  //binaryfunc nb_inplace_divide;        __idiv__\n",
      "2210     0,  //binaryfunc nb_inplace_remainder;     __imod__\n",
      "2211     0,  //ternaryfunc nb_inplace_power;        __ipow__\n",
      "2212     0,  //binaryfunc nb_inplace_lshift;        __ilshift__\n",
      "2213     0,  //binaryfunc nb_inplace_rshift;        __irshift__\n",
      "2214     0,  //binaryfunc nb_inplace_and;           __iand__\n",
      "2215     0,  //binaryfunc nb_inplace_xor;           __ixor__\n",
      "2216     0,  //binaryfunc nb_inplace_or;            __ior__\n",
      "2217 \n",
      "2218     /* Added in release 2.2 */\n",
      "2219     0,  //binaryfunc nb_floor_divide;          __floordiv__\n",
      "2220     0,  //binaryfunc nb_true_divide;           __truediv__\n",
      "2221     0,  //binaryfunc nb_inplace_floor_divide;  __ifloordiv__\n",
      "2222     0,  //binaryfunc nb_inplace_true_divide;   __itruediv__\n",
      "2223 \n",
      "2224 #if PY_MINOR_VERSION > 4\n",
      "2225     /* Added in release 2.5 */\n",
      "2226     0  //unaryfunc nb_index;  __index__\n",
      "2227 #endif\n",
      "2228 };\n",
      "2229 #endif\n",
      "2230 \n",
      "2231 \n",
      "2232 /////////////////////\n",
      "2233 // Mapping protocol\n",
      "2234 /////////////////////\n",
      "2235 \n",
      "2236 // Will by called by __len__ in Python\n",
      "2237 static Py_ssize_t\n",
      "2238 CudaNdarray_len(PyObject * py_self)\n",
      "2239 {\n",
      "2240     CudaNdarray * self = (CudaNdarray*) py_self;\n",
      "2241     if (self->nd <= 0)\n",
      "2242     {\n",
      "2243         return (Py_ssize_t) 0;\n",
      "2244     }\n",
      "2245     else\n",
      "2246     {\n",
      "2247         return (Py_ssize_t) CudaNdarray_HOST_DIMS(self)[0];\n",
      "2248     }\n",
      "2249 }\n",
      "2250 \n",
      "2251 // Will by called by __getitem__ in Python\n",
      "2252 PyObject *\n",
      "2253 CudaNdarray_Subscript(PyObject * py_self, PyObject * key)\n",
      "2254 {\n",
      "2255     int verbose = 0;\n",
      "2256     if (verbose) fprintf(stderr, \"Subscript .... \\n\");\n",
      "2257     CudaNdarray * self = (CudaNdarray*) py_self;\n",
      "2258     PyObject * py_rval = NULL;\n",
      "2259     CudaNdarray * rval = NULL;\n",
      "2260     PyObject * intobj = NULL;\n",
      "2261 \n",
      "2262     //PyObject_Print(key, stderr, 0);\n",
      "2263 \n",
      "2264     if (key == Py_Ellipsis)\n",
      "2265     {\n",
      "2266         Py_INCREF(py_self);\n",
      "2267         return py_self;\n",
      "2268     }\n",
      "2269     if ((intobj=PyNumber_Int(key))) //INDEXING BY INTEGER\n",
      "2270     //else if (PyInt_Check(key)) //INDEXING BY INTEGER\n",
      "2271     {\n",
      "2272         int d_idx = PyInt_AsLong(intobj);\n",
      "2273         Py_DECREF(intobj); intobj=NULL;\n",
      "2274         //int d_idx = PyInt_AsLong(key);\n",
      "2275         if (self->nd == 0)\n",
      "2276         {\n",
      "2277             PyErr_SetString(PyExc_IndexError, \"0-d arrays can't be indexed\");\n",
      "2278             return NULL;\n",
      "2279         }\n",
      "2280         int d_dim = CudaNdarray_HOST_DIMS(self)[0];\n",
      "2281         int offset = 0;\n",
      "2282 \n",
      "2283         if ((d_idx >= 0) && (d_idx < d_dim))\n",
      "2284         {\n",
      "2285             //normal indexing\n",
      "2286             offset += d_idx * CudaNdarray_HOST_STRIDES(self)[0];\n",
      "2287         }\n",
      "2288         else if ((d_idx < 0) && (d_idx >= -d_dim))\n",
      "2289         {\n",
      "2290             //end-based indexing\n",
      "2291             // d_idx is negative\n",
      "2292             offset += (d_dim + d_idx) * CudaNdarray_HOST_STRIDES(self)[0];\n",
      "2293         }\n",
      "2294         else\n",
      "2295         {\n",
      "2296             PyErr_Format(PyExc_IndexError,\n",
      "2297                          \"index out of bounds. Asked %d, but size of %d\",\n",
      "2298                          d_idx, d_dim);\n",
      "2299             return NULL;\n",
      "2300         }\n",
      "2301 \n",
      "2302         //allocate our subtensor view\n",
      "2303         py_rval = CudaNdarray_new_nd(self->nd - 1);\n",
      "2304         rval = (CudaNdarray*) py_rval;\n",
      "2305         if (!rval) return NULL;\n",
      "2306         assert (0 == rval->data_allocated);\n",
      "2307 \n",
      "2308         //initialize the view's data pointer to our own.\n",
      "2309         if (CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self) + offset, self))\n",
      "2310         {\n",
      "2311             Py_DECREF(rval);\n",
      "2312             return NULL;\n",
      "2313         }\n",
      "2314         for (int d = 1; d < self->nd; ++d)\n",
      "2315         {\n",
      "2316             CudaNdarray_set_stride(rval, d-1, CudaNdarray_HOST_STRIDES(self)[d]);\n",
      "2317             CudaNdarray_set_dim(rval, d-1, CudaNdarray_HOST_DIMS(self)[d]);\n",
      "2318         }\n",
      "2319     }\n",
      "2320     else\n",
      "2321     {\n",
      "2322         PyErr_Clear();\n",
      "2323     }\n",
      "2324     if (PySlice_Check(key)) //INDEXING BY SLICE\n",
      "2325     {\n",
      "2326         if (verbose) fprintf(stderr, \"by slice\\n\");\n",
      "2327         if (self->nd == 0)\n",
      "2328         {\n",
      "2329             PyErr_SetString(PyExc_ValueError, \"cannot slice a 0-d array\");\n",
      "2330             return NULL;\n",
      "2331         }\n",
      "2332 \n",
      "2333         int d_dim = CudaNdarray_HOST_DIMS(self)[0];\n",
      "2334         Py_ssize_t start, stop, step, slen;\n",
      "2335         if (PySlice_GetIndicesEx(SLICE_CAST(key), d_dim, &start, &stop, &step, &slen))\n",
      "2336         {\n",
      "2337             if (verbose)\n",
      "2338                 fprintf(stderr, \"PySlice_GetIndicesEx failed\\n\");\n",
      "2339             return NULL;\n",
      "2340         }\n",
      "2341         if (verbose)\n",
      "2342         {\n",
      "2343             std::cerr << \"start \" << start << \"\\n\";\n",
      "2344             std::cerr << \"stop \" << stop << \"\\n\";\n",
      "2345             std::cerr << \"step \" << step << \"\\n\";\n",
      "2346             std::cerr << \"slen \" << slen << \"\\n\";\n",
      "2347         }\n",
      "2348 \n",
      "2349         //allocate our subtensor view\n",
      "2350         py_rval = CudaNdarray_new_nd(self->nd);\n",
      "2351         rval = (CudaNdarray*) py_rval;\n",
      "2352         if (!rval) return NULL;\n",
      "2353         assert (0 == rval->data_allocated);\n",
      "2354 \n",
      "2355 \n",
      "2356         //initialize the view's data pointer to our own.\n",
      "2357         if (CudaNdarray_set_device_data(rval,\n",
      "2358                     CudaNdarray_DEV_DATA(self) + start * CudaNdarray_HOST_STRIDES(self)[0],\n",
      "2359                     self))\n",
      "2360         {\n",
      "2361             Py_DECREF(rval);\n",
      "2362             return NULL;\n",
      "2363         }\n",
      "2364         //initialize dimension 0 of rval\n",
      "2365         CudaNdarray_set_stride(rval, 0,\n",
      "2366                 (slen == 1) ? 0 : step * CudaNdarray_HOST_STRIDES(self)[0]);\n",
      "2367         CudaNdarray_set_dim(rval, 0, slen);\n",
      "2368         if (verbose) std::cerr << \"rval stride \" << CudaNdarray_HOST_STRIDES(rval)[0] << \"\\n\";\n",
      "2369         // initialize dimensions > 0 of rval\n",
      "2370         for (int d = 1; d < self->nd; ++d)\n",
      "2371         {\n",
      "2372             CudaNdarray_set_stride(rval, d, CudaNdarray_HOST_STRIDES(self)[d]);\n",
      "2373             CudaNdarray_set_dim(rval, d, CudaNdarray_HOST_DIMS(self)[d]);\n",
      "2374         }\n",
      "2375     }\n",
      "2376     if (PyTuple_Check(key)) //INDEXING BY TUPLE\n",
      "2377     {\n",
      "2378         if (verbose) fprintf(stderr, \"by tuple\\n\");\n",
      "2379         //elements of the tuple can be either integers or slices\n",
      "2380         //the dimensionality of the view we will return is diminished for each slice in the tuple\n",
      "2381 \n",
      "2382         if (PyTuple_Size(key) > self->nd)\n",
      "2383         {\n",
      "2384             PyErr_SetString(PyExc_IndexError, \"index error\");\n",
      "2385             return NULL;\n",
      "2386         }\n",
      "2387 \n",
      "2388         //calculate the number of dimensions in the return value\n",
      "2389         int rval_nd = self->nd;\n",
      "2390         for (int d = 0; d < PyTuple_Size(key); ++d)\n",
      "2391         {\n",
      "2392             //On some paltform PyInt_Check(<type 'numpy.int64'>) return true, other it return false.\n",
      "2393             //So we use PyArray_IsAnyScalar that should covert everything.\n",
      "2394             rval_nd -= PyArray_IsAnyScalar(PyTuple_GetItem(key, d));\n",
      "2395         }\n",
      "2396 \n",
      "2397         //allocate our subtensor view\n",
      "2398         py_rval = CudaNdarray_new_nd(rval_nd);\n",
      "2399         rval = (CudaNdarray*) py_rval;\n",
      "2400         if (!rval) return NULL;\n",
      "2401         assert (0 == rval->data_allocated);\n",
      "2402 \n",
      "2403         //initialize the view's data pointer to our own.\n",
      "2404         if (CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self), self))\n",
      "2405         {\n",
      "2406             Py_DECREF(rval);\n",
      "2407             return NULL;\n",
      "2408         }\n",
      "2409 \n",
      "2410         // rval_d will refer to the current dimension in the rval.\n",
      "2411         // It will not be incremented for integer keys, but will be incremented for slice\n",
      "2412         // keys\n",
      "2413         int rval_d = 0;\n",
      "2414 \n",
      "2415         for (int d = 0; d < self->nd; ++d)\n",
      "2416         {\n",
      "2417             // keys can be shorter than self->nd.\n",
      "2418             // when that happens, it means that the remaining dimensions are \"full slices\"\n",
      "2419             if (d >=PyTuple_Size(key))\n",
      "2420             {\n",
      "2421                 CudaNdarray_set_stride(rval, rval_d, CudaNdarray_HOST_STRIDES(self)[d]);\n",
      "2422                 CudaNdarray_set_dim(rval, rval_d, CudaNdarray_HOST_DIMS(self)[d]);\n",
      "2423                 ++rval_d;\n",
      "2424             }\n",
      "2425             else\n",
      "2426             {\n",
      "2427                 PyObject * key_d = PyTuple_GetItem(key, d);\n",
      "2428 \n",
      "2429                 if (PySlice_Check(key_d))\n",
      "2430                 {\n",
      "2431                     Py_ssize_t start, stop, step, slen;\n",
      "2432                     if (PySlice_GetIndicesEx(SLICE_CAST(key_d), CudaNdarray_HOST_DIMS(self)[d], &start, &stop, &step, &slen))\n",
      "2433                     {\n",
      "2434                         Py_DECREF(rval);\n",
      "2435                         return NULL;\n",
      "2436                     }\n",
      "2437                     rval->devdata += start * CudaNdarray_HOST_STRIDES(self)[d];\n",
      "2438                     CudaNdarray_set_stride(rval, rval_d,\n",
      "2439                             (slen == 1) ? 0 : step * CudaNdarray_HOST_STRIDES(self)[d]);\n",
      "2440                     CudaNdarray_set_dim(rval, rval_d, slen);\n",
      "2441                     if (0)\n",
      "2442                     {\n",
      "2443                         std::cerr << \"start \" << start << \"\\n\";\n",
      "2444                         std::cerr << \"stop \" << stop << \"\\n\";\n",
      "2445                         std::cerr << \"step \" << step << \"\\n\";\n",
      "2446                         std::cerr << \"slen \" << slen << \"\\n\";\n",
      "2447                     }\n",
      "2448                     ++rval_d;\n",
      "2449                 }\n",
      "2450                 else if ((intobj=PyNumber_Int(key_d)))\n",
      "2451                 {\n",
      "2452                     assert(PyArray_IsAnyScalar(key_d));\n",
      "2453                     int d_idx = PyInt_AsLong(intobj);\n",
      "2454                     Py_DECREF(intobj);\n",
      "2455                     intobj = NULL;\n",
      "2456                     int d_dim = CudaNdarray_HOST_DIMS(self)[d];\n",
      "2457 \n",
      "2458                     if ((d_idx >= 0) && (d_idx < d_dim))\n",
      "2459                     {\n",
      "2460                         //normal indexing\n",
      "2461                         rval->devdata += d_idx * CudaNdarray_HOST_STRIDES(self)[d];\n",
      "2462                     }\n",
      "2463                     else if ((d_idx < 0) && (d_idx >= -d_dim))\n",
      "2464                     {\n",
      "2465                         //end-based indexing\n",
      "2466                         rval->devdata += (d_dim + d_idx) * CudaNdarray_HOST_STRIDES(self)[d];\n",
      "2467                     }\n",
      "2468                     else\n",
      "2469                     {\n",
      "2470                         PyErr_Format(PyExc_IndexError,\n",
      "2471                                      \"index out of bounds. Asked %d for dimensions %d, but size of %d\",\n",
      "2472                                      d_idx, d, d_dim);\n",
      "2473                         Py_DECREF(rval);\n",
      "2474                         return NULL;\n",
      "2475                     }\n",
      "2476                 }\n",
      "2477                 else\n",
      "2478                 {\n",
      "2479                     PyErr_Clear(); // clear the error set by PyNumber_Int\n",
      "2480                     PyErr_SetString(PyExc_IndexError, \"index must be either int or slice\");\n",
      "2481                     Py_DECREF(rval);\n",
      "2482                     return NULL;\n",
      "2483                 }\n",
      "2484             }\n",
      "2485         }\n",
      "2486     }\n",
      "2487     if (py_rval)\n",
      "2488     {\n",
      "2489         if (verbose) fprint_CudaNdarray(stderr, self);\n",
      "2490         if (verbose) fprint_CudaNdarray(stderr, rval);\n",
      "2491     }\n",
      "2492     else\n",
      "2493     {\n",
      "2494         PyErr_SetString(PyExc_NotImplementedError, \"Unknown key type\");\n",
      "2495         return NULL;\n",
      "2496     }\n",
      "2497     return py_rval;\n",
      "2498 }\n",
      "2499 \n",
      "2500 // Will by called by __setitem__ in Python\n",
      "2501 // See http://docs.python.org/dev/py3k/c-api/object.html#PyObject_SetItem\n",
      "2502 // Doesn't handle broadcasting, e.g. a[:] = 5\n",
      "2503 // Can only be assigned from a CudaNdarray on the right side\n",
      "2504 // Or a ndarray\n",
      "2505 // Or a python scalar with value 0 when the left side part is c contiguous.\n",
      "2506 static int\n",
      "2507 CudaNdarray_setitem(PyObject *o, PyObject  *key, PyObject  *value)\n",
      "2508 {\n",
      "2509     int verbose = 0;\n",
      "2510     if (verbose) fprintf(stderr, \"CudaNdarray_setitem start\\n\");\n",
      "2511     // We try to copy directly into this CudaNdarray from the ndarray\n",
      "2512     CudaNdarray* rval = (CudaNdarray*)CudaNdarray_Subscript(o, key);\n",
      "2513     CudaNdarray* new_value = NULL;\n",
      "2514 \n",
      "2515     if(!rval){\n",
      "2516         // CudaNdarray_Subscript failed and set the error msg.\n",
      "2517         Py_XDECREF(rval);\n",
      "2518         return -1;\n",
      "2519     }\n",
      "2520 \n",
      "2521     if(rval != (CudaNdarray*)o &&\n",
      "2522                 (rval->data_allocated ||\n",
      "2523                  // The new array should have a base\n",
      "2524                  !(((CudaNdarray*)rval)->base) ||\n",
      "2525                  // If the original array has no base, the base of the new\n",
      "2526                  // array should be the original one\n",
      "2527                  (!((CudaNdarray*)o)->base && ((CudaNdarray*)rval)->base != o) ||\n",
      "2528                  // Else, the two arrays should have the same base\n",
      "2529                  (((CudaNdarray*)o)->base && ((CudaNdarray*)rval)->base != ((CudaNdarray*)o)->base)))\n",
      "2530     {\n",
      "2531         // This case shouldn't happen, based on what I see in Subscript\n",
      "2532         // but just in case it happens sometime in the future\n",
      "2533 \n",
      "2534         PyErr_Format(PyExc_RuntimeError,\n",
      "2535                      \"__getitem__ must return a CudaNdarray that refers to\"\n",
      "2536                      \" the original CudaNdarray, not a copy. rval.base=%p\"\n",
      "2537                      \" o.base=%p o=%p\",\n",
      "2538                      (((CudaNdarray*)rval)->base), ((CudaNdarray*)o)->base, o);\n",
      "2539         Py_DECREF(rval);\n",
      "2540         return -1;\n",
      "2541     }\n",
      "2542 \n",
      "2543     PyObject * intobj = NULL;\n",
      "2544     if (CudaNdarray_Check(o)  && PyArray_Check(value)){\n",
      "2545         if (verbose)\n",
      "2546             fprintf(stderr,\n",
      "2547                     \"CudaNdarray_setitem dest is a CudaNdarray and\"\n",
      "2548                     \" value is a ndarray\\n\");\n",
      "2549         new_value = (CudaNdarray*) CudaNdarray_New();\n",
      "2550         if(!new_value)\n",
      "2551         {\n",
      "2552             return -1;\n",
      "2553         }\n",
      "2554         if (CudaNdarray_CopyFromArray(new_value, (PyArrayObject *) value))\n",
      "2555         {\n",
      "2556             Py_XDECREF(new_value);\n",
      "2557             Py_XDECREF(rval);\n",
      "2558             return -1;\n",
      "2559         }\n",
      "2560         value = (PyObject *) new_value;\n",
      "2561     }\n",
      "2562     else if ((intobj=PyNumber_Int(value)))\n",
      "2563     {\n",
      "2564         if (verbose)\n",
      "2565             fprintf(stderr,\n",
      "2566                     \"CudaNdarray_setitem dest and value is a python number\\n\");\n",
      "2567         if(! CudaNdarray_is_c_contiguous(rval)){\n",
      "2568             PyErr_SetString(PyExc_NotImplementedError,\n",
      "2569                  \"CudaNdarray.__setitem__: When the new value is a scalar\"\n",
      "2570                  \" of value 0 the part where we copy to must be c contiguous.\");\n",
      "2571             Py_XDECREF(rval);\n",
      "2572             return -1;\n",
      "2573         }\n",
      "2574 \n",
      "2575         long val = PyInt_AsLong(intobj);\n",
      "2576         Py_DECREF(intobj); intobj=NULL;\n",
      "2577         if (val == 0)\n",
      "2578         {\n",
      "2579             cudaError_t err = cudaMemset(rval->devdata, 0,\n",
      "2580                                          CudaNdarray_SIZE(rval) * sizeof(real));\n",
      "2581             Py_XDECREF(rval);\n",
      "2582             if (err)\n",
      "2583             {\n",
      "2584                 // Clear the error flag, cudaMemset doesn't do it.\n",
      "2585                 // Currently this returns the same thing as err, but if in future\n",
      "2586                 // it returns something else I still don't see why we should ignore\n",
      "2587                 // it.  All we want to do here is reset the flag.\n",
      "2588                 cudaGetLastError();\n",
      "2589                 PyErr_SetString(PyExc_RuntimeError,\n",
      "2590                                 \"CudaNdarray.__setitem__: cudaMemset failed\");\n",
      "2591                 return -1;\n",
      "2592             }\n",
      "2593             return 0;\n",
      "2594         } else {\n",
      "2595             Py_XDECREF(rval);\n",
      "2596             PyErr_SetString(PyExc_NotImplementedError,\n",
      "2597                   \"CudaNdarray.__setitem__: we support setting only python\"\n",
      "2598                   \" scalar of value 0, numpy nd array and CudaNdarray.\");\n",
      "2599                 return -1;\n",
      "2600         }\n",
      "2601     }\n",
      "2602 \n",
      "2603     PyErr_Clear(); // clear PyNumber_Int error.\n",
      "2604 \n",
      "2605     if(!CudaNdarray_Check(o) || !CudaNdarray_Check(value))\n",
      "2606     {\n",
      "2607         PyErr_SetString(PyExc_TypeError,\n",
      "2608           \"CudaNdarray.__setitem__: left must be a CudaNdarrays and right\"\n",
      "2609           \" must be a CudaNdarrays, an ndarray or a python scalar of value 0.\");\n",
      "2610         Py_XDECREF(new_value);\n",
      "2611         return -1;\n",
      "2612     }\n",
      "2613 \n",
      "2614     if (verbose)\n",
      "2615         fprintf(stderr, \"CudaNdarray_setitem dest and value are CudaNdarray\\n\");\n",
      "2616 \n",
      "2617     if (cnda_copy_structure_to_device(rval))\n",
      "2618     {\n",
      "2619         PyErr_SetString(PyExc_RuntimeError,\n",
      "2620                 \"CudaNdarray.__setitem__: syncing structure to device failed\");\n",
      "2621         Py_DECREF(rval);\n",
      "2622         Py_XDECREF(new_value);\n",
      "2623 \n",
      "2624         if (verbose)\n",
      "2625             fprintf(stderr, \"CudaNdarray_setitem error end\\n\");\n",
      "2626         return -1;\n",
      "2627     }\n",
      "2628 \n",
      "2629     PyObject *baseSavedForComparison = rval->base;\n",
      "2630 \n",
      "2631     if (CudaNdarray_CopyFromCudaNdarray(rval, (CudaNdarray*)value, true))\n",
      "2632     {\n",
      "2633         Py_DECREF((PyObject*)rval);\n",
      "2634         Py_XDECREF(new_value);\n",
      "2635 \n",
      "2636         if (verbose)\n",
      "2637             fprintf(stderr, \"CudaNdarray_setitem error end\\n\");\n",
      "2638         return -1;\n",
      "2639     }\n",
      "2640 \n",
      "2641     assert (rval->base == baseSavedForComparison);\n",
      "2642     assert (rval->dev_structure_fresh);\n",
      "2643 \n",
      "2644     // Clean up locally-created references\n",
      "2645     Py_DECREF(rval);\n",
      "2646     Py_XDECREF(new_value);\n",
      "2647 \n",
      "2648     return 0;\n",
      "2649 }\n",
      "2650 \n",
      "2651 \n",
      "2652 PyMappingMethods CudaNdarrayMappingMethods = {\n",
      "2653     CudaNdarray_len, //lenfunc mp_length;               __len__\n",
      "2654     CudaNdarray_Subscript, //binaryfunc mp_subscript;   __getitem__\n",
      "2655     CudaNdarray_setitem //objobjargproc mp_ass_subscript;                __setitem__\n",
      "2656 };\n",
      "2657 \n",
      "2658 ////////////////////\n",
      "2659 //\n",
      "2660 ////////////////////\n",
      "2661 \n",
      "2662 static PyObject *\n",
      "2663 CudaNdarray_get_shape(CudaNdarray *self, void *closure)\n",
      "2664 {\n",
      "2665     if (self->nd < 0)\n",
      "2666     {\n",
      "2667         PyErr_SetString(PyExc_ValueError, \"CudaNdarray not initialized\");\n",
      "2668         return NULL;\n",
      "2669     }\n",
      "2670     PyObject * rval = PyTuple_New(self->nd);\n",
      "2671     for (int i = 0; i < self->nd; ++i)\n",
      "2672     {\n",
      "2673         if (!rval || PyTuple_SetItem(rval, i, PyInt_FromLong(CudaNdarray_HOST_DIMS(self)[i])))\n",
      "2674         {\n",
      "2675             Py_XDECREF(rval);\n",
      "2676             return NULL;\n",
      "2677         }\n",
      "2678 \n",
      "2679     }\n",
      "2680     return rval;\n",
      "2681 }\n",
      "2682 \n",
      "2683 static int\n",
      "2684 CudaNdarray_set_shape(CudaNdarray *self, PyObject *value, void *closure)\n",
      "2685 {\n",
      "2686     PyErr_SetString(PyExc_NotImplementedError, \"TODO: call reshape\");\n",
      "2687     return -1;\n",
      "2688 }\n",
      "2689 \n",
      "2690 static PyObject *\n",
      "2691 CudaNdarray_get_strides(CudaNdarray *self, void *closure)\n",
      "2692 {\n",
      "2693     if (self->nd < 0)\n",
      "2694     {\n",
      "2695         PyErr_SetString(PyExc_ValueError, \"CudaNdarray not initialized\");\n",
      "2696         return NULL;\n",
      "2697     }\n",
      "2698     PyObject * rval = PyTuple_New(self->nd);\n",
      "2699     for (int i = 0; i < self->nd; ++i)\n",
      "2700     {\n",
      "2701         if (!rval || PyTuple_SetItem(rval, i, PyInt_FromLong(CudaNdarray_HOST_STRIDES(self)[i])))\n",
      "2702         {\n",
      "2703             Py_XDECREF(rval);\n",
      "2704             return NULL;\n",
      "2705         }\n",
      "2706 \n",
      "2707     }\n",
      "2708     return rval;\n",
      "2709 }\n",
      "2710 \n",
      "2711 static int\n",
      "2712 CudaNdarray_set_strides(CudaNdarray *self, PyObject *value, void *closure)\n",
      "2713 {\n",
      "2714     //npy_intp newstrides_bytes[PyTuple_Size(value)];\n",
      "2715     if (PyTuple_Check(value)){\n",
      "2716         if (PyTuple_Size(value) != CudaNdarray_NDIM(self)){\n",
      "2717             PyErr_SetString(PyExc_ValueError,\n",
      "2718                             \"The new strides tuple must have the same length\"\n",
      "2719                             \" as the number of dimensions\");\n",
      "2720             return -1;\n",
      "2721         }\n",
      "2722     }else if (PyList_Check(value)){\n",
      "2723         if (PyList_Size(value) != CudaNdarray_NDIM(self)){\n",
      "2724             PyErr_SetString(PyExc_ValueError,\n",
      "2725                             \"The new strides list must have the same length\"\n",
      "2726                             \" as the number of dimensions\");\n",
      "2727             return -1;\n",
      "2728         }\n",
      "2729     }else{\n",
      "2730         PyErr_SetString(PyExc_ValueError,\n",
      "2731                         \"The new strides need to be encoded in a tuple or list\");\n",
      "2732         return -1;\n",
      "2733     }\n",
      "2734     npy_intp* newstrides = (npy_intp*) alloca(CudaNdarray_NDIM(self) * sizeof(npy_intp));\n",
      "2735     if (PyTuple_Check(value)){\n",
      "2736         for(int i=0; i < CudaNdarray_NDIM(self); i++){\n",
      "2737             newstrides[i] = PyInt_AsLong(PyTuple_GetItem(value, Py_ssize_t(i)));\n",
      "2738             //newstrides_bytes[i] = newstrides[i] * 4;\n",
      "2739         }\n",
      "2740     }else if (PyList_Check(value)){\n",
      "2741         for(int i=0; i < CudaNdarray_NDIM(self); i++){\n",
      "2742             newstrides[i] = PyInt_AsLong(PyList_GetItem(value, Py_ssize_t(i)));\n",
      "2743             //newstrides_bytes[i] = newstrides[i] * 4;\n",
      "2744         }\n",
      "2745     }\n",
      "2746     /*\n",
      "2747     // Do not do this check, as ExtractDiag needs that, and NumPy does not seem\n",
      "2748     // to do it.\n",
      "2749     npy_intp dims[PyTuple_Size(value)];\n",
      "2750     for(int i=0; i < CudaNdarray_NDIM(self); i++){\n",
      "2751         dims[i] = CudaNdarray_HOST_DIMS(self)[i];\n",
      "2752     }\n",
      "2753     if (!PyArray_CheckStrides(4,\n",
      "2754                               CudaNdarray_NDIM(self),\n",
      "2755                               0, 0,\n",
      "2756                               dims,\n",
      "2757                               newstrides_bytes)){\n",
      "2758         PyErr_SetString(PyExc_ValueError, \"bad new strides\");\n",
      "2759         return -1;\n",
      "2760         }\n",
      "2761     */\n",
      "2762     for(int i=0; i < CudaNdarray_NDIM(self); i++){\n",
      "2763         CudaNdarray_set_stride(self, i, newstrides[i]);\n",
      "2764     }\n",
      "2765     return 0;\n",
      "2766 }\n",
      "2767 \n",
      "2768 static PyObject *\n",
      "2769 CudaNdarray_get_dev_data(CudaNdarray *self, void *closure)\n",
      "2770 {\n",
      "2771     float * p =  CudaNdarray_DEV_DATA(self);\n",
      "2772     //printf(\"get_dev_data %p %li \\n\", p, (long int)p );\n",
      "2773     return PyInt_FromSize_t((size_t) CudaNdarray_DEV_DATA(self));\n",
      "2774 }\n",
      "2775 \n",
      "2776 static int\n",
      "2777 CudaNdarray_set_dev_data(CudaNdarray *self, PyObject *value, void *closure)\n",
      "2778 {\n",
      "2779     Py_ssize_t newdevdata = PyInt_AsSsize_t(value);\n",
      "2780     //printf(\"set_dev_data %p %li \\n\",(float*)newdevdata ,newdevdata);\n",
      "2781     if (PyErr_Occurred())\n",
      "2782     {\n",
      "2783         return -1;\n",
      "2784     }\n",
      "2785     return  CudaNdarray_set_device_data(self, (float*)newdevdata, (CudaNdarray*)self->base);\n",
      "2786 }\n",
      "2787 \n",
      "2788 static PyObject *\n",
      "2789 CudaNdarray_get_dtype(CudaNdarray *self, void *closure)\n",
      "2790 {\n",
      "2791     return PyString_FromString(\"float32\");\n",
      "2792 }\n",
      "2793 \n",
      "2794 static PyObject *\n",
      "2795 CudaNdarray_get_ndim(CudaNdarray *self, void *closure)\n",
      "2796 {\n",
      "2797     return PyInt_FromLong(self->nd);\n",
      "2798 }\n",
      "2799 \n",
      "2800 static PyObject *\n",
      "2801 CudaNdarray_get_base(CudaNdarray *self, void *closure)\n",
      "2802 {\n",
      "2803     PyObject * base = self->base;\n",
      "2804     if (!base)\n",
      "2805     {\n",
      "2806         // We cannot return a NULL pointer, use None instead\n",
      "2807         base = Py_None;\n",
      "2808     }\n",
      "2809     Py_INCREF(base);\n",
      "2810     return base;\n",
      "2811 }\n",
      "2812 \n",
      "2813 void put_in_dict(PyObject * dict, const char * key, int val)\n",
      "2814 {\n",
      "2815   PyObject * k = PyString_FromString(key);\n",
      "2816   PyObject * v = PyInt_FromLong(val);\n",
      "2817   PyDict_SetItem(dict, k, v);\n",
      "2818   Py_DECREF(k);\n",
      "2819   Py_DECREF(v);\n",
      "2820 }\n",
      "2821 \n",
      "2822 PyObject *\n",
      "2823 GetDeviceProperties(PyObject* _unused, PyObject* args)\n",
      "2824 {\n",
      "2825   int dev_id = -1;\n",
      "2826   if (! PyArg_ParseTuple(args, \"i\", &dev_id))\n",
      "2827     return NULL;\n",
      "2828   cudaDeviceProp deviceProp;\n",
      "2829   cudaGetDeviceProperties(&deviceProp, dev_id);\n",
      "2830 \n",
      "2831   PyObject * dict = PyDict_New();\n",
      "2832   PyObject * str= PyString_FromString(\"name\");\n",
      "2833   PyObject * i = PyString_FromString(deviceProp.name);\n",
      "2834   PyDict_SetItem(dict, str, i);\n",
      "2835   Py_DECREF(str);\n",
      "2836   Py_DECREF(i);\n",
      "2837 \n",
      "2838   put_in_dict(dict, \"major\", deviceProp.major);\n",
      "2839   put_in_dict(dict, \"minor\", deviceProp.minor);\n",
      "2840 #if CUDART_VERSION >= 2020\n",
      "2841   int driverVersion = 0, runtimeVersion = 0;\n",
      "2842   cudaDriverGetVersion(&driverVersion);\n",
      "2843   cudaRuntimeGetVersion(&runtimeVersion);\n",
      "2844   put_in_dict(dict, \"driverVersion\", driverVersion);\n",
      "2845   put_in_dict(dict, \"runtimeVersion\", runtimeVersion);\n",
      "2846 #endif\n",
      "2847 #if CUDART_VERSION >= 2000\n",
      "2848 \n",
      "2849   put_in_dict(dict, \"multiProcessorCount\", deviceProp.multiProcessorCount);\n",
      "2850   //if ConvertSMVer2Cores is not defined in cuda_runtime_api.h, the run time is too old.\n",
      "2851   int sm_cores = -1;\n",
      "2852   if(deviceProp.major==1)\n",
      "2853     sm_cores = 32;\n",
      "2854   else if(deviceProp.major==2 && deviceProp.minor==0)\n",
      "2855     sm_cores = 32;\n",
      "2856   else if(deviceProp.major==2 && deviceProp.minor==1)\n",
      "2857     sm_cores = 48;\n",
      "2858   put_in_dict(dict, \"coresCount\", sm_cores * deviceProp.multiProcessorCount);\n",
      "2859 #endif\n",
      "2860   put_in_dict(dict, \"totalConstMem\", deviceProp.totalConstMem);\n",
      "2861   put_in_dict(dict, \"sharedMemPerBlock\", deviceProp.sharedMemPerBlock);\n",
      "2862   put_in_dict(dict, \"regsPerBlock\", deviceProp.regsPerBlock);\n",
      "2863   put_in_dict(dict, \"warpSize\", deviceProp.warpSize);\n",
      "2864   put_in_dict(dict, \"maxThreadsPerBlock\", deviceProp.maxThreadsPerBlock);\n",
      "2865   put_in_dict(dict, \"maxThreadsDim0\", deviceProp.maxThreadsDim[0]);\n",
      "2866   put_in_dict(dict, \"maxThreadsDim1\", deviceProp.maxThreadsDim[1]);\n",
      "2867   put_in_dict(dict, \"maxThreadsDim2\", deviceProp.maxThreadsDim[2]);\n",
      "2868   put_in_dict(dict, \"maxGridSize0\", deviceProp.maxGridSize[0]);\n",
      "2869   put_in_dict(dict, \"maxGridSize1\", deviceProp.maxGridSize[1]);\n",
      "2870   put_in_dict(dict, \"maxGridSize2\", deviceProp.maxGridSize[2]);\n",
      "2871   put_in_dict(dict, \"memPitch\", deviceProp.memPitch);\n",
      "2872   put_in_dict(dict, \"textureAlignment\", deviceProp.textureAlignment);\n",
      "2873   put_in_dict(dict, \"clockRate\", deviceProp.clockRate);\n",
      "2874 #if CUDART_VERSION >= 2000\n",
      "2875   put_in_dict(dict, \"deviceOverlap\", deviceProp.deviceOverlap);\n",
      "2876 #endif\n",
      "2877 #if CUDART_VERSION >= 2020\n",
      "2878   put_in_dict(dict, \"kernelExecTimeoutEnabled\", deviceProp.kernelExecTimeoutEnabled);\n",
      "2879   put_in_dict(dict, \"integrated\", deviceProp.integrated);\n",
      "2880   put_in_dict(dict, \"canMapHostMemory\", deviceProp.canMapHostMemory);\n",
      "2881   put_in_dict(dict, \"computeMode\", deviceProp.computeMode);\n",
      "2882   //in the doc of this fct tell that 0 - Normal mode, 1 - only 1 context, 2 - no context\n",
      "2883 #endif\n",
      "2884 #if CUDART_VERSION >= 3000\n",
      "2885   put_in_dict(dict, \"concurrentKernels\", deviceProp.concurrentKernels);\n",
      "2886 #endif\n",
      "2887 #if CUDART_VERSION >= 3010\n",
      "2888   put_in_dict(dict, \"ECCEnabled\", deviceProp.ECCEnabled);\n",
      "2889 #endif\n",
      "2890 #if CUDART_VERSION >= 3020\n",
      "2891   put_in_dict(dict, \"tccDriver\", deviceProp.tccDriver);\n",
      "2892 #endif\n",
      "2893 \n",
      "2894   return dict;\n",
      "2895 }\n",
      "2896 \n",
      "2897 /*\n",
      "2898  * Returns in *free and *total respectively, the free and total amount of memory available for allocation by the device in bytes.\n",
      "2899  */\n",
      "2900 PyObject *\n",
      "2901 GetDeviceMemInfo(PyObject* _unused, PyObject* dummy)\n",
      "2902 {\n",
      "2903     size_t free = 0, total = 0;\n",
      "2904     if(g_gpu_context_active == 0){\n",
      "2905         PyErr_Format(PyExc_RuntimeError, \"No gpu device selected yet. Please make sure the gpu device was initialized by Theano before.\");\n",
      "2906         return NULL;\n",
      "2907     }\n",
      "2908 \n",
      "2909     cudaError_t err = cudaMemGetInfo(&free, &total);\n",
      "2910     if (err != cudaSuccess){\n",
      "2911         // Clear the error flag, cudaMemGetInfo doesn't do it.\n",
      "2912         // Currently this returns the same thing as err, but if in future\n",
      "2913         // it returns something else I still don't see why we should ignore\n",
      "2914         // it.  All we want to do here is reset the flag.\n",
      "2915         cudaGetLastError();\n",
      "2916         PyErr_Format(PyExc_RuntimeError,\n",
      "2917                      \"Error while getting memory info about the gpu: %s\",\n",
      "2918                      cudaGetErrorString(err));\n",
      "2919         return NULL;\n",
      "2920     }\n",
      "2921     return PyTuple_Pack(2, PyLong_FromLong(free), PyLong_FromLong(total));\n",
      "2922 }\n",
      "2923 \n",
      "2924 /*\n",
      "2925  * Synchronize with all the gpu device stream.\n",
      "2926  */\n",
      "2927 PyObject *\n",
      "2928 CudaNdarray_synchronize(PyObject* _unused, PyObject* dummy)\n",
      "2929 {\n",
      "2930     CNDA_BEGIN_ALLOW_THREADS\n",
      "2931     cudaThreadSynchronize();\n",
      "2932     CNDA_END_ALLOW_THREADS\n",
      "2933     Py_INCREF(Py_None);\n",
      "2934     return Py_None;\n",
      "2935 }\n",
      "2936 \n",
      "2937 /*\n",
      "2938  * Exist and return true if we link with cublas v2.\n",
      "2939  */\n",
      "2940 PyObject *\n",
      "2941 CudaNdarray_cublasv2(PyObject* _unused, PyObject* dummy)\n",
      "2942 {\n",
      "2943     Py_INCREF(Py_True);\n",
      "2944     return Py_True;\n",
      "2945 }\n",
      "2946 \n",
      "2947 PyObject *\n",
      "2948 CudaNdarray_select_a_gpu(PyObject* _unused, PyObject* dummy)\n",
      "2949 {\n",
      "2950     void * rval = NULL;\n",
      "2951     cudaError_t err;\n",
      "2952     int num_gpus = 0;\n",
      "2953 \n",
      "2954     err = cudaGetDeviceCount(&num_gpus);\n",
      "2955     if (cudaSuccess != err){\n",
      "2956         printf(\"ERR!\\\\n\");\n",
      "2957             PyErr_Format(PyExc_RuntimeError,\n",
      "2958                          \"Not able to get number of GPUs (%s).\",\n",
      "2959                          cudaGetErrorString(err));\n",
      "2960             return NULL;\n",
      "2961     }\n",
      "2962 \n",
      "2963     for (int device = 0; device < num_gpus; device++) {\n",
      "2964         cudaSetDevice(device);\n",
      "2965         err = cudaDeviceSynchronize(); // << CUDA context gets created here.\n",
      "2966         cudaGetLastError(); // reset the error state     \n",
      "2967         if (cudaSuccess == err)\n",
      "2968             break;\n",
      "2969     }\n",
      "2970         \n",
      "2971     if (cudaSuccess != err){\n",
      "2972             printf(\"ERR!\\\\n\");\n",
      "2973                 PyErr_Format(PyExc_RuntimeError,\n",
      "2974                              \"Not able to select available GPU from %d cards (%s).\",\n",
      "2975                              num_gpus, cudaGetErrorString(err));\n",
      "2976                 return NULL;\n",
      "2977     }\n",
      "2978 \n",
      "2979     Py_INCREF(Py_None);\n",
      "2980     return Py_None;\n",
      "2981 }\n",
      "2982 \n",
      "2983 #if COMPUTE_GPU_MEM_USED\n",
      "2984 /*\n",
      "2985  * Return the size in bytes that Theano currently have allocated on the gpu.\n",
      "2986  */\n",
      "2987 PyObject *\n",
      "2988 GetTheanoAllocInfo(PyObject* _unused, PyObject* dummy)\n",
      "2989 {\n",
      "2990     PyObject* a = PyLong_FromLong(_allocated_size);\n",
      "2991     PyObject* b = PyLong_FromLong(_max_allocated_size);\n",
      "2992 \n",
      "2993     PyObject* tuple = PyTuple_New(2);\n",
      "2994     PyTuple_SetItem(tuple, 0, a);\n",
      "2995     PyTuple_SetItem(tuple, 1, b);\n",
      "2996     return tuple;\n",
      "2997 }\n",
      "2998 #endif\n",
      "2999 \n",
      "3000 static PyGetSetDef CudaNdarray_getset[] = {\n",
      "3001     {\"shape\",\n",
      "3002         (getter)CudaNdarray_get_shape,\n",
      "3003         (setter)CudaNdarray_set_shape,\n",
      "3004         \"shape of this ndarray (tuple)\",\n",
      "3005         NULL},\n",
      "3006     {\"_strides\",\n",
      "3007         (getter)CudaNdarray_get_strides,\n",
      "3008         (setter)CudaNdarray_set_strides,\n",
      "3009         \"data pointer strides (in elements)\",\n",
      "3010         NULL},\n",
      "3011     {\"strides\",\n",
      "3012         (getter)CudaNdarray_get_strides,\n",
      "3013         (setter)CudaNdarray_set_strides,\n",
      "3014         \"data pointer strides (in elements)\",\n",
      "3015         NULL},\n",
      "3016     //gpudata is needed to allow calling pycuda fct with CudaNdarray input.\n",
      "3017     {\"gpudata\",\n",
      "3018         (getter)CudaNdarray_get_dev_data,\n",
      "3019         NULL,\n",
      "3020         \"device data pointer\",\n",
      "3021         NULL},\n",
      "3022     {\"_dev_data\",\n",
      "3023         (getter)CudaNdarray_get_dev_data,\n",
      "3024         (setter)CudaNdarray_set_dev_data,\n",
      "3025         \"device data pointer\",\n",
      "3026         NULL},\n",
      "3027     {\"dtype\",\n",
      "3028         (getter)CudaNdarray_get_dtype,\n",
      "3029         NULL,\n",
      "3030         \"The dtype of the element. Now always float32\",\n",
      "3031         NULL},\n",
      "3032     {\"size\",\n",
      "3033         (getter)CudaNdarray_SIZE_Object,\n",
      "3034         NULL,\n",
      "3035         \"The number of elements in this object.\",\n",
      "3036         NULL},\n",
      "3037     //mem_size is neede for pycuda.elementwise.ElementwiseKernel Why do they use size and mem_size of the same value?\n",
      "3038     {\"mem_size\",\n",
      "3039         (getter)CudaNdarray_SIZE_Object,\n",
      "3040         NULL,\n",
      "3041         \"The number of elements in this object.\",\n",
      "3042         NULL},\n",
      "3043     {\"ndim\",\n",
      "3044         (getter)CudaNdarray_get_ndim,\n",
      "3045         NULL,\n",
      "3046         \"The number of dimensions in this object.\",\n",
      "3047         NULL},\n",
      "3048     {\"base\",\n",
      "3049         (getter)CudaNdarray_get_base,\n",
      "3050         NULL,\n",
      "3051         \"If this ndarray is a view, base is the original ndarray.\",\n",
      "3052         NULL},\n",
      "3053 \n",
      "3054     {NULL, NULL, NULL, NULL}  /* Sentinel */\n",
      "3055 };\n",
      "3056 \n",
      "3057 PyObject *CudaNdarray_repr(PyObject *self)\n",
      "3058 {\n",
      "3059     CudaNdarray *object = (CudaNdarray *)self;\n",
      "3060     PyObject * np_object = CudaNdarray_CreateArrayObj(object);\n",
      "3061     PyObject * str = PyObject_Str((PyObject *) np_object);\n",
      "3062     char * cstr = PyString_AsString(str);\n",
      "3063     PyObject * out = PyString_FromFormat(\"%s%s%s\",\n",
      "3064                         \"CudaNdarray(\",\n",
      "3065                         cstr,\n",
      "3066                         \")\");\n",
      "3067     Py_DECREF(str);\n",
      "3068     Py_DECREF(np_object);\n",
      "3069     #if PY_MAJOR_VERSION >= 3\n",
      "3070     // In Python 3 PyString_FromFormat return a Bytes object\n",
      "3071     PyObject* out2 = PyObject_Str(out);\n",
      "3072     Py_DECREF(out);\n",
      "3073     return out2;\n",
      "3074     #endif\n",
      "3075     return out;\n",
      "3076 }\n",
      "3077 \n",
      "3078 static PyTypeObject CudaNdarrayType =\n",
      "3079 {\n",
      "3080 #if PY_MAJOR_VERSION >= 3\n",
      "3081     PyVarObject_HEAD_INIT(NULL, 0)\n",
      "3082 #else\n",
      "3083     PyObject_HEAD_INIT(NULL)\n",
      "3084     0,                         /*ob_size*/\n",
      "3085 #endif\n",
      "3086     \"CudaNdarray\",             /*tp_name*/\n",
      "3087     sizeof(CudaNdarray),       /*tp_basicsize*/\n",
      "3088     0,                         /*tp_itemsize*/\n",
      "3089     (destructor)CudaNdarray_dealloc, /*tp_dealloc*/\n",
      "3090     0,                         /*tp_print*/\n",
      "3091     0,                         /*tp_getattr*/\n",
      "3092     0,                         /*tp_setattr*/\n",
      "3093     0,                         /*tp_compare*/\n",
      "3094     CudaNdarray_repr,          /*tp_repr*/\n",
      "3095     &CudaNdarrayNumberMethods, /*tp_as_number*/\n",
      "3096     0,                         /*tp_as_sequence*/\n",
      "3097     &CudaNdarrayMappingMethods,/*tp_as_mapping*/\n",
      "3098     0,                         /*tp_hash */\n",
      "3099     0,                         /*tp_call*/\n",
      "3100     0,                         /*tp_str*/\n",
      "3101     0,                         /*tp_getattro*/\n",
      "3102     0,                         /*tp_setattro*/\n",
      "3103     0,                         /*tp_as_buffer*/\n",
      "3104 #if PY_MAJOR_VERSION >= 3\n",
      "3105     // Py_TPFLAGS_CHECKTYPES is always true and was removed in Python 3.\n",
      "3106     Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE, /*tp_flags*/\n",
      "3107 #else\n",
      "3108     Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE | Py_TPFLAGS_CHECKTYPES, /*tp_flags*/\n",
      "3109 #endif\n",
      "3110     \"CudaNdarray objects\",     /* tp_doc */\n",
      "3111     0,                         /* tp_traverse */\n",
      "3112     0,                         /* tp_clear */\n",
      "3113     0,                         /* tp_richcompare */\n",
      "3114     0,                         /* tp_weaklistoffset */\n",
      "3115     0,                         /* tp_iter */\n",
      "3116     0,                         /* tp_iternext */\n",
      "3117     CudaNdarray_methods,       /* tp_methods */\n",
      "3118     CudaNdarray_members,       /* tp_members */\n",
      "3119     CudaNdarray_getset,        /* tp_getset */\n",
      "3120     0,                         /* tp_base */\n",
      "3121     0,                         /* tp_dict */\n",
      "3122     0,                         /* tp_descr_get */\n",
      "3123     0,                         /* tp_descr_set */\n",
      "3124     0,                         /* tp_dictoffset */\n",
      "3125     (initproc)CudaNdarray_init,/* tp_init */\n",
      "3126     0,                         /* tp_alloc */\n",
      "3127     CudaNdarray_new,           /* tp_new */\n",
      "3128 };\n",
      "3129 \n",
      "3130 static __global__ void get_gpu_ptr_size(int* dst)\n",
      "3131 {\n",
      "3132     dst[0] = sizeof(float*);\n",
      "3133     dst[1] = sizeof(int);\n",
      "3134 }\n",
      "3135 \n",
      "3136 PyObject *\n",
      "3137 CudaNdarray_ptr_int_size(PyObject* _unused, PyObject* args)\n",
      "3138 {\n",
      "3139     int *gpu_data = (int*)device_malloc(sizeof(int)*2);\n",
      "3140     if(gpu_data == NULL){\n",
      "3141         return NULL;\n",
      "3142     }\n",
      "3143     get_gpu_ptr_size<<<1,1>>>(gpu_data);\n",
      "3144 \n",
      "3145     cudaError_t cudaErr = cudaGetLastError();\n",
      "3146     if (cudaSuccess != cudaErr){\n",
      "3147 \n",
      "3148         device_free(gpu_data);\n",
      "3149         return PyErr_Format(PyExc_RuntimeError,\n",
      "3150                             \"CudaNdarray_ptr_int_size: error when calling the gpu code. (%s)\",\n",
      "3151                             cudaGetErrorString(cudaErr));\n",
      "3152     }\n",
      "3153 \n",
      "3154     // Transfer the result to cpu\n",
      "3155     int gpu_sizes[] = {-1,-1};\n",
      "3156     cublasStatus_t err;\n",
      "3157     err = cublasGetVector(2, sizeof(int), gpu_data, 1, gpu_sizes, 1);\n",
      "3158     device_free(gpu_data);\n",
      "3159 \n",
      "3160     if (CUBLAS_STATUS_SUCCESS != err){\n",
      "3161         PyErr_SetString(PyExc_RuntimeError, \"error copying data to from memory\");\n",
      "3162         return NULL;\n",
      "3163     }\n",
      "3164     return Py_BuildValue(\"iiii\", (int) gpu_sizes[0], (int)sizeof(float*),\n",
      "3165                          (int)sizeof(int), (int) gpu_sizes[1]);\n",
      "3166 }\n",
      "3167 \n",
      "3168 static int cublas_init();\n",
      "3169 static void cublas_shutdown();\n",
      "3170 // Initialize the gpu.\n",
      "3171 // Takes two optional parameters, the device number and if we should use cnmem.\n",
      "3172 // If the device number is provided, it sets that device to be the active device.\n",
      "3173 // If not provided (usually just to test whether the gpu is available at all),\n",
      "3174 // it does not set an active device.\n",
      "3175 // Raises EnvironmentError or ValueError (as appropriate) if the initialization failed.\n",
      "3176 // cnmem is threaded like a bool. If converted to 0, don't use cnmem. Otherwise, use it.\n",
      "3177 PyObject *\n",
      "3178 CudaNdarray_gpu_init(PyObject* _unused, PyObject* args)\n",
      "3179 {\n",
      "3180     int card_nb = 0;\n",
      "3181     int card_number_provided = 1;\n",
      "3182     float cnmem = 0; // Theano flag lib.cnmem\n",
      "3183     // if we're given something wildly invalid, this will throw a TypeError\n",
      "3184     if(!PyArg_ParseTuple(args, \"|if\", &card_nb, &cnmem))\n",
      "3185         return NULL;\n",
      "3186     if(cnmem)\n",
      "3187         g_use_cnmem = true;\n",
      "3188 \n",
      "3189     if(PyTuple_Size(args) == 0) {\n",
      "3190         card_number_provided = 0;\n",
      "3191         card_nb = 0;\n",
      "3192     }\n",
      "3193 \n",
      "3194     int deviceCount;\n",
      "3195     cudaError err = cudaGetDeviceCount(&deviceCount);\n",
      "3196     if(cudaSuccess != err) {\n",
      "3197         return PyErr_Format(PyExc_EnvironmentError,\n",
      "3198                             \"Unable to get the number of gpus available: %s\",\n",
      "3199                             cudaGetErrorString(cudaGetLastError()));\n",
      "3200     }\n",
      "3201 \n",
      "3202     // as soon as the first successful call to a cuda* function is made, a\n",
      "3203     // gpu context has been created\n",
      "3204     g_gpu_context_active = 1;\n",
      "3205 \n",
      "3206     if(deviceCount <= 0) {\n",
      "3207         return PyErr_Format(PyExc_EnvironmentError,\n",
      "3208                             \"Can't use the GPU, no devices support CUDA\");\n",
      "3209     }\n",
      "3210     if(card_number_provided && (card_nb < 0 || card_nb > (deviceCount - 1))) {\n",
      "3211         return PyErr_Format(PyExc_ValueError,\n",
      "3212                             \"Bad device number %d. Only %d devices available.\",\n",
      "3213                             card_nb,\n",
      "3214                             deviceCount);\n",
      "3215     }\n",
      "3216 \n",
      "3217     cudaDeviceProp deviceProp;\n",
      "3218     err = cudaGetDeviceProperties(&deviceProp, card_nb);\n",
      "3219     if(cudaSuccess != err) {\n",
      "3220         return PyErr_Format(PyExc_EnvironmentError,\n",
      "3221                             \"Unable to get properties of gpu %i: %s\",\n",
      "3222                             card_nb,\n",
      "3223                             cudaGetErrorString(cudaGetLastError()));\n",
      "3224     }\n",
      "3225 \n",
      "3226     if(deviceProp.major == 9999 && deviceProp.minor == 9999 ){\n",
      "3227         return PyErr_Format(PyExc_EnvironmentError,\n",
      "3228                             \"There is no device that supports CUDA\");\n",
      "3229     }\n",
      "3230 \n",
      "3231     if(card_number_provided) {\n",
      "3232         err = cudaSetDevice(card_nb);\n",
      "3233         if(cudaSuccess != err) {\n",
      "3234             return PyErr_Format(PyExc_EnvironmentError,\n",
      "3235                                 \"Unable to set device %i: %s\",\n",
      "3236                                 card_nb,\n",
      "3237                                 cudaGetErrorString(cudaGetLastError()));\n",
      "3238         }\n",
      "3239         if (cublas_init() == -1)\n",
      "3240             return NULL;\n",
      "3241     }\n",
      "3242     if(card_number_provided && g_use_cnmem) {\n",
      "3243         size_t mem = 0;\n",
      "3244         if (cnmem > 1)\n",
      "3245             mem = cnmem * 1024 * 1024;\n",
      "3246         else{\n",
      "3247             // Clip to 95% to let memory for the driver.\n",
      "3248             // 98% didn't worked in some cases.\n",
      "3249             if (cnmem > .95){\n",
      "3250                 cnmem = .95;\n",
      "3251             }\n",
      "3252             size_t free = 0, total = 0;\n",
      "3253             cudaError_t err = cudaMemGetInfo(&free, &total);\n",
      "3254             if (err != cudaSuccess){\n",
      "3255                 // Clear the error flag, cudaMemGetInfo doesn't do it.\n",
      "3256                 // Currently this returns the same thing as err, but if in future\n",
      "3257                 // it returns something else I still don't see why we should ignore\n",
      "3258                 // it.  All we want to do here is reset the flag.\n",
      "3259                 cudaGetLastError();\n",
      "3260                 PyErr_Format(PyExc_RuntimeError,\n",
      "3261                              \"Error while getting memory info about the gpu: %s\",\n",
      "3262                              cudaGetErrorString(err));\n",
      "3263                 return NULL;\n",
      "3264             }\n",
      "3265             mem = total * cnmem;\n",
      "3266         }\n",
      "3267         if(initCnmem(card_number_provided, card_nb, mem) == -1){\n",
      "3268             return NULL;\n",
      "3269         }\n",
      "3270     }\n",
      "3271 \n",
      "3272     Py_INCREF(Py_None);\n",
      "3273     return Py_None;\n",
      "3274 }\n",
      "3275 \n",
      "3276 PyObject *\n",
      "3277 CudaNdarray_active_device_number(PyObject* _unused, PyObject* _unused_args) {\n",
      "3278     // NB: No cuda error checking here; keeps things simple, and it's not\n",
      "3279     // really necessary.\n",
      "3280     int currentDevice;\n",
      "3281     cudaGetDevice(&currentDevice);\n",
      "3282     return PyInt_FromLong(currentDevice);\n",
      "3283 }\n",
      "3284 \n",
      "3285 PyObject *\n",
      "3286 CudaNdarray_active_device_name(PyObject* _unused, PyObject* _unused_args) {\n",
      "3287     // NB: No cuda error checking here; keeps things simple, and it's not\n",
      "3288     // really necessary.\n",
      "3289     int currentDevice;\n",
      "3290     cudaGetDevice(&currentDevice);\n",
      "3291 \n",
      "3292     cudaDeviceProp deviceProp;\n",
      "3293     cudaGetDeviceProperties(&deviceProp, currentDevice);\n",
      "3294     return PyString_FromString(deviceProp.name);\n",
      "3295 }\n",
      "3296 \n",
      "3297 PyObject *\n",
      "3298 CudaNdarray_gpu_shutdown(PyObject* _unused, PyObject* _unused_args) {\n",
      "3299     // Don't handle errors here\n",
      "3300     cublas_shutdown();\n",
      "3301     g_gpu_context_active = 0; // context has now been closed down\n",
      "3302     if(g_use_cnmem) {\n",
      "3303         cnmemStatus_t status = cnmemFinalize();\n",
      "3304         if(status != CNMEM_STATUS_SUCCESS) {\n",
      "3305             fprintf(stderr, \"CudaNdarray_gpu_shutdown: cnmemFinalize failed! Reason=%s\\n\",\n",
      "3306                     cnmemGetErrorString(status));\n",
      "3307             if(status == CNMEM_STATUS_CUDA_ERROR) {\n",
      "3308                 fprintf(stderr, \"  Cuda-Reason=%s\\n\",\n",
      "3309                         cudaGetErrorString(cudaGetLastError()));\n",
      "3310             }\n",
      "3311         }\n",
      "3312     }\n",
      "3313     cudaThreadExit();\n",
      "3314 \n",
      "3315     Py_INCREF(Py_None);\n",
      "3316     return Py_None;\n",
      "3317 }\n",
      "3318 \n",
      "3319 /*\n",
      "3320  * This function is tested in theano/misc/test_pycuda_theano_simple.py\n",
      "3321  */\n",
      "3322 PyObject *\n",
      "3323 CudaNdarray_from_gpu_pointer(PyObject* _unused, PyObject* args)\n",
      "3324 {\n",
      "3325     int verbose = 0;\n",
      "3326     PyObject *gpu_ptr = NULL;\n",
      "3327     PyObject *shapes = NULL;\n",
      "3328     PyObject *strides = NULL;\n",
      "3329     PyObject *base = NULL;\n",
      "3330     PyObject *rval = NULL;\n",
      "3331 \n",
      "3332     //args should consist of 3 python objects\n",
      "3333     //The first is the gpu ptr\n",
      "3334     //The second if the shape\n",
      "3335     //The third if the strides\n",
      "3336     if (! PyArg_ParseTuple(args, \"OOOO\", &gpu_ptr, &shapes, &strides, &base))\n",
      "3337         return NULL;\n",
      "3338 \n",
      "3339     if (verbose) printf(\"In CudaNdarray_from_gpu_pointer\\n\");\n",
      "3340     if (!PyLong_Check(gpu_ptr))\n",
      "3341     {\n",
      "3342         PyErr_Format(PyExc_Exception, \"CudaNdarray_from_gpu_pointer: The gpu pointor is not an long\");\n",
      "3343         return NULL;\n",
      "3344     }\n",
      "3345 \n",
      "3346     Py_ssize_t nd =  PyObject_Length(shapes);\n",
      "3347     if (nd < 0)\n",
      "3348     {\n",
      "3349         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_from_gpu_pointer: Couldn't get length of second argument\");\n",
      "3350         return NULL;\n",
      "3351     }\n",
      "3352     Py_ssize_t nd_stride =  PyObject_Length(strides);\n",
      "3353     if (nd_stride < 0)\n",
      "3354     {\n",
      "3355         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_from_gpu_pointer: Couldn't get length of third argument\");\n",
      "3356         return NULL;\n",
      "3357     }\n",
      "3358 \n",
      "3359     if (nd != nd_stride)\n",
      "3360     {\n",
      "3361         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_from_gpu_pointer: We need the same number of shapes and strides\");\n",
      "3362         return NULL;\n",
      "3363     }\n",
      "3364 \n",
      "3365     rval = CudaNdarray_New();\n",
      "3366 \n",
      "3367     if (CudaNdarray_set_nd((CudaNdarray *)rval, nd))\n",
      "3368     {\n",
      "3369         //CudaNdarray_set_nd set the error msg\n",
      "3370         return NULL;\n",
      "3371     }\n",
      "3372     // set gpu pointeur\n",
      "3373     assert(((CudaNdarray *)rval)->data_allocated == 0);\n",
      "3374     if (CudaNdarray_set_device_data((CudaNdarray *)rval, (float *)PyInt_AsLong(gpu_ptr), base))\n",
      "3375     {\n",
      "3376         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_from_gpu_pointer: Error while setting the gpu pointor\");\n",
      "3377         return NULL;\n",
      "3378 \n",
      "3379     }\n",
      "3380 \n",
      "3381     // Set dims and strides\n",
      "3382     for (int i = nd-1; i >= 0; --i)\n",
      "3383     {\n",
      "3384         PyObject * idx = PyLong_FromLong(i);\n",
      "3385         if (idx == NULL)\n",
      "3386         {\n",
      "3387             PyErr_SetString(PyExc_Exception, \"CudaNdarray_from_gpu_pointer: Couldn't make long object to loop over list/tuple\");\n",
      "3388             return NULL;\n",
      "3389         }\n",
      "3390         PyObject* dim_ = PyObject_GetItem(shapes, idx);\n",
      "3391         PyObject* strd_ = PyObject_GetItem(strides, idx);\n",
      "3392         if (!PyInt_Check(dim_))\n",
      "3393         {\n",
      "3394             PyErr_Format(PyExc_Exception, \"CudaNdarray_from_gpu_pointer: shapes[%d] is not an int\", i);\n",
      "3395             return NULL;\n",
      "3396         }\n",
      "3397         if (!PyInt_Check(strd_))\n",
      "3398         {\n",
      "3399             PyErr_Format(PyExc_Exception, \"CudaNdarray_from_gpu_pointer: strides[%d] is not an int\", i);\n",
      "3400             return NULL;\n",
      "3401         }\n",
      "3402         int dim = PyInt_AsLong(dim_);\n",
      "3403         int strd = PyInt_AsLong(strd_);\n",
      "3404         CudaNdarray_set_stride((CudaNdarray *)rval, i, strd);\n",
      "3405         CudaNdarray_set_dim((CudaNdarray *)rval, i, dim);\n",
      "3406         Py_DECREF(idx);\n",
      "3407         Py_DECREF(dim_);\n",
      "3408         Py_DECREF(strd_);\n",
      "3409     }\n",
      "3410     if (verbose) printf(\"CudaNdarray_from_gpu_pointer normal return\\n\");\n",
      "3411     return rval;\n",
      "3412 }\n",
      "3413 \n",
      "3414 PyObject *\n",
      "3415 CudaNdarray_Dot(PyObject* _unused, PyObject* args)\n",
      "3416 {\n",
      "3417     PyObject *l=NULL;\n",
      "3418     PyObject *r=NULL;\n",
      "3419     PyObject * rval = NULL;\n",
      "3420 \n",
      "3421     //args should consist of two python objects (\"OO\")\n",
      "3422     if (! PyArg_ParseTuple(args, \"OO\", &l, &r))\n",
      "3423         return NULL;\n",
      "3424 \n",
      "3425     if (!CudaNdarray_Check(l) || !CudaNdarray_Check(r))\n",
      "3426     {\n",
      "3427         PyErr_SetString(PyExc_TypeError, \"CudaNdarray arguments required \");\n",
      "3428         goto CudaNdarray_dot_fail;\n",
      "3429     }\n",
      "3430     if (((CudaNdarray*)l)->nd != 2)\n",
      "3431     {\n",
      "3432         PyErr_SetString(PyExc_TypeError, \"need 2d CudaNdarray arg for now\");\n",
      "3433         goto CudaNdarray_dot_fail;\n",
      "3434     }\n",
      "3435     if (((CudaNdarray*)r)->nd != 2)\n",
      "3436     {\n",
      "3437         PyErr_SetString(PyExc_TypeError, \"need 2d CudaNdarray arg for now\");\n",
      "3438         goto CudaNdarray_dot_fail;\n",
      "3439     }\n",
      "3440     rval = CudaNdarray_New();\n",
      "3441     if (!rval)\n",
      "3442     {\n",
      "3443         goto CudaNdarray_dot_fail;\n",
      "3444     }\n",
      "3445     int dims[2];\n",
      "3446     dims[0] = CudaNdarray_HOST_DIMS((CudaNdarray*)l)[0];\n",
      "3447     dims[1] = CudaNdarray_HOST_DIMS((CudaNdarray*)r)[1];\n",
      "3448     if (CudaNdarray_alloc_contiguous((CudaNdarray*)rval, 2, dims))\n",
      "3449     {\n",
      "3450         goto CudaNdarray_dot_fail;\n",
      "3451     }\n",
      "3452     if (CudaNdarray_gemm(1.0, (CudaNdarray*)l, (CudaNdarray*)r, 0.0, (CudaNdarray*)rval))\n",
      "3453     {\n",
      "3454         goto CudaNdarray_dot_fail;\n",
      "3455     }\n",
      "3456 \n",
      "3457     return rval;\n",
      "3458 \n",
      "3459     CudaNdarray_dot_fail:\n",
      "3460     Py_XDECREF(rval);\n",
      "3461     return NULL;\n",
      "3462 }\n",
      "3463 \n",
      "3464 static PyObject *\n",
      "3465 filter(PyObject* __unsed_self, PyObject *args) // args = (data, broadcastable, strict, storage)\n",
      "3466 {\n",
      "3467     /*\n",
      "3468      * TODO: DOC what this function should do in the various cases of\n",
      "3469      * What is 'strict' supposed to mean in the context of this function?\n",
      "3470      * What do we do with input that could be interpreted as matching the broadcastable pattern in strict vs. non-strict cases?\n",
      "3471      *\n",
      "3472      */\n",
      "3473     PyObject *py_data=NULL;\n",
      "3474     PyArrayObject * data = NULL;\n",
      "3475     int strict = 0;\n",
      "3476     PyObject * broadcastable=NULL;\n",
      "3477     PyObject * storage=NULL;\n",
      "3478     CudaNdarray * rval=NULL;\n",
      "3479 \n",
      "3480     //Python object references which are provided to the caller are borrowed references\n",
      "3481     if (!PyArg_ParseTuple(args, \"OOiO\", &py_data, &broadcastable, &strict, &storage)) return NULL;\n",
      "3482 \n",
      "3483     if (!PyTuple_Check(broadcastable)){\n",
      "3484         PyErr_SetString(PyExc_TypeError, \"broadcastable arg should be a tuple of int.\");\n",
      "3485         return NULL;\n",
      "3486     }\n",
      "3487     Py_INCREF(py_data);\n",
      "3488     Py_INCREF(broadcastable);\n",
      "3489 \n",
      "3490     CudaNdarray * cnda = (CudaNdarray*)py_data;\n",
      "3491 \n",
      "3492     if (strict || CudaNdarray_Check(py_data))\n",
      "3493     {\n",
      "3494         //TODO: support non-strict \"casting\" from a vt to the broadcastable/type/size that we need.\n",
      "3495         if (!CudaNdarray_Check(py_data))\n",
      "3496         {\n",
      "3497             Py_DECREF(py_data);\n",
      "3498             Py_DECREF(broadcastable);\n",
      "3499             PyErr_SetString(PyExc_TypeError, \"strict mode requires CudaNdarray\");\n",
      "3500             return NULL;\n",
      "3501         }\n",
      "3502         if (cnda->nd != PyTuple_Size(broadcastable))\n",
      "3503         {\n",
      "3504             Py_DECREF(py_data);\n",
      "3505             Py_DECREF(broadcastable);\n",
      "3506             PyErr_Format(PyExc_TypeError, \"Wrong rank: %i vs %li\", cnda->nd, (long)PyTuple_Size(broadcastable));\n",
      "3507             return NULL;\n",
      "3508         }\n",
      "3509         for (int i = 0; i < cnda->nd; ++i)\n",
      "3510         {\n",
      "3511             if ((CudaNdarray_HOST_DIMS(cnda)[i] > 1) && PyInt_AsLong(PyTuple_GetItem(broadcastable, Py_ssize_t(i))))\n",
      "3512             {\n",
      "3513                 PyErr_Format(PyExc_TypeError, \"Non-unit size in broadcastable vt dimension %i\", i);\n",
      "3514                 Py_DECREF(py_data);\n",
      "3515                 Py_DECREF(broadcastable);\n",
      "3516                 return NULL;\n",
      "3517             }else if (CudaNdarray_HOST_DIMS(cnda)[i] == 1 && CudaNdarray_HOST_STRIDES(cnda)[i] != 0){\n",
      "3518                 PyErr_Format(PyExc_TypeError, \"Non-zeros strides(%d) on dimension %d of size 1\",\n",
      "3519                              CudaNdarray_HOST_STRIDES(cnda)[i], i);\n",
      "3520                 Py_DECREF(py_data);\n",
      "3521                 Py_DECREF(broadcastable);\n",
      "3522                 return NULL;\n",
      "3523             }\n",
      "3524         }\n",
      "3525         Py_DECREF(broadcastable);\n",
      "3526         return py_data;\n",
      "3527     }\n",
      "3528     else\n",
      "3529     {\n",
      "3530         data = (PyArrayObject*)PyArray_FromObject(py_data, REAL_TYPENUM, PyTuple_Size(broadcastable), PyTuple_Size(broadcastable));\n",
      "3531         if (!data)\n",
      "3532         {\n",
      "3533             //err message already defined\n",
      "3534             Py_DECREF(py_data);\n",
      "3535             Py_DECREF(broadcastable);\n",
      "3536             return NULL;\n",
      "3537         }\n",
      "3538         for (int i = 0; i < PyArray_NDIM(data); ++i)\n",
      "3539         {\n",
      "3540             if ((PyArray_DIMS(data)[i] > 1) && PyInt_AsLong(PyTuple_GetItem(broadcastable, Py_ssize_t(i))))\n",
      "3541             {\n",
      "3542                 PyErr_Format(PyExc_TypeError, \"Non-unit size in broadcastable dimension %i\", i);\n",
      "3543                 Py_DECREF(data);\n",
      "3544                 Py_DECREF(py_data);\n",
      "3545                 Py_DECREF(broadcastable);\n",
      "3546                 return NULL;\n",
      "3547             }\n",
      "3548         }\n",
      "3549         if (storage && CudaNdarray_Check(storage))\n",
      "3550         {\n",
      "3551             rval = (CudaNdarray*) storage;\n",
      "3552             Py_INCREF(rval);\n",
      "3553         }\n",
      "3554         else\n",
      "3555         {\n",
      "3556             rval = (CudaNdarray*) CudaNdarray_New();\n",
      "3557         }\n",
      "3558         if (rval)\n",
      "3559         {\n",
      "3560             if (CudaNdarray_CopyFromArray(rval, data))\n",
      "3561             {\n",
      "3562                 Py_DECREF(rval);\n",
      "3563                 rval = NULL;\n",
      "3564             }\n",
      "3565         }\n",
      "3566         Py_DECREF(data);\n",
      "3567         Py_DECREF(py_data);\n",
      "3568         Py_DECREF(broadcastable);\n",
      "3569         return (PyObject*)rval;\n",
      "3570     }\n",
      "3571 }\n",
      "3572 \n",
      "3573 //TODO-- CudaNdarray_Dot and CudaNdarray_active_device_name are following different capitalization conventions.\n",
      "3574 //       Pick one and standardize it, this file is already annoying enough to grep through\n",
      "3575 static PyMethodDef module_methods[] = {\n",
      "3576     {\"dimshuffle\", CudaNdarray_Dimshuffle, METH_VARARGS, \"Returns the dimshuffle of a CudaNdarray.\"},\n",
      "3577     {\"dot\", CudaNdarray_Dot, METH_VARARGS, \"Returns the matrix product of two CudaNdarray arguments.\"},\n",
      "3578     {\"gpu_init\", CudaNdarray_gpu_init, METH_VARARGS, \"Select the gpu card to use; also usable to test whether CUDA is available.\"},\n",
      "3579     {\"select_a_gpu\", CudaNdarray_select_a_gpu, METH_NOARGS, \"Call this method if you want to select a GPU before gpu_init call and let the driver choose the GPU.\"},\n",
      "3580     {\"active_device_name\", CudaNdarray_active_device_name, METH_VARARGS, \"Get the name of the active device.\"},\n",
      "3581     {\"active_device_number\", CudaNdarray_active_device_number, METH_VARARGS, \"Get the number of the active device.\"},\n",
      "3582     {\"gpu_shutdown\", CudaNdarray_gpu_shutdown, METH_VARARGS, \"Shut down the gpu.\"},\n",
      "3583     {\"device_properties\", GetDeviceProperties, METH_VARARGS, \"Return a dictionary with the device properties.\"},\n",
      "3584     {\"mem_info\", GetDeviceMemInfo, METH_NOARGS, \"Return a tuple with the free and total memory on the gpu in bytes.\"},\n",
      "3585 #if COMPUTE_GPU_MEM_USED\n",
      "3586     {\"theano_allocated\", GetTheanoAllocInfo, METH_NOARGS, \"Return the size in bytes of memory Theano currently have allocated on the gpu.\"},\n",
      "3587 #endif\n",
      "3588     {\"ptr_int_size\", CudaNdarray_ptr_int_size, METH_VARARGS, \"Return a tuple with the size of gpu pointer, cpu pointer and int in bytes.\"},\n",
      "3589     {\"filter\", filter, METH_VARARGS, \"filter(obj, broadcastable, strict, storage) returns a CudaNdarray initialized to obj if it matches the constraints of broadcastable.  strict=True prevents any numeric casting. If storage is a CudaNdarray it may be overwritten and used as the return value.\"},\n",
      "3590     {\"outstanding_mallocs\", outstanding_mallocs, METH_VARARGS, \"how many more mallocs have been called than free's\"},\n",
      "3591     {\"from_gpu_pointer\", CudaNdarray_from_gpu_pointer, METH_VARARGS, \"Used to create a CudaNdarray from already allocated memory on the gpu.(example by pycuda)\"},\n",
      "3592     {\"synchronize\", CudaNdarray_synchronize, METH_NOARGS, \"Used to synchronize the device\"},\n",
      "3593     {\"cublas_v2\", CudaNdarray_cublasv2, METH_NOARGS,\n",
      "3594      \"Used to know if this version of cuda_ndarray is linked with cublas v2.\"},\n",
      "3595     {NULL, NULL, NULL, NULL}  /* Sentinel */\n",
      "3596 };\n",
      "3597 \n",
      "3598 #define CNDA_MOD_NAME \"cuda_ndarray\"\n",
      "3599 #define CNDA_DOCSTRING \"CUDA implementation of a numpy ndarray-like object.\"\n",
      "3600 \n",
      "3601 #if PY_MAJOR_VERSION == 3\n",
      "3602 static struct PyModuleDef cuda_ndarray_moduledef =\n",
      "3603 {\n",
      "3604     PyModuleDef_HEAD_INIT,\n",
      "3605     CNDA_MOD_NAME,\n",
      "3606     CNDA_DOCSTRING,\n",
      "3607     -1,     /* size of per-interpreter state of the module,\n",
      "3608                or -1 if the module keeps state in global variables. */\n",
      "3609     module_methods\n",
      "3610 };\n",
      "3611 \n",
      "3612 PyMODINIT_FUNC\n",
      "3613 PyInit_cuda_ndarray(void)\n",
      "3614 #else\n",
      "3615 PyMODINIT_FUNC\n",
      "3616 initcuda_ndarray(void)\n",
      "3617 #endif\n",
      "3618 {\n",
      "3619     import_array();\n",
      "3620 \n",
      "3621     PyObject* m;\n",
      "3622 \n",
      "3623     if (PyType_Ready(&CudaNdarrayType) < 0) {\n",
      "3624 #if PY_MAJOR_VERSION == 3\n",
      "3625         return NULL;\n",
      "3626 #else\n",
      "3627         return;\n",
      "3628 #endif\n",
      "3629     }\n",
      "3630 \n",
      "3631 #if PY_MAJOR_VERSION == 3\n",
      "3632     m = PyModule_Create(&cuda_ndarray_moduledef);\n",
      "3633 #else\n",
      "3634     m = Py_InitModule3(CNDA_MOD_NAME, module_methods, CNDA_DOCSTRING);\n",
      "3635 #endif\n",
      "3636 \n",
      "3637     if (m == NULL) {\n",
      "3638 #if PY_MAJOR_VERSION == 3\n",
      "3639         return NULL;\n",
      "3640 #else\n",
      "3641         return;\n",
      "3642 #endif\n",
      "3643     }\n",
      "3644 \n",
      "3645     Py_INCREF(&CudaNdarrayType);\n",
      "3646     PyModule_AddObject(m, \"CudaNdarray\", (PyObject *)&CudaNdarrayType);\n",
      "3647 #if COMPUTE_GPU_MEM_USED\n",
      "3648     for(int i=0;i<TABLE_SIZE;i++){\n",
      "3649         _alloc_size_table[i].ptr=NULL;\n",
      "3650         _alloc_size_table[i].size=0;\n",
      "3651     }\n",
      "3652 #endif\n",
      "3653     //    cublasInit();\n",
      "3654     //if (0&&CUBLAS_STATUS_SUCCESS != cublasGetError())\n",
      "3655     //{\n",
      "3656         //std::cerr << \"WARNING: initcuda_ndarray: error initializing device\\n\";\n",
      "3657     //}\n",
      "3658     if (0) //TODO: is this necessary?\n",
      "3659     {\n",
      "3660         int deviceId = 0; // TODO: what number goes here?\n",
      "3661         cudaSetDevice(deviceId);\n",
      "3662         cudaError_t err = cudaGetLastError();\n",
      "3663         if( cudaSuccess != err)\n",
      "3664         {\n",
      "3665             std::cerr << \"Error in SetDevice:\" << cudaGetErrorString(err) << \"\\n\";\n",
      "3666         }\n",
      "3667     }\n",
      "3668 \n",
      "3669 #if PY_MAJOR_VERSION == 3\n",
      "3670     return m;\n",
      "3671 #endif\n",
      "3672 }\n",
      "3673 \n",
      "3674 \n",
      "3675 //////////////////////////////////////\n",
      "3676 //\n",
      "3677 // C API FOR CudaNdarray\n",
      "3678 //\n",
      "3679 //////////////////////////////////////\n",
      "3680 \n",
      "3681 int\n",
      "3682 CudaNdarray_Check(const PyObject * ob)\n",
      "3683 {\n",
      "3684     //TODO: doesn't work with inheritance\n",
      "3685     return CudaNdarray_CheckExact(ob);\n",
      "3686 }\n",
      "3687 int\n",
      "3688 CudaNdarray_CheckExact(const PyObject * ob)\n",
      "3689 {\n",
      "3690     return ((Py_TYPE(ob) == &CudaNdarrayType) ? 1 : 0);\n",
      "3691 }\n",
      "3692 \n",
      "3693 PyObject *\n",
      "3694 CudaNdarray_New(int nd)\n",
      "3695 {\n",
      "3696     CudaNdarray *self = (CudaNdarray *)CudaNdarrayType.tp_alloc(&CudaNdarrayType, 0);\n",
      "3697     if (self == NULL)\n",
      "3698     {\n",
      "3699         PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_New failed to allocate self\");\n",
      "3700         return NULL;\n",
      "3701     }\n",
      "3702     CudaNdarray_null_init(self);\n",
      "3703 \n",
      "3704     if (nd == 0)\n",
      "3705     {\n",
      "3706         self->nd = 0;\n",
      "3707     }\n",
      "3708     else if (nd > 0)\n",
      "3709     {\n",
      "3710         if (CudaNdarray_set_nd(self, nd))\n",
      "3711         {\n",
      "3712             Py_DECREF(self);\n",
      "3713             return NULL;\n",
      "3714         }\n",
      "3715     }\n",
      "3716     ++_outstanding_mallocs[1];\n",
      "3717     return (PyObject *)self;\n",
      "3718 }\n",
      "3719 \n",
      "3720 \n",
      "3721 \n",
      "3722 //////////////////////////////\n",
      "3723 //\n",
      "3724 // Published helper functions\n",
      "3725 //\n",
      "3726 //////////////////////////////\n",
      "3727 \n",
      "3728 static int\n",
      "3729 cublas_init()\n",
      "3730 {\n",
      "3731     cublasStatus_t err;\n",
      "3732     err = cublasCreate(&handle);\n",
      "3733     if (CUBLAS_STATUS_SUCCESS != err)\n",
      "3734     {\n",
      "3735         if(CUBLAS_STATUS_NOT_INITIALIZED == err)\n",
      "3736             PyErr_SetString(PyExc_RuntimeError,\n",
      "3737                             \"cublasCreate() returned this error \"\n",
      "3738                             \"'the CUDA Runtime initialization failed'\");\n",
      "3739         else if(CUBLAS_STATUS_ALLOC_FAILED == err)\n",
      "3740             PyErr_SetString(PyExc_RuntimeError,\n",
      "3741                             \"cublasCreate() returned this error \"\n",
      "3742                             \"'the resources could not be allocated'\");\n",
      "3743         else\n",
      "3744             PyErr_SetString(PyExc_RuntimeError,\n",
      "3745                             \"unknow error during returned by cublasCreate()\");\n",
      "3746         return -1;\n",
      "3747     }\n",
      "3748     // Set the default stream as the one to execute on (default)\n",
      "3749     cublasSetStream(handle, NULL);\n",
      "3750     // Pointer to scalars are on the host (also default)\n",
      "3751     cublasSetPointerMode(handle, CUBLAS_POINTER_MODE_HOST);\n",
      "3752 #if CUDA_VERSION >= 5000\n",
      "3753     // atomics can be used in kernels to speed up operations (not default)\n",
      "3754     // This may lead to a slight variance from run to run in some operations\n",
      "3755     cublasSetAtomicsMode(handle, CUBLAS_ATOMICS_ALLOWED);\n",
      "3756 #endif\n",
      "3757     return 0;\n",
      "3758 }\n",
      "3759 \n",
      "3760 static void\n",
      "3761 cublas_shutdown()\n",
      "3762 {\n",
      "3763     if (handle != NULL)\n",
      "3764         cublasDestroy(handle);\n",
      "3765     // No point in handling any errors here\n",
      "3766     handle = NULL;\n",
      "3767 }\n",
      "3768 \n",
      "3769 int\n",
      "3770 CudaNdarray_CopyFromArray(CudaNdarray * self, PyArrayObject*obj)\n",
      "3771 {\n",
      "3772     int err = CudaNdarray_alloc_contiguous(self, PyArray_NDIM(obj),\n",
      "3773                                            PyArray_DIMS(obj));\n",
      "3774     if (err) {\n",
      "3775         return err;\n",
      "3776     }\n",
      "3777 \n",
      "3778     int typenum = PyArray_TYPE(obj);\n",
      "3779     if (typenum != REAL_TYPENUM)\n",
      "3780     {\n",
      "3781         PyErr_SetString(PyExc_TypeError, \"can only copy from float arrays\");\n",
      "3782         return -1;\n",
      "3783     }\n",
      "3784     assert( 4 ==  PyArray_ITEMSIZE(obj));\n",
      "3785     PyArrayObject * py_src = (PyArrayObject *)PyArray_ContiguousFromAny(\n",
      "3786         (PyObject*)obj, typenum, self->nd, self->nd);\n",
      "3787     if (!py_src) {\n",
      "3788         return -1;\n",
      "3789     }\n",
      "3790     npy_intp py_src_size = PyArray_SIZE(py_src);\n",
      "3791     void *py_src_data = PyArray_DATA(py_src);\n",
      "3792     cudaError_t cerr;\n",
      "3793     CNDA_BEGIN_ALLOW_THREADS;\n",
      "3794     cerr = cudaMemcpy(self->devdata, py_src_data,\n",
      "3795                       py_src_size * sizeof(real),\n",
      "3796                       cudaMemcpyHostToDevice);\n",
      "3797     //CNDA_THREAD_SYNC;  // unneeded because cudaMemcpy is blocking anyway\n",
      "3798     CNDA_END_ALLOW_THREADS;\n",
      "3799     if (cudaSuccess != cerr)\n",
      "3800     {\n",
      "3801         PyErr_Format(PyExc_RuntimeError,\n",
      "3802                      \"Cuda error '%s' while copying %lli data element\"\n",
      "3803                      \" to device memory\",\n",
      "3804                      cudaGetErrorString(cerr),\n",
      "3805                      (long long)py_src_size);\n",
      "3806         Py_DECREF(py_src);\n",
      "3807         return -1;\n",
      "3808     }\n",
      "3809     Py_DECREF(py_src);\n",
      "3810     return 0;\n",
      "3811 }\n",
      "3812 \n",
      "3813 PyObject *\n",
      "3814 CudaNdarray_new_nd(int nd)\n",
      "3815 {\n",
      "3816     CudaNdarray * rval = (CudaNdarray*) CudaNdarray_New();\n",
      "3817     if (!rval || CudaNdarray_set_nd(rval, nd))\n",
      "3818     {\n",
      "3819         Py_XDECREF(rval);\n",
      "3820         rval = NULL;\n",
      "3821     }\n",
      "3822     return (PyObject *) rval;\n",
      "3823 }\n",
      "3824 \n",
      "3825 \n",
      "3826 /**\n",
      "3827  * Initialize 'self' as a view of 'base', with memory storage 'data'\n",
      "3828  */\n",
      "3829 \n",
      "3830 int CudaNdarray_set_device_data(CudaNdarray * self, float * data, PyObject * base)\n",
      "3831 {\n",
      "3832     if (self->data_allocated)\n",
      "3833     {\n",
      "3834         assert(self->devdata);\n",
      "3835         if (device_free(self->devdata))\n",
      "3836         {\n",
      "3837             self->devdata = NULL;\n",
      "3838             self->data_allocated = 0;\n",
      "3839             return -1;\n",
      "3840         }\n",
      "3841     }\n",
      "3842     // Get the original base object (base.base.base...)\n",
      "3843     PyObject * orig_base = base;\n",
      "3844     // base is not always a CudaNdarray. It can be a GpuArray from pycuda, ...\n",
      "3845     while (orig_base && CudaNdarray_Check(orig_base) && ((CudaNdarray*) orig_base)->base)\n",
      "3846     {\n",
      "3847         // base_base is itself a view\n",
      "3848         orig_base = ((CudaNdarray*) orig_base)->base;\n",
      "3849     }\n",
      "3850     //N.B. XDECREF and XINCREF are no-ops for NULL pointers\n",
      "3851     if (self->base != orig_base)\n",
      "3852     {\n",
      "3853         Py_XDECREF(self->base);\n",
      "3854         self->base = orig_base;\n",
      "3855         Py_XINCREF(self->base);\n",
      "3856     }\n",
      "3857     self->data_allocated = 0;\n",
      "3858     self->devdata = data;\n",
      "3859     return 0;\n",
      "3860 }\n",
      "3861 \n",
      "3862 static __global__ void k_copy_1d(const int N, const float * x, const int sx, float * y, const int sy)\n",
      "3863 {\n",
      "3864     for (int i = threadIdx.x + blockIdx.x * blockDim.x; i < N; i += gridDim.x*blockDim.x)\n",
      "3865     {\n",
      "3866         y[i*sy] = x[i*sx];\n",
      "3867     }\n",
      "3868 }\n",
      "3869 \n",
      "3870 // N1 through N4 are the size of y\n",
      "3871 static __global__ void k_copy_4d(const int N1,\n",
      "3872         const int N2, const int N3, const int N4,\n",
      "3873         const float * x, const int sx1, const int sx2, const int sx3,\n",
      "3874         const int sx4,  float * y, const int sy1, const int sy2,\n",
      "3875         const int sy3, const int sy4)\n",
      "3876 {\n",
      "3877     // These must be made int instead of unsigned int due to a bug in nvcc\n",
      "3878     int bx = blockIdx.x;\n",
      "3879     int by = blockIdx.y;\n",
      "3880 \n",
      "3881     for (int i = bx; i < N1; i += gridDim.x)\n",
      "3882     {\n",
      "3883         for (int j = by; j < N2; j += gridDim.y)\n",
      "3884         {\n",
      "3885             for (int k = threadIdx.x; k < N3; k += (int) blockDim.x)\n",
      "3886             {\n",
      "3887                 for (int l = threadIdx.y; l < N4; l += (int) blockDim.y)\n",
      "3888                 {\n",
      "3889                     y[i * sy1 + j * sy2 + k * sy3 + l * sy4] =\n",
      "3890                         x[i * sx1 + j * sx2 + k * sx3 + l * sx4];\n",
      "3891                 }\n",
      "3892             }\n",
      "3893         }\n",
      "3894     }\n",
      "3895 }\n",
      "3896 \n",
      "3897 //copy from other into self\n",
      "3898 int CudaNdarray_CopyFromCudaNdarray(CudaNdarray * self,\n",
      "3899                                     const CudaNdarray * other,\n",
      "3900                                     bool unbroadcast)\n",
      "3901 {\n",
      "3902     int verbose = 0;\n",
      "3903     if (verbose>1) fprintf(stderr, \"CudaNdarray_CopyFromCudaNdarray\\n\");\n",
      "3904 \n",
      "3905     //standard elemwise size checks\n",
      "3906     if (self->nd == -1)\n",
      "3907     {\n",
      "3908         PyErr_SetString(PyExc_TypeError,\n",
      "3909                         \"can't copy into un-initialized CudaNdarray\");\n",
      "3910         return -1;\n",
      "3911     }\n",
      "3912     CudaNdarray * new_other = NULL;\n",
      "3913 \n",
      "3914     if (self->nd < other->nd)\n",
      "3915     {\n",
      "3916         PyErr_Format(PyExc_NotImplementedError,\n",
      "3917             \"CudaNdarray_CopyFromCudaNdarray: The number of dimensions of the \"\n",
      "3918             \"destination needs to be >= the number of dimensions of the \"\n",
      "3919             \"source. Got %d and %d.\", self->nd, other->nd);\n",
      "3920         return -1;\n",
      "3921     }\n",
      "3922     else if (self->nd != other->nd)\n",
      "3923     {\n",
      "3924         new_other = (CudaNdarray *) CudaNdarray_View(other);\n",
      "3925         int added_dims = self->nd - other->nd;\n",
      "3926         int* pattern = (int*) alloca(self->nd * sizeof(int));\n",
      "3927         for(int i = 0; i < added_dims; i++)\n",
      "3928             pattern[i] = -1;\n",
      "3929         for(int i = 0; i < other->nd; i++)\n",
      "3930             pattern[i + added_dims] = i;\n",
      "3931         CudaNdarray_dimshuffle(new_other, self->nd, pattern);\n",
      "3932         other = new_other;\n",
      "3933     }\n",
      "3934     assert(self->nd == other->nd);\n",
      "3935     //standard elemwise dim checks (also compute total size)\n",
      "3936     unsigned int size = 1;\n",
      "3937     unsigned int size_source = 1;\n",
      "3938     for (int i = 0; i< self->nd; ++i)\n",
      "3939     {\n",
      "3940         if ((CudaNdarray_HOST_DIMS(self)[i] != CudaNdarray_HOST_DIMS(other)[i])\n",
      "3941             && (1!=CudaNdarray_HOST_DIMS(other)[i] || !unbroadcast) )\n",
      "3942         {\n",
      "3943           PyErr_Format(PyExc_ValueError,\n",
      "3944                        \"CudaNdarray_CopyFromCudaNdarray:\"\n",
      "3945                        \" need same dimensions for dim %d,\"\n",
      "3946                        \" destination=%d, source=%d\",\n",
      "3947                        i, CudaNdarray_HOST_DIMS(self)[i],\n",
      "3948                        CudaNdarray_HOST_DIMS(other)[i]);\n",
      "3949           Py_XDECREF(new_other);\n",
      "3950           return -1;\n",
      "3951         }\n",
      "3952         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];\n",
      "3953         size_source *= (unsigned int) CudaNdarray_HOST_DIMS(other)[i];\n",
      "3954     }\n",
      "3955     if (0 == size)\n",
      "3956     {\n",
      "3957         Py_XDECREF(new_other);\n",
      "3958         return 0; //nothing to copy, we're done.\n",
      "3959     }\n",
      "3960     if (CudaNdarray_is_c_contiguous(self) &&\n",
      "3961         CudaNdarray_is_c_contiguous(other) &&\n",
      "3962         size == size_source)\n",
      "3963     {\n",
      "3964         if (verbose)\n",
      "3965             fprintf(stderr, \"Copying contiguous vector with cublasScopy\\n\");\n",
      "3966 \n",
      "3967         cublasStatus_t err;\n",
      "3968         err = cublasScopy(handle, size, CudaNdarray_DEV_DATA(other), 1,\n",
      "3969                           CudaNdarray_DEV_DATA(self), 1);\n",
      "3970         CNDA_THREAD_SYNC;\n",
      "3971         Py_XDECREF(new_other);\n",
      "3972         if (CUBLAS_STATUS_SUCCESS != err)\n",
      "3973         {\n",
      "3974             PyErr_SetString(PyExc_RuntimeError, \"Error copying memory\");\n",
      "3975             return -1;\n",
      "3976         }\n",
      "3977         return 0;\n",
      "3978     }\n",
      "3979     //TODO: rewrite these copy operations to be more efficient\n",
      "3980     //      See, for example the transpose example in the cuda_sdk.\n",
      "3981     switch (self->nd)\n",
      "3982     {\n",
      "3983         case 0: // scalar\n",
      "3984             {\n",
      "3985                 // THIS CASE SHOULD NEVER HAPPEN BECAUSE SCALARS ARE ALWAYS C CONTIGUOUS\n",
      "3986                 assert(0);\n",
      "3987             }; break;\n",
      "3988         case 1: // vector\n",
      "3989             {\n",
      "3990                 if (verbose) fprintf(stderr, \"Copying non-contiguous vector\\n\");\n",
      "3991                 if (verbose) fprint_CudaNdarray(stderr, other);\n",
      "3992                 unsigned int n_blocks = std::min(size,\n",
      "3993                                                  (unsigned int)NUM_VECTOR_OP_BLOCKS);\n",
      "3994                 unsigned int n_threads = std::min(ceil_intdiv(size, n_blocks),\n",
      "3995                                                   (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
      "3996                 k_copy_1d<<<n_blocks, n_threads>>>(size,\n",
      "3997                                             CudaNdarray_DEV_DATA(other),\n",
      "3998                                             CudaNdarray_HOST_STRIDES(other)[0],\n",
      "3999                                             CudaNdarray_DEV_DATA(self),\n",
      "4000                                             CudaNdarray_HOST_STRIDES(self)[0]);\n",
      "4001                 CNDA_THREAD_SYNC;\n",
      "4002                 cudaError_t err = cudaGetLastError();\n",
      "4003                 if( cudaSuccess != err)\n",
      "4004                 {\n",
      "4005                     PyErr_Format(PyExc_RuntimeError,\n",
      "4006                                  \"Cuda error: %s: %s. (n_blocks=%i,\"\n",
      "4007                                  \" n_threads_per_block=%i)\\n\", \"k_copy_1d\",\n",
      "4008                                  cudaGetErrorString(err), n_blocks, n_threads);\n",
      "4009                     Py_XDECREF(new_other);\n",
      "4010                     return -1;\n",
      "4011                 }\n",
      "4012             }; break;\n",
      "4013         case 4: // 4-tensor\n",
      "4014             {\n",
      "4015                 if (verbose)\n",
      "4016                 {\n",
      "4017                     if (0 != fprint_CudaNdarray(stderr, other))\n",
      "4018                     {\n",
      "4019                         Py_XDECREF(new_other);\n",
      "4020                         return -1;\n",
      "4021                     }\n",
      "4022                 }\n",
      "4023 \n",
      "4024                 // The blocks implement the looping over the first two axes so\n",
      "4025                 // this needs to be (N1, N2)\n",
      "4026                 dim3 n_blocks( std::min(CudaNdarray_HOST_DIMS(self)[0],\n",
      "4027                                         NUM_VECTOR_OP_BLOCKS),\n",
      "4028                                std::min(CudaNdarray_HOST_DIMS(self)[1],\n",
      "4029                                         NUM_VECTOR_OP_BLOCKS));\n",
      "4030                 // For the threads, just make as many as possible\n",
      "4031                 dim3 n_threads( std::min( (unsigned int) CudaNdarray_HOST_DIMS(self)[2],\n",
      "4032                                  (unsigned int) NUM_VECTOR_OP_THREADS_PER_BLOCK),\n",
      "4033                                 std::min( (unsigned int) CudaNdarray_HOST_DIMS(self)[3],\n",
      "4034                                     (unsigned int) NUM_VECTOR_OP_THREADS_PER_BLOCK));\n",
      "4035 \n",
      "4036                 n_threads.x = std::min( (unsigned int) 32, (unsigned int) n_threads.x);\n",
      "4037                 n_threads.y = std::min( n_threads.y, NUM_VECTOR_OP_THREADS_PER_BLOCK / n_threads.x);\n",
      "4038 \n",
      "4039                 k_copy_4d<<<n_blocks, n_threads>>>(\n",
      "4040                                             // size of y\n",
      "4041                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[0], // N1\n",
      "4042                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[1], // N2\n",
      "4043                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[2], // N3\n",
      "4044                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[3], // N4\n",
      "4045                                             CudaNdarray_DEV_DATA(other), // x\n",
      "4046                                             // x strides\n",
      "4047                                             CudaNdarray_HOST_STRIDES(other)[0],\n",
      "4048                                             CudaNdarray_HOST_STRIDES(other)[1],\n",
      "4049                                             CudaNdarray_HOST_STRIDES(other)[2],\n",
      "4050                                             CudaNdarray_HOST_STRIDES(other)[3],\n",
      "4051                                             CudaNdarray_DEV_DATA(self), // y\n",
      "4052                                             // y strides\n",
      "4053                                             CudaNdarray_HOST_STRIDES(self)[0],\n",
      "4054                                             CudaNdarray_HOST_STRIDES(self)[1],\n",
      "4055                                             CudaNdarray_HOST_STRIDES(self)[2],\n",
      "4056                                             CudaNdarray_HOST_STRIDES(self)[3]\n",
      "4057                                             );\n",
      "4058                 CNDA_THREAD_SYNC;\n",
      "4059                 cudaError_t err = cudaGetLastError();\n",
      "4060                 if( cudaSuccess != err)\n",
      "4061                 {\n",
      "4062                     PyErr_Format(PyExc_RuntimeError,\n",
      "4063                                  \"Cuda error: %s: %s.\",\n",
      "4064                                  \"k_copy_4d\",\n",
      "4065                                  cudaGetErrorString(err));\n",
      "4066                     Py_XDECREF(new_other);\n",
      "4067                     return -1;\n",
      "4068                 }\n",
      "4069             }; break;\n",
      "4070         default:\n",
      "4071             {\n",
      "4072                 assert (cudaSuccess == cudaGetLastError());\n",
      "4073                 if (verbose)\n",
      "4074                     fprintf(stderr,\n",
      "4075                             \"Copying with default version unbroadcast=%d\\n\",\n",
      "4076                             unbroadcast);\n",
      "4077                 // call worker routine\n",
      "4078                 unsigned int threads_per_block = std::min(size,\n",
      "4079                                                           (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
      "4080                 unsigned int n_blocks = std::min(ceil_intdiv(size, threads_per_block),\n",
      "4081                                                  (unsigned int)NUM_VECTOR_OP_BLOCKS);\n",
      "4082                 const CudaNdarray * cuda_dims = other;\n",
      "4083                 if(unbroadcast)\n",
      "4084                     cuda_dims = self;\n",
      "4085                 //copy from other into self\n",
      "4086                 k_elemwise_unary_rowmajor_copy<<<n_blocks, threads_per_block>>>(\n",
      "4087                         size,\n",
      "4088                         (unsigned int)other->nd,\n",
      "4089                         (const int *)CudaNdarray_DEV_DIMS(cuda_dims),\n",
      "4090                         (const float*)CudaNdarray_DEV_DATA(other),\n",
      "4091                         (const int *)CudaNdarray_DEV_STRIDES(other),\n",
      "4092                         CudaNdarray_DEV_DATA(self),\n",
      "4093                         (const int *)CudaNdarray_DEV_STRIDES(self));\n",
      "4094                 CNDA_THREAD_SYNC;\n",
      "4095                 cudaError_t err = cudaGetLastError();\n",
      "4096                 if(verbose>1)\n",
      "4097                     fprintf(stderr,\n",
      "4098                             \"INFO k_elemwise_unary_rowmaj (n_blocks=%i,\"\n",
      "4099                             \" n_threads_per_block=%i)\\n\",\n",
      "4100                             n_blocks, threads_per_block);\n",
      "4101                 if( cudaSuccess != err)\n",
      "4102                 {\n",
      "4103                     //fprint_CudaNdarray(stderr, self);\n",
      "4104                     //fprint_CudaNdarray(stderr, other);\n",
      "4105                     PyErr_Format(PyExc_RuntimeError,\n",
      "4106                                  \"Cuda error: %s: %s. (n_blocks=%i,\"\n",
      "4107                                  \" n_threads_per_block=%i)\\n\",\n",
      "4108                                  \"k_elemwise_unary_rowmajor_copy\",\n",
      "4109                                  cudaGetErrorString(err), n_blocks,\n",
      "4110                                  threads_per_block);\n",
      "4111                     Py_XDECREF(new_other);\n",
      "4112                     return -1;\n",
      "4113                 }\n",
      "4114             }\n",
      "4115     };\n",
      "4116     Py_XDECREF(new_other);\n",
      "4117     return 0;\n",
      "4118 }\n",
      "4119 \n",
      "4120 int CudaNdarray_gemm(float alpha, const CudaNdarray * A, const CudaNdarray * B, float beta, CudaNdarray * C)\n",
      "4121 {\n",
      "4122     if (A->nd != 2)\n",
      "4123     {\n",
      "4124         PyErr_SetString(PyExc_ValueError, \"non-matrix arg A to gemm\");\n",
      "4125         return -1;\n",
      "4126     }\n",
      "4127     if (B->nd != 2)\n",
      "4128     {\n",
      "4129         PyErr_SetString(PyExc_ValueError, \"non-matrix arg B to gemm\");\n",
      "4130         return -1;\n",
      "4131     }\n",
      "4132     if (C->nd != 2)\n",
      "4133     {\n",
      "4134         PyErr_SetString(PyExc_ValueError, \"non-matrix arg C to gemm\");\n",
      "4135         return -1;\n",
      "4136     }\n",
      "4137 \n",
      "4138     // We must allow dimensions to be zeros.\n",
      "4139     if ((CudaNdarray_HOST_DIMS(A)[1] != CudaNdarray_HOST_DIMS(B)[0])\n",
      "4140             || (CudaNdarray_HOST_DIMS(A)[0] != CudaNdarray_HOST_DIMS(C)[0])\n",
      "4141             || (CudaNdarray_HOST_DIMS(B)[1] != CudaNdarray_HOST_DIMS(C)[1]))\n",
      "4142     {\n",
      "4143         PyErr_Format(PyExc_ValueError, \"dimension mismatch in args to gemm (%i,%i)x(%i,%i)->(%i,%i)\",\n",
      "4144                 CudaNdarray_HOST_DIMS(A)[0],\n",
      "4145                 CudaNdarray_HOST_DIMS(A)[1],\n",
      "4146                 CudaNdarray_HOST_DIMS(B)[0],\n",
      "4147                 CudaNdarray_HOST_DIMS(B)[1],\n",
      "4148                 CudaNdarray_HOST_DIMS(C)[0],\n",
      "4149                 CudaNdarray_HOST_DIMS(C)[1]);\n",
      "4150         return -1;\n",
      "4151     }\n",
      "4152 \n",
      "4153     // If matrix A or B has non-unit size and non-unit stride in both\n",
      "4154     // dimensions, we can make a copy.\n",
      "4155     CudaNdarray * A_new = NULL;\n",
      "4156     CudaNdarray * B_new = NULL;\n",
      "4157     if (((CudaNdarray_HOST_DIMS(A)[0] > 1)\n",
      "4158          && (CudaNdarray_HOST_STRIDES(A)[0] != 1)\n",
      "4159          && (CudaNdarray_HOST_DIMS(A)[1] > 1)\n",
      "4160          && (CudaNdarray_HOST_STRIDES(A)[1] != 1))\n",
      "4161         || (CudaNdarray_HOST_STRIDES(A)[0] < 0)\n",
      "4162         || (CudaNdarray_HOST_STRIDES(A)[1] < 0))\n",
      "4163     {\n",
      "4164         A_new = (CudaNdarray*) CudaNdarray_Copy(A);\n",
      "4165         if (!A_new)\n",
      "4166             return -1;\n",
      "4167         A = A_new;\n",
      "4168     }\n",
      "4169 \n",
      "4170     if (((CudaNdarray_HOST_DIMS(B)[0] > 1)\n",
      "4171          && (CudaNdarray_HOST_STRIDES(B)[0] != 1)\n",
      "4172          && (CudaNdarray_HOST_DIMS(B)[1] > 1)\n",
      "4173          && (CudaNdarray_HOST_STRIDES(B)[1] != 1))\n",
      "4174         || (CudaNdarray_HOST_STRIDES(B)[0] < 0)\n",
      "4175         || (CudaNdarray_HOST_STRIDES(B)[1] < 0))\n",
      "4176     {\n",
      "4177         B_new = (CudaNdarray*) CudaNdarray_Copy(B);\n",
      "4178         if (!B_new)\n",
      "4179         {\n",
      "4180             // If A_new is NULL, meaning A was not copied nothing happens\n",
      "4181             Py_XDECREF(A_new);\n",
      "4182             return -1;\n",
      "4183         }\n",
      "4184         B = B_new;\n",
      "4185     }\n",
      "4186 \n",
      "4187     // If matrix C has non-unit size and non-unit stride in both\n",
      "4188     // dimensions, or negative strides, we can't operate. We cannot copy\n",
      "4189     // C either, because the calling code will expect the result to be\n",
      "4190     // in the original C container.\n",
      "4191     if (((CudaNdarray_HOST_DIMS(C)[0] > 1)\n",
      "4192          && (CudaNdarray_HOST_STRIDES(C)[0] != 1)\n",
      "4193          && (CudaNdarray_HOST_DIMS(C)[1] > 1)\n",
      "4194          && (CudaNdarray_HOST_STRIDES(C)[1] != 1))\n",
      "4195         || (CudaNdarray_HOST_STRIDES(C)[0] < 0)\n",
      "4196         || (CudaNdarray_HOST_STRIDES(C)[1] < 0))\n",
      "4197     {\n",
      "4198         PyErr_Format(PyExc_AssertionError,\n",
      "4199                      \"non-unit or negative stride in gemm arg C (%i,%i) of shape (%i,%i)\",\n",
      "4200                      CudaNdarray_HOST_STRIDES(C)[0],\n",
      "4201                      CudaNdarray_HOST_STRIDES(C)[1],\n",
      "4202                      CudaNdarray_HOST_DIMS(C)[0],\n",
      "4203                      CudaNdarray_HOST_DIMS(C)[1]);\n",
      "4204         Py_XDECREF(A_new);\n",
      "4205         Py_XDECREF(B_new);\n",
      "4206         return -1;\n",
      "4207     }\n",
      "4208 \n",
      "4209     // the unit integer is divided logically into three fields of 4 bits\n",
      "4210     // the lowermost 4 bits encode the stride pattern of the output\n",
      "4211     // the next higher 4 bits encode the B variable (or y)\n",
      "4212     // the next higher 4 bits encode the C variable (or x)\n",
      "4213     //\n",
      "4214     // the stride pattern for each input is encoded as 0 for unit stride from col to col (Row major)\n",
      "4215     //                                                 1 for unit stride from row to row (Col major)\n",
      "4216 \n",
      "4217     // a stride of 0 implies a dimension of 1 - so we can actually define\n",
      "4218     // a stride of 0 as a 'unit' stride because gemm will never use it.\n",
      "4219     // If a dimension is 0, its stride will not be used either, so we can\n",
      "4220     // consider it a 'unit' stride too.\n",
      "4221     int unit = 0;\n",
      "4222     if (CudaNdarray_HOST_STRIDES(A)[1] == 1 || CudaNdarray_HOST_DIMS(A)[1] <= 1) {\n",
      "4223         unit |= (0x0 << 8);\n",
      "4224     } else if (CudaNdarray_HOST_STRIDES(A)[0] == 1 || CudaNdarray_HOST_DIMS(A)[0] <= 1) {\n",
      "4225         unit |= (0x1 << 8);\n",
      "4226     } else {\n",
      "4227         unit |= (0x2 << 8);\n",
      "4228     }\n",
      "4229     if (CudaNdarray_HOST_STRIDES(B)[1] == 1 || CudaNdarray_HOST_DIMS(B)[1] <= 1) {\n",
      "4230         unit |= (0x0 << 4);\n",
      "4231     } else if (CudaNdarray_HOST_STRIDES(B)[0] == 1 || CudaNdarray_HOST_DIMS(B)[0] <= 1) {\n",
      "4232         unit |= (0x1 << 4);\n",
      "4233     } else {\n",
      "4234         unit |= (0x2 << 4);\n",
      "4235     }\n",
      "4236     if (CudaNdarray_HOST_STRIDES(C)[1] == 1 || CudaNdarray_HOST_DIMS(C)[1] <= 1) {\n",
      "4237         unit |= (0x0 << 0);\n",
      "4238     } else if (CudaNdarray_HOST_STRIDES(C)[0] == 1 || CudaNdarray_HOST_DIMS(C)[0] <= 1) {\n",
      "4239         unit |= (0x1 << 0);\n",
      "4240     } else {\n",
      "4241         unit |= (0x2 << 0);\n",
      "4242     }\n",
      "4243 \n",
      "4244     /* create appropriate strides for malformed matrices that are row or column\n",
      "4245      * vectors\n",
      "4246      */\n",
      "4247     int sa_0 = (CudaNdarray_HOST_DIMS(A)[0] > 1) ? CudaNdarray_HOST_STRIDES(A)[0] : CudaNdarray_HOST_DIMS(A)[1];\n",
      "4248     int sa_1 = (CudaNdarray_HOST_DIMS(A)[1] > 1) ? CudaNdarray_HOST_STRIDES(A)[1] : CudaNdarray_HOST_DIMS(A)[0];\n",
      "4249     int sb_0 = (CudaNdarray_HOST_DIMS(B)[0] > 1) ? CudaNdarray_HOST_STRIDES(B)[0] : CudaNdarray_HOST_DIMS(B)[1];\n",
      "4250     int sb_1 = (CudaNdarray_HOST_DIMS(B)[1] > 1) ? CudaNdarray_HOST_STRIDES(B)[1] : CudaNdarray_HOST_DIMS(B)[0];\n",
      "4251     int sc_0 = (CudaNdarray_HOST_DIMS(C)[0] > 1) ? CudaNdarray_HOST_STRIDES(C)[0] : CudaNdarray_HOST_DIMS(C)[1];\n",
      "4252     int sc_1 = (CudaNdarray_HOST_DIMS(C)[1] > 1) ? CudaNdarray_HOST_STRIDES(C)[1] : CudaNdarray_HOST_DIMS(C)[0];\n",
      "4253 \n",
      "4254     float* a = CudaNdarray_DEV_DATA(A);\n",
      "4255     float* b = CudaNdarray_DEV_DATA(B);\n",
      "4256     float* c = CudaNdarray_DEV_DATA(C);\n",
      "4257     cublasOperation_t N = CUBLAS_OP_N;\n",
      "4258     cublasOperation_t T = CUBLAS_OP_T;\n",
      "4259     //std::cerr << (unit/256) MOD 16 << (unit / 16) MOD 16 << unit MOD 16<< '\\\\n';\n",
      "4260     // There should be no negative stride at that point\n",
      "4261 #define CHK_STRIDE_SGEMM(T0, T1, D0, D1, D2, a, x, sx, y, sy, b, z, sz) \\\n",
      "4262     if (sx == 0){sx = 1;}\\\n",
      "4263     if (sy == 0){sy = 1;}\\\n",
      "4264     if (sz == 0){sz = 1;}\\\n",
      "4265     if ((sx > 0) && (sy > 0) && (sz > 0)) { \\\n",
      "4266         err = cublasSgemm(handle, T0, T1, D0, D1, D2, &a, x, sx, y, sy, &b, z, sz); \\\n",
      "4267     } else { \\\n",
      "4268         PyErr_SetString(PyExc_AssertionError, \"negative stride to sGemm\");\\\n",
      "4269         Py_XDECREF(A_new);\\\n",
      "4270         Py_XDECREF(B_new);\\\n",
      "4271         return -1; \\\n",
      "4272     }\n",
      "4273 \n",
      "4274     cublasStatus_t err;\n",
      "4275     switch(unit)\n",
      "4276     {\n",
      "4277         case 0x000: CHK_STRIDE_SGEMM(N, N, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_0, a, sa_0, beta, c, sc_0); break;\n",
      "4278         case 0x100: CHK_STRIDE_SGEMM(N, T, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_0, a, sa_1, beta, c, sc_0); break;\n",
      "4279         case 0x010: CHK_STRIDE_SGEMM(T, N, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_1, a, sa_0, beta, c, sc_0); break;\n",
      "4280         case 0x110: CHK_STRIDE_SGEMM(T, T, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_1, a, sa_1, beta, c, sc_0); break;\n",
      "4281         case 0x001: CHK_STRIDE_SGEMM(T, T, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_0, b, sb_0, beta, c, sc_1); break;\n",
      "4282         case 0x101: CHK_STRIDE_SGEMM(N, T, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_1, b, sb_0, beta, c, sc_1); break;\n",
      "4283         case 0x011: CHK_STRIDE_SGEMM(T, N, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_0, b, sb_1, beta, c, sc_1); break;\n",
      "4284         case 0x111: CHK_STRIDE_SGEMM(N, N, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_1, b, sb_1, beta, c, sc_1); break;\n",
      "4285         default: PyErr_Format(PyExc_ValueError, \"some matrix has no unit stride (unit=%x)\", unit);\n",
      "4286                  return -1;\n",
      "4287     };\n",
      "4288     CNDA_THREAD_SYNC;\n",
      "4289     Py_XDECREF(A_new);\n",
      "4290     Py_XDECREF(B_new);\n",
      "4291 \n",
      "4292     if (CUBLAS_STATUS_SUCCESS != err)\n",
      "4293     {\n",
      "4294         PyErr_Format(PyExc_RuntimeError,\n",
      "4295                      \"cublasSgemm failed (%i) %s\\n\"\n",
      "4296                      \" unit=%x N=%d, c.dims=[%d %d], a.dim=[%d %d], alpha=%f, beta=%f, a=%p, b=%p, c=%p\"\n",
      "4297                      \" sa_0=%d, sa_1=%d, sb_0=%d, sb_1=%d, sc_0=%d, sc_1=%d\",\n",
      "4298                      err,  cublasGetErrorString(err),\n",
      "4299                      unit, N,\n",
      "4300                      CudaNdarray_HOST_DIMS(C)[0],\n",
      "4301                      CudaNdarray_HOST_DIMS(C)[1],\n",
      "4302                      CudaNdarray_HOST_DIMS(A)[0], CudaNdarray_HOST_DIMS(A)[1],\n",
      "4303                      alpha, beta, a, b, c, sa_0, sa_1, sb_0, sb_1, sc_0, sc_1);\n",
      "4304 \n",
      "4305         return -1;\n",
      "4306     }\n",
      "4307     return 0;\n",
      "4308 }\n",
      "4309 \n",
      "4310 int CudaNdarray_sgemv(float alpha, const CudaNdarray * A, const CudaNdarray * B, float beta, CudaNdarray * C)\n",
      "4311 {\n",
      "4312     /**\n",
      "4313     * C <- alpha A B + beta C\n",
      "4314     *    A : matrix\n",
      "4315     *    B, C: vector\n",
      "4316     *    alpha, beta: scalars\n",
      "4317     */\n",
      "4318     if (A->nd != 2) { PyErr_SetString(PyExc_ValueError, \"non-matrix arg to gemv\"); return -1; }\n",
      "4319     if (B->nd != 1) { PyErr_SetString(PyExc_ValueError, \"non-vector arg to gemv\"); return -1; }\n",
      "4320     if (C->nd != 1) { PyErr_SetString(PyExc_ValueError, \"non-vector arg to gemv\"); return -1; }\n",
      "4321 \n",
      "4322     // We must allow dimensions to be zeros.\n",
      "4323     if ((CudaNdarray_HOST_DIMS(A)[1] != CudaNdarray_HOST_DIMS(B)[0])\n",
      "4324             || (CudaNdarray_HOST_DIMS(A)[0] != CudaNdarray_HOST_DIMS(C)[0]))\n",
      "4325     {\n",
      "4326         PyErr_Format(PyExc_ValueError, \"dimension mismatch in args to gemv (%i,%i)x(%i)->(%i)\",\n",
      "4327                 CudaNdarray_HOST_DIMS(A)[0],\n",
      "4328                 CudaNdarray_HOST_DIMS(A)[1],\n",
      "4329                 CudaNdarray_HOST_DIMS(B)[0],\n",
      "4330                 CudaNdarray_HOST_DIMS(C)[0]);\n",
      "4331         return -1;\n",
      "4332     }\n",
      "4333 \n",
      "4334     // If matrix A has non-unit size and non-unit stride in both\n",
      "4335     // dimensions, or negative strides, we cannot operate, but we can\n",
      "4336     // make a copy.\n",
      "4337     CudaNdarray * A_new = NULL;\n",
      "4338     CudaNdarray * B_new = NULL;\n",
      "4339     if (((CudaNdarray_HOST_DIMS(A)[0] > 1)\n",
      "4340          && (CudaNdarray_HOST_STRIDES(A)[0] != 1)\n",
      "4341          && (CudaNdarray_HOST_DIMS(A)[1] > 1)\n",
      "4342          && (CudaNdarray_HOST_STRIDES(A)[1] != 1))\n",
      "4343         || (CudaNdarray_HOST_STRIDES(A)[0] < 0)\n",
      "4344         || (CudaNdarray_HOST_STRIDES(A)[1] < 0))\n",
      "4345     {\n",
      "4346         A_new = (CudaNdarray*) CudaNdarray_Copy(A);\n",
      "4347         if (!A_new)\n",
      "4348             return -1;\n",
      "4349         A = A_new;\n",
      "4350     }\n",
      "4351 \n",
      "4352     // If vector B as a negative stride, we also have to make a copy.\n",
      "4353     if (CudaNdarray_HOST_STRIDES(B)[0] < 0)\n",
      "4354     {\n",
      "4355         B_new = (CudaNdarray*) CudaNdarray_Copy(B);\n",
      "4356         if (!B_new)\n",
      "4357         {\n",
      "4358             // If A was not copied, A_new is NULL, and Py_XDECREF does not\n",
      "4359             // do anything\n",
      "4360             Py_XDECREF(A_new);\n",
      "4361             return -1;\n",
      "4362         }\n",
      "4363         B = B_new;\n",
      "4364     }\n",
      "4365 \n",
      "4366     // cudablas does not handle negative strides as expected\n",
      "4367     if (   (CudaNdarray_HOST_STRIDES(A)[0] < 0)\n",
      "4368         || (CudaNdarray_HOST_STRIDES(A)[1] < 0))\n",
      "4369     {\n",
      "4370         PyErr_Format(PyExc_ValueError, \"illegal strides in args to gemv (%i,%i)\",\n",
      "4371                 CudaNdarray_HOST_STRIDES(A)[0],\n",
      "4372                 CudaNdarray_HOST_STRIDES(A)[1]);\n",
      "4373         Py_XDECREF(A_new);\n",
      "4374         Py_XDECREF(B_new);\n",
      "4375         return -1;\n",
      "4376     }\n",
      "4377 \n",
      "4378     /* create appropriate strides for malformed matrices that are row or column\n",
      "4379      * vectors\n",
      "4380      */\n",
      "4381     int sa_0 = (CudaNdarray_HOST_DIMS(A)[0] > 1) ? CudaNdarray_HOST_STRIDES(A)[0] : CudaNdarray_HOST_DIMS(A)[1];\n",
      "4382     int sa_1 = (CudaNdarray_HOST_DIMS(A)[1] > 1) ? CudaNdarray_HOST_STRIDES(A)[1] : CudaNdarray_HOST_DIMS(A)[0];\n",
      "4383     int sb_0 = (CudaNdarray_HOST_DIMS(B)[0] > 1) ? CudaNdarray_HOST_STRIDES(B)[0] : 1;\n",
      "4384     int sc_0 = (CudaNdarray_HOST_DIMS(C)[0] > 1) ? CudaNdarray_HOST_STRIDES(C)[0] : 1;\n",
      "4385 \n",
      "4386     if (sa_0 == 0)\n",
      "4387         sa_0 = 1;\n",
      "4388     if (sa_1 == 0)\n",
      "4389         sa_1 = 1;\n",
      "4390 \n",
      "4391     // This is important because we can end up not calling Sgemv at all\n",
      "4392     cublasStatus_t err = CUBLAS_STATUS_SUCCESS;\n",
      "4393     if (CudaNdarray_SIZE(C)) {\n",
      "4394         if ((CudaNdarray_HOST_DIMS(A)[0] <= 1)\n",
      "4395             || ((CudaNdarray_HOST_STRIDES(A)[0] == 1)\n",
      "4396                 && (CudaNdarray_HOST_STRIDES(A)[1] > 0)))\n",
      "4397         {\n",
      "4398             err = cublasSgemv(handle, CUBLAS_OP_N,\n",
      "4399                     CudaNdarray_HOST_DIMS(A)[0], CudaNdarray_HOST_DIMS(A)[1],\n",
      "4400                     &alpha,\n",
      "4401                     CudaNdarray_DEV_DATA(A), sa_1,\n",
      "4402                     CudaNdarray_DEV_DATA(B), sb_0,\n",
      "4403                     &beta,\n",
      "4404                     CudaNdarray_DEV_DATA(C), sc_0);\n",
      "4405         }\n",
      "4406         else if ((CudaNdarray_HOST_DIMS(A)[1] <= 1)\n",
      "4407                 || ((CudaNdarray_HOST_STRIDES(A)[1] == 1)\n",
      "4408                     && (CudaNdarray_HOST_STRIDES(A)[0] > 0)))\n",
      "4409         {\n",
      "4410             err = cublasSgemv(handle, CUBLAS_OP_T,\n",
      "4411                     CudaNdarray_HOST_DIMS(A)[1], CudaNdarray_HOST_DIMS(A)[0],\n",
      "4412                     &alpha,\n",
      "4413                     CudaNdarray_DEV_DATA(A), sa_0,\n",
      "4414                     CudaNdarray_DEV_DATA(B), sb_0,\n",
      "4415                     &beta,\n",
      "4416                     CudaNdarray_DEV_DATA(C), sc_0);\n",
      "4417         }\n",
      "4418         else\n",
      "4419         {\n",
      "4420             PyErr_Format(PyExc_AssertionError,\n",
      "4421                          \"Unexpected stride pattern in gemv: (%i, %i) x %i -> %i.\\n\"\n",
      "4422                          \"Shapes are: (%i, %i) x %i -> %i\\n\",\n",
      "4423                          CudaNdarray_HOST_STRIDES(A)[0],\n",
      "4424                          CudaNdarray_HOST_STRIDES(A)[1],\n",
      "4425                          CudaNdarray_HOST_STRIDES(B)[0],\n",
      "4426                          CudaNdarray_HOST_STRIDES(C)[0],\n",
      "4427                          CudaNdarray_HOST_DIMS(A)[0],\n",
      "4428                          CudaNdarray_HOST_DIMS(A)[1],\n",
      "4429                          CudaNdarray_HOST_DIMS(B)[0],\n",
      "4430                          CudaNdarray_HOST_DIMS(C)[0]);\n",
      "4431             Py_XDECREF(A_new);\n",
      "4432             Py_XDECREF(B_new);\n",
      "4433             return -1;\n",
      "4434         }\n",
      "4435     }\n",
      "4436 \n",
      "4437     CNDA_THREAD_SYNC;\n",
      "4438     Py_XDECREF(A_new);\n",
      "4439     Py_XDECREF(B_new);\n",
      "4440 \n",
      "4441     if (CUBLAS_STATUS_SUCCESS != err)\n",
      "4442     {\n",
      "4443         PyErr_Format(PyExc_RuntimeError,\n",
      "4444                      \"cublasSgemv failed (%i)\",\n",
      "4445                      err);\n",
      "4446         return -1;\n",
      "4447     }\n",
      "4448     return 0;\n",
      "4449 }\n",
      "4450 \n",
      "4451 int CudaNdarray_sger(float alpha, const CudaNdarray * x, const CudaNdarray * y, CudaNdarray * A) {\n",
      "4452     if (x->nd != 1) { PyErr_SetString(PyExc_ValueError, \"non-vector arg x to sger\"); return -1; }\n",
      "4453     if (y->nd != 1) { PyErr_SetString(PyExc_ValueError, \"non-vector arg y to sger\"); return -1; }\n",
      "4454     if (A->nd != 2) { PyErr_SetString(PyExc_ValueError, \"non-matrix arg A to sger\"); return -1; }\n",
      "4455 \n",
      "4456     if ((CudaNdarray_HOST_DIMS(A)[0] != CudaNdarray_HOST_DIMS(x)[0])\n",
      "4457         || (CudaNdarray_HOST_DIMS(A)[1] != CudaNdarray_HOST_DIMS(y)[0])) {\n",
      "4458         PyErr_Format(PyExc_ValueError,\n",
      "4459                      \"dimension mismatch in args to sger (%i)x(%i)->(%i,%i)\",\n",
      "4460                      CudaNdarray_HOST_DIMS(x)[0],\n",
      "4461                      CudaNdarray_HOST_DIMS(y)[0],\n",
      "4462                      CudaNdarray_HOST_DIMS(A)[0],\n",
      "4463                      CudaNdarray_HOST_DIMS(A)[1]);\n",
      "4464         return -1;\n",
      "4465     }\n",
      "4466 \n",
      "4467     int x_strides = CudaNdarray_HOST_STRIDES(x)[0];\n",
      "4468     CudaNdarray * x_new = NULL;\n",
      "4469     if(x_strides == 0){\n",
      "4470         if(CudaNdarray_HOST_DIMS(x)[0] != 1){\n",
      "4471             PyErr_Format(PyExc_RuntimeError,\n",
      "4472                          \"CudaNdarray_sger: Invalid input x (should not happen).\"\n",
      "4473                          \" We received a CudaNdarray vector with a stride of 0\"\n",
      "4474                          \" that has more than 1 element!\");\n",
      "4475             return -1;\n",
      "4476         }\n",
      "4477         x_strides = 1;\n",
      "4478     } else if(x_strides < 0){\n",
      "4479         x_new = (CudaNdarray*) CudaNdarray_Copy(x);\n",
      "4480         x = x_new;\n",
      "4481         x_strides = CudaNdarray_HOST_STRIDES(x)[0];\n",
      "4482     }\n",
      "4483 \n",
      "4484     int y_strides = CudaNdarray_HOST_STRIDES(y)[0];\n",
      "4485     CudaNdarray * y_new = NULL;\n",
      "4486     if(y_strides == 0){\n",
      "4487         if(CudaNdarray_HOST_DIMS(y)[0] != 1){\n",
      "4488             PyErr_Format(PyExc_RuntimeError,\n",
      "4489                          \"CudaNdarray_sger: Invalid input y (should not happen).\"\n",
      "4490                          \" We received a CudaNdarray vector with a stride of 0\"\n",
      "4491                          \" that has more than 1 elements!\");\n",
      "4492             Py_XDECREF(x_new);\n",
      "4493             return -1;\n",
      "4494         }\n",
      "4495         y_strides = 1;\n",
      "4496     } else if(y_strides < 0){\n",
      "4497         y_new = (CudaNdarray*) CudaNdarray_Copy(y);\n",
      "4498         y = y_new;\n",
      "4499         y_strides = CudaNdarray_HOST_STRIDES(y)[0];\n",
      "4500     }\n",
      "4501 \n",
      "4502     // Create appropriate strides if A is a row or column vector\n",
      "4503     int sa_0 = (CudaNdarray_HOST_DIMS(A)[0] > 1) ? CudaNdarray_HOST_STRIDES(A)[0]\n",
      "4504                                                  : CudaNdarray_HOST_DIMS(A)[1];\n",
      "4505     int sa_1 = (CudaNdarray_HOST_DIMS(A)[1] > 1) ? CudaNdarray_HOST_STRIDES(A)[1]\n",
      "4506                                                  : CudaNdarray_HOST_DIMS(A)[0];\n",
      "4507 \n",
      "4508     // This is important because we can end up not calling Sger at all\n",
      "4509     cublasStatus_t err = CUBLAS_STATUS_SUCCESS;\n",
      "4510     if(CudaNdarray_SIZE(A)){\n",
      "4511         // If A is in col-major\n",
      "4512         if ((CudaNdarray_HOST_DIMS(A)[0] <= 1)\n",
      "4513             || ((CudaNdarray_HOST_STRIDES(A)[0] == 1)\n",
      "4514                 && (CudaNdarray_HOST_STRIDES(A)[1] > 0)))\n",
      "4515         {\n",
      "4516             err = cublasSger(handle, CudaNdarray_HOST_DIMS(x)[0], CudaNdarray_HOST_DIMS(y)[0], &alpha,\n",
      "4517                        CudaNdarray_DEV_DATA(x), x_strides,\n",
      "4518                        CudaNdarray_DEV_DATA(y), y_strides,\n",
      "4519                        CudaNdarray_DEV_DATA(A), sa_1);\n",
      "4520         }\n",
      "4521         // Since Sger expects A in col-major, we invert x and y to fake this.\n",
      "4522         else if ((CudaNdarray_HOST_DIMS(A)[1] <= 1)\n",
      "4523                 || ((CudaNdarray_HOST_STRIDES(A)[1] == 1)\n",
      "4524                     && (CudaNdarray_HOST_STRIDES(A)[0] > 0)))\n",
      "4525         {\n",
      "4526             err = cublasSger(handle, CudaNdarray_HOST_DIMS(y)[0], CudaNdarray_HOST_DIMS(x)[0], &alpha,\n",
      "4527                        CudaNdarray_DEV_DATA(y), y_strides,\n",
      "4528                        CudaNdarray_DEV_DATA(x), x_strides,\n",
      "4529                        CudaNdarray_DEV_DATA(A), sa_0);\n",
      "4530         }\n",
      "4531         // A has to be either c- or f-contiguous, with no negative strides\n",
      "4532         else\n",
      "4533         {\n",
      "4534             PyErr_SetString(PyExc_NotImplementedError,\n",
      "4535                             \"non-contiguous A, or negative strides, in sger\");\n",
      "4536             Py_XDECREF(x_new);\n",
      "4537             Py_XDECREF(y_new);\n",
      "4538             return -1;\n",
      "4539         }\n",
      "4540     }\n",
      "4541     CNDA_THREAD_SYNC;\n",
      "4542     Py_XDECREF(x_new);\n",
      "4543     Py_XDECREF(y_new);\n",
      "4544 \n",
      "4545     if (CUBLAS_STATUS_SUCCESS != err)\n",
      "4546     {\n",
      "4547         PyErr_Format(PyExc_RuntimeError,\n",
      "4548                      \"cublasSger failed (%i)\",\n",
      "4549                      err);\n",
      "4550         return -1;\n",
      "4551     }\n",
      "4552 \n",
      "4553     return 0;\n",
      "4554 }\n",
      "4555 \n",
      "4556 /**\n",
      "4557  *\n",
      "4558  * Precondition:\n",
      "4559  *  a->dim[d] == (dims_a[d]==0) ? (1 << log2_dims_a[d]) : dims_a[d]\n",
      "4560  *  z->dim[d] == (z_str[d]==0) ? 1 : dims_a[d];\n",
      "4561  *\n",
      "4562  *  TODO: templatize this function to support other reductions.\n",
      "4563  *  All that needs to change is the initial value for sum, and the reduction operator.\n",
      "4564  */\n",
      "4565 \n",
      "4566 static __global__ void kernel_reduce_sum(const unsigned int size_z,\n",
      "4567         const unsigned int nd,\n",
      "4568         const int * dims_a,\n",
      "4569         const int * log2_dims_a,\n",
      "4570         const int * a_str,\n",
      "4571         const float * a_data,\n",
      "4572         const int * z_str,\n",
      "4573         float * z_data)\n",
      "4574 {\n",
      "4575     const unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "4576     const unsigned int numThreads = blockDim.x * gridDim.x;\n",
      "4577 \n",
      "4578     //structure data contains the strides and dimensions of both a and z\n",
      "4579     // a_dim[0], a_dim[1], ... a_dim[nd-1],\n",
      "4580     // a_log2dim[0], a_log2dim[1], ... a_log2dim[nd-1],\n",
      "4581     // a_str[0], ... a_str[nd-1],\n",
      "4582     // z_str[0], ... z_str[nd-1]\n",
      "4583     extern __shared__ int structure_data[];\n",
      "4584     for (unsigned int i = threadIdx.x; i < nd; i += blockDim.x)\n",
      "4585     {\n",
      "4586         structure_data[i+0*nd] = dims_a[i];\n",
      "4587         structure_data[i+1*nd] = log2_dims_a[i];\n",
      "4588         structure_data[i+2*nd] = a_str[i];\n",
      "4589         structure_data[i+3*nd] = z_str[i];\n",
      "4590     }\n",
      "4591     dims_a = structure_data;\n",
      "4592     log2_dims_a = structure_data + nd;\n",
      "4593     a_str = structure_data + 2*nd;\n",
      "4594     z_str = structure_data + 3*nd;\n",
      "4595 \n",
      "4596     __syncthreads(); //wait for all the shared structure to be loaded\n",
      "4597 \n",
      "4598     for (unsigned int i = idx; i < size_z; i += numThreads)\n",
      "4599     {\n",
      "4600         unsigned int ii = i;\n",
      "4601         const float * a_data_i = a_data;\n",
      "4602         float * z_data_i = z_data;\n",
      "4603         unsigned int n_reduce_elements = 1;\n",
      "4604         unsigned int n_reduce_dims = 0;\n",
      "4605         unsigned int reduce_dim0 = nd-1;\n",
      "4606 \n",
      "4607 \n",
      "4608         //In this loop, we locate the initial element of the slice that we'd like to reduce with this thread\n",
      "4609         //  At the same time, we [re]calculate the size of that slice (n_reduce_elements)\n",
      "4610         for (unsigned int d = 0; d < nd; ++d)\n",
      "4611         {\n",
      "4612             if (a_str[d] && (!z_str[d])) // this means 'd' is a dimension we are reducing over\n",
      "4613             {\n",
      "4614                 n_reduce_elements *= dims_a[d];\n",
      "4615                 n_reduce_dims += 1;\n",
      "4616                 reduce_dim0 = (d < reduce_dim0) ? d : reduce_dim0;\n",
      "4617             }\n",
      "4618             else //'d' is not a dimension that we are reducing over\n",
      "4619             {\n",
      "4620                 unsigned int pos_d;\n",
      "4621                 if (log2_dims_a[d]==-1) //TODO: when things are working, use this switch\n",
      "4622                 {\n",
      "4623                     // this branch is not preferred,\n",
      "4624                     // because the manual said that integer mod and div operations are slow on gpu\n",
      "4625                     pos_d = (ii % dims_a[d]);\n",
      "4626                     ii = (ii / dims_a[d]);\n",
      "4627                 }\n",
      "4628                 else\n",
      "4629                 {\n",
      "4630                     pos_d = (ii & ((1 << log2_dims_a[d])-1)); //take the lower log2_dims bits\n",
      "4631                     ii = (ii >> log2_dims_a[d]);  //shift those lower log2_dims bits off of ii\n",
      "4632                 }\n",
      "4633                 a_data_i += pos_d * a_str[d];\n",
      "4634                 z_data_i += pos_d * z_str[d];\n",
      "4635             }\n",
      "4636         }\n",
      "4637         // now we've got pointers a_data_i and z_data_i into element 0 of the slice over which we are reducing\n",
      "4638         // do a similar loop\n",
      "4639 \n",
      "4640         float sum = 0.0f;\n",
      "4641         switch(n_reduce_dims)\n",
      "4642         {\n",
      "4643             case 0:\n",
      "4644                 {\n",
      "4645                     sum = a_data_i[0];\n",
      "4646                 }\n",
      "4647                 break;\n",
      "4648             case 1:\n",
      "4649                 {\n",
      "4650                     const int stride = a_str[reduce_dim0];\n",
      "4651                     const float * a_data_i_max = a_data_i + dims_a[reduce_dim0] * stride;\n",
      "4652                     while (a_data_i != a_data_i_max)\n",
      "4653                     {\n",
      "4654                         sum += a_data_i[0];\n",
      "4655                         a_data_i += stride;\n",
      "4656                     }\n",
      "4657                 }\n",
      "4658                 break;\n",
      "4659             case 2:\n",
      "4660                 {\n",
      "4661                     int rd = reduce_dim0+1;\n",
      "4662                     for (; rd < nd; ++rd)\n",
      "4663                     {\n",
      "4664                         if (a_str[rd] && (!z_str[rd])) // this means 'rd' is a dimension we are reducing over\n",
      "4665                             break;\n",
      "4666                     }\n",
      "4667                     const int stride0 = a_str[reduce_dim0];\n",
      "4668                     const int stride1 = a_str[rd];\n",
      "4669                     for (int ii = 0; ii < dims_a[rd]; ++ii)\n",
      "4670                     {\n",
      "4671                         const float * a_data_ri = a_data_i + ii * stride1;\n",
      "4672                         const float * a_data_ri_max = a_data_ri + dims_a[reduce_dim0] * stride0;\n",
      "4673                         while (a_data_ri != a_data_ri_max)\n",
      "4674                         {\n",
      "4675                             sum += a_data_ri[0];\n",
      "4676                             a_data_ri += stride0;\n",
      "4677                         }\n",
      "4678                     }\n",
      "4679                 };\n",
      "4680                 break;\n",
      "4681             default:\n",
      "4682                 {\n",
      "4683                     for (unsigned int reduce_i = 0; reduce_i < n_reduce_elements; ++reduce_i)\n",
      "4684                     {\n",
      "4685                         //TODO: optimize this loop to work more like theano's Elemwise.  It's serial code.\n",
      "4686                         unsigned int reduce_ii = reduce_i;\n",
      "4687                         const float * a_data_ri = a_data_i;\n",
      "4688 \n",
      "4689                         //This loop finds the element in the a slice to add.\n",
      "4690                         for (unsigned int rd = reduce_dim0; rd < nd; ++rd)\n",
      "4691                         {\n",
      "4692                             unsigned int pos_d;\n",
      "4693                             if (a_str[rd] && (!z_str[rd])) // this means 'd' is a dimension we are reducing over\n",
      "4694                             {\n",
      "4695                                 if (log2_dims_a[rd]==-1)\n",
      "4696                                 {\n",
      "4697                                     // this branch is not preferred,\n",
      "4698                                     // because the manual said that integer mod and div operations are slow on gpu\n",
      "4699                                     pos_d = (reduce_ii % dims_a[rd]);\n",
      "4700                                     reduce_ii = (reduce_ii / dims_a[rd]);\n",
      "4701                                 }\n",
      "4702                                 else\n",
      "4703                                 {\n",
      "4704                                     pos_d = (reduce_ii & ((1 << log2_dims_a[rd])-1)); //take the lower log2_dims bits\n",
      "4705                                     reduce_ii = (reduce_ii >> log2_dims_a[rd]);  //shift those lower log2_dims bits off of ii\n",
      "4706                                 }\n",
      "4707                                 a_data_ri += pos_d * a_str[rd];\n",
      "4708                             }\n",
      "4709                         }\n",
      "4710                         sum += a_data_ri[0];\n",
      "4711                     }\n",
      "4712                 }\n",
      "4713         }\n",
      "4714         z_data_i[0] = sum;\n",
      "4715     }\n",
      "4716 }\n",
      "4717 \n",
      "4718 static __global__ void kernel_reduce_sum_1011(\n",
      "4719         const unsigned int d0,\n",
      "4720         const unsigned int d1,\n",
      "4721         const unsigned int d2,\n",
      "4722         const unsigned int d3,\n",
      "4723         const float *A, const int sA0, const int sA1, const int sA2, const int sA3,\n",
      "4724         float * Z, const int sZ0)\n",
      "4725 {\n",
      "4726     const int threadCount = blockDim.x * blockDim.y * blockDim.z;\n",
      "4727     const int threadNum = threadIdx.z * blockDim.x * blockDim.y + threadIdx.y * blockDim.x + threadIdx.x;\n",
      "4728     extern __shared__ float buf[];\n",
      "4729     float mysum = 0.0f;\n",
      "4730 \n",
      "4731     if (warpSize != 32)\n",
      "4732     {\n",
      "4733         return;  //TODO: set error code\n",
      "4734     }\n",
      "4735 \n",
      "4736     for (int i0 = threadIdx.z; i0 < d0; i0 += blockDim.z)\n",
      "4737     {\n",
      "4738         float Ai = A[i0 * sA0 + blockIdx.x * sA1 + threadIdx.y * sA2 + threadIdx.x * sA3];\n",
      "4739         mysum += Ai;\n",
      "4740     }\n",
      "4741     buf[threadNum] = mysum;\n",
      "4742     __syncthreads();\n",
      "4743 \n",
      "4744     // rest of function is handled by one warp\n",
      "4745     if (threadNum < warpSize)\n",
      "4746     {\n",
      "4747         for (int i = threadNum + warpSize; i < threadCount; i += warpSize)\n",
      "4748         {\n",
      "4749             mysum += buf[i];\n",
      "4750         }\n",
      "4751         buf[threadNum] = mysum;\n",
      "4752         if (threadNum < 16)\n",
      "4753         {\n",
      "4754             //reduce so that threadNum 0 has the sum of everything\n",
      "4755             if(threadNum + 16 < threadCount) buf[threadNum] += buf[threadNum+16];\n",
      "4756             if(threadNum + 8 < threadCount) buf[threadNum] += buf[threadNum+8];\n",
      "4757             if(threadNum + 4 < threadCount) buf[threadNum] += buf[threadNum+4];\n",
      "4758             if(threadNum + 2 < threadCount) buf[threadNum] += buf[threadNum+2];\n",
      "4759             if(threadNum + 1 < threadCount) buf[threadNum] += buf[threadNum+1];\n",
      "4760             if (threadNum == 0)\n",
      "4761             {\n",
      "4762                 Z[blockIdx.x*sZ0] = buf[0];\n",
      "4763             }\n",
      "4764         }\n",
      "4765     }\n",
      "4766 }\n",
      "4767 /**\n",
      "4768  * Dimensions in which the self has size 1 and A has size > 1 are considered summing dimensions\n",
      "4769  * Dimensions in which self has size > 1 and A has size > 1 are considered non-summing dimensions, and in this case their sizes must be equal.\n",
      "4770  */\n",
      "4771 int\n",
      "4772 CudaNdarray_reduce_sum(CudaNdarray * self, CudaNdarray * A)\n",
      "4773 {\n",
      "4774     int verbose = 0;\n",
      "4775     //check input rank\n",
      "4776     if (self->nd != A->nd)\n",
      "4777     {\n",
      "4778         PyErr_Format(PyExc_TypeError, \"Rank mismatch in CudaNdarray_sum: %i vs %i\", self->nd, A->nd);\n",
      "4779         return -1;\n",
      "4780     }\n",
      "4781     for (int i = 0; i < self->nd; ++i)\n",
      "4782     {\n",
      "4783         if ((CudaNdarray_HOST_DIMS(self)[i] > 1) && (CudaNdarray_HOST_DIMS(self)[i] != CudaNdarray_HOST_DIMS(A)[i]))\n",
      "4784         {\n",
      "4785             PyErr_Format(PyExc_TypeError, \"Dimension mismatch in CudaNdarray_sum: self->dim[%i] == %i , A->dim[%i] = %i\",\n",
      "4786                     i, CudaNdarray_HOST_DIMS(self)[i], i, CudaNdarray_HOST_DIMS(A)[i]);\n",
      "4787             return -1;\n",
      "4788         }\n",
      "4789     }\n",
      "4790 \n",
      "4791     int n_summations = (unsigned int)CudaNdarray_SIZE(self);\n",
      "4792     if (verbose)\n",
      "4793     {\n",
      "4794         std::cerr << \"reduce_sum n_summations \" << n_summations  << '\\n';\n",
      "4795         std::cerr << \"reduce_sum nd \" << self->nd  << '\\n';\n",
      "4796         fprint_CudaNdarray(stderr, A);\n",
      "4797         fprint_CudaNdarray(stderr, self);\n",
      "4798     }\n",
      "4799     if (0 && (A->nd == 4) //check to see if kernel_reduce_sum_1011 applies\n",
      "4800             && (CudaNdarray_HOST_DIMS(self)[0] == 1)\n",
      "4801             && (CudaNdarray_HOST_DIMS(self)[2] == 1)\n",
      "4802             && (CudaNdarray_HOST_DIMS(self)[3] == 1)\n",
      "4803        )\n",
      "4804     {\n",
      "4805         dim3 n_threads(CudaNdarray_HOST_DIMS(A)[3], CudaNdarray_HOST_DIMS(A)[2]);\n",
      "4806         dim3 n_blocks(CudaNdarray_HOST_DIMS(A)[1]);\n",
      "4807         while (n_threads.x * n_threads.y * n_threads.z < NUM_VECTOR_OP_THREADS_PER_BLOCK) ++n_threads.z;\n",
      "4808         n_threads.z -= 1;\n",
      "4809         if (n_threads.z > 64) n_threads.z = 64;\n",
      "4810         if (n_threads.z)\n",
      "4811         {\n",
      "4812             if (verbose) printf(\"trying kernel_reduce_sum_1011\\n\");\n",
      "4813             int n_shared = sizeof(float) * n_threads.x * n_threads.y * n_threads.z;\n",
      "4814             kernel_reduce_sum_1011<<<n_blocks, n_threads, n_shared>>>(\n",
      "4815                     CudaNdarray_HOST_DIMS(A)[0],\n",
      "4816                     CudaNdarray_HOST_DIMS(A)[1],\n",
      "4817                     CudaNdarray_HOST_DIMS(A)[2],\n",
      "4818                     CudaNdarray_HOST_DIMS(A)[3],\n",
      "4819                     CudaNdarray_DEV_DATA(A),\n",
      "4820                     CudaNdarray_HOST_STRIDES(A)[0],\n",
      "4821                     CudaNdarray_HOST_STRIDES(A)[1],\n",
      "4822                     CudaNdarray_HOST_STRIDES(A)[2],\n",
      "4823                     CudaNdarray_HOST_STRIDES(A)[3],\n",
      "4824                     CudaNdarray_DEV_DATA(self),\n",
      "4825                     CudaNdarray_HOST_STRIDES(self)[1]);\n",
      "4826             CNDA_THREAD_SYNC;\n",
      "4827             if (cudaSuccess == cudaGetLastError()) return 0;\n",
      "4828             if (verbose) printf(\"failed, falling back to kernel_reduce_sum\\n\");\n",
      "4829         }\n",
      "4830     }\n",
      "4831 \n",
      "4832     int n_threads_per_block = std::min(n_summations,\n",
      "4833             NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
      "4834     int n_blocks = std::min(ceil_intdiv(n_summations,n_threads_per_block),\n",
      "4835             NUM_VECTOR_OP_BLOCKS);\n",
      "4836     int n_structure_cache = self->nd * 4 * sizeof(int);\n",
      "4837 \n",
      "4838     if (verbose)\n",
      "4839     {\n",
      "4840         std::cerr << \"n_blocks, n_threads_per_block \" << n_blocks << ' ' << n_threads_per_block  << '\\n';\n",
      "4841     }\n",
      "4842     assert (self->nd > 0);\n",
      "4843     assert (self->nd == A->nd);\n",
      "4844     kernel_reduce_sum<<<n_blocks, n_threads_per_block, n_structure_cache>>>(\n",
      "4845             n_summations,\n",
      "4846             self->nd,\n",
      "4847             CudaNdarray_DEV_DIMS(A),\n",
      "4848             CudaNdarray_DEV_LOG2DIMS(A),\n",
      "4849             CudaNdarray_DEV_STRIDES(A),\n",
      "4850             CudaNdarray_DEV_DATA(A),\n",
      "4851             CudaNdarray_DEV_STRIDES(self),\n",
      "4852             CudaNdarray_DEV_DATA(self));\n",
      "4853     CNDA_THREAD_SYNC;\n",
      "4854     cudaError_t err = cudaGetLastError();\n",
      "4855     if (cudaSuccess != err)\n",
      "4856     {\n",
      "4857         PyErr_Format(PyExc_RuntimeError, \"Cuda error: %s: %s.\\n\", \"kernel_reduce_sum\", cudaGetErrorString(err));\n",
      "4858         return -1;\n",
      "4859     }\n",
      "4860     return 0;\n",
      "4861 }\n",
      "4862 int\n",
      "4863 CudaNdarray_reduce_prod(CudaNdarray * self, const CudaNdarray * A)\n",
      "4864 {\n",
      "4865     PyErr_SetString(PyExc_NotImplementedError, \"\");\n",
      "4866     return -1;\n",
      "4867 }\n",
      "4868 int\n",
      "4869 CudaNdarray_reduce_min(CudaNdarray * self, const CudaNdarray * A)\n",
      "4870 {\n",
      "4871     PyErr_SetString(PyExc_NotImplementedError, \"\");\n",
      "4872     return -1;\n",
      "4873 }\n",
      "4874 int\n",
      "4875 CudaNdarray_reduce_max(CudaNdarray * self, const CudaNdarray * A)\n",
      "4876 {\n",
      "4877     PyErr_SetString(PyExc_NotImplementedError, \"\");\n",
      "4878     return -1;\n",
      "4879 }\n",
      "4880 \n",
      "4881 \n",
      "4882 /**\n",
      "4883  *\n",
      "4884  *  pattern is a permutation of [0, 1, ... self->nd-1] with the following twists:\n",
      "4885  *  - an element 'd' of the permutation can be dropped if CudaNdarray_HOST_DIMS(self)[d] == 1\n",
      "4886  *  - any number of '-1' elements can be in the pattern, and they will cause new ranks (with dim==1) to be inserted.\n",
      "4887  *\n",
      "4888  *  For example, if CudaNdarray_HOST_DIMS(self) == [4, 5, 1, 6], and pattern = [0,3,-1,-1, 1], then CudaNdarray_HOST_DIMS(self) would be modified to become:\n",
      "4889  *     [4, 6, 1, 1, 5] (we dropped the original dim[2]==1, and inserted two singleton dimensions with the -1s.\n",
      "4890  */\n",
      "4891 int\n",
      "4892 CudaNdarray_dimshuffle(CudaNdarray * self, unsigned int len, const int * pattern)\n",
      "4893 {\n",
      "4894     //TODO: pass a workspace pointer to avoid the internal malloc\n",
      "4895     int * newdims = (int *)malloc(sizeof(int) * (len + len + self->nd)); //we tack on the taken buffer here for speed of not having to malloc twice.\n",
      "4896     int * newstrides = newdims + len;\n",
      "4897     int * dims_taken = newstrides + len;\n",
      "4898     if (!newdims)\n",
      "4899     {\n",
      "4900         PyErr_SetString(PyExc_MemoryError, \"CudaNdarray_dimshuffle: Failed to allocate temporary space\");\n",
      "4901         return -1;\n",
      "4902     }\n",
      "4903     for (int i = 0; i < self->nd; ++i)\n",
      "4904     {\n",
      "4905         dims_taken[i] = 0;\n",
      "4906     }\n",
      "4907     for (int i = 0; i < len; ++i)\n",
      "4908     {\n",
      "4909         if (pattern[i] < 0)\n",
      "4910         {\n",
      "4911             newdims[i] = 1;\n",
      "4912             newstrides[i] = 0;\n",
      "4913         }\n",
      "4914         else if(dims_taken[pattern[i]])\n",
      "4915         {\n",
      "4916             PyErr_Format(PyExc_ValueError, \"Cudandarray_dimshuffle: invalid pattern for Cudandarray_dimshuffle. You used the dimensions %d multiple time\",\n",
      "4917                          pattern[i]);\n",
      "4918             free(newdims);\n",
      "4919             return -1;\n",
      "4920         }\n",
      "4921         else if (pattern[i]>= self->nd)\n",
      "4922         {\n",
      "4923             PyErr_Format(PyExc_ValueError, \"Cudandarray_dimshuffle: invalid pattern for Cudandarray_dimshuffle. You asked for a dimensions that don't exist %d for a %d dims CudaNdarray\",\n",
      "4924                          pattern[i], self->nd);\n",
      "4925             free(newdims);\n",
      "4926             return -1;\n",
      "4927         }\n",
      "4928         else\n",
      "4929         {\n",
      "4930             newdims[i] = CudaNdarray_HOST_DIMS(self)[pattern[i]];\n",
      "4931             newstrides[i] = CudaNdarray_HOST_STRIDES(self)[pattern[i]];\n",
      "4932             dims_taken[pattern[i]] = 1;\n",
      "4933         }\n",
      "4934     }\n",
      "4935     //Check if we dropped not broadcastable dims\n",
      "4936     for (int i = 0; i < self->nd; ++i)\n",
      "4937     {\n",
      "4938         if (dims_taken[i]==0 && CudaNdarray_HOST_DIMS(self)[i]!=1)\n",
      "4939         {\n",
      "4940             PyErr_SetString(PyExc_ValueError, \"Cudandarray_dimshuffle: You cannot drop a non-broadcastable dimension.\");\n",
      "4941             free(newdims);\n",
      "4942             return -1;\n",
      "4943         }\n",
      "4944     }\n",
      "4945     //swap this structure in for the one in self, and sync to the card\n",
      "4946     if (CudaNdarray_set_nd(self, len))\n",
      "4947     {\n",
      "4948         free(newdims);\n",
      "4949         return -1;\n",
      "4950     }\n",
      "4951     for (int i = 0; i < len; ++i)\n",
      "4952     {\n",
      "4953         CudaNdarray_set_dim(self, i, newdims[i]);\n",
      "4954         CudaNdarray_set_stride(self, i, newstrides[i]);\n",
      "4955     }\n",
      "4956     if (cnda_copy_structure_to_device(self))\n",
      "4957     {\n",
      "4958         free(newdims);\n",
      "4959         return -1;\n",
      "4960     }\n",
      "4961     free(newdims);\n",
      "4962     return 0;\n",
      "4963 }\n",
      "4964 \n",
      "4965 \n",
      "4966 \n",
      "4967 /**\n",
      "4968  *\n",
      "4969  *  This is the function that bind to python.\n",
      "4970  *  See CudaNdarray_dimshuffle to call from C.\n",
      "4971  *  We use -1 to mean 'x' as in Tensor Dimshuffle.\n",
      "4972  */\n",
      "4973 PyObject *\n",
      "4974 CudaNdarray_Dimshuffle(PyObject* _unused, PyObject* args)\n",
      "4975 {\n",
      "4976     PyObject * self = NULL;\n",
      "4977     PyObject * pattern_object = NULL;\n",
      "4978     int * pattern = NULL;\n",
      "4979     PyObject * rval = NULL;\n",
      "4980     int success = -1;\n",
      "4981     //const int * dims = NULL;\n",
      "4982 \n",
      "4983     //args should consist of two python objects (\"OO\")\n",
      "4984     if (! PyArg_ParseTuple(args, \"OO\", &self, &pattern_object))\n",
      "4985         return NULL;\n",
      "4986 \n",
      "4987     if (!CudaNdarray_Check(self) )\n",
      "4988     {\n",
      "4989         PyErr_SetString(PyExc_TypeError, \"First argument to cuda_ndarray.dimshuffle must be a CudaNdarray\");\n",
      "4990         return NULL;\n",
      "4991     }\n",
      "4992 \n",
      "4993     //parse pattern_object into int * pattern\n",
      "4994 \n",
      "4995     Py_ssize_t pattern_dim =  PyObject_Length(pattern_object);\n",
      "4996 \n",
      "4997     if (pattern_dim < 0)\n",
      "4998     {\n",
      "4999         PyErr_SetString(PyExc_TypeError, \"Couldn't get length of third argument to cuda_ndarray.dimshuffle\");\n",
      "5000         return NULL;\n",
      "5001     }\n",
      "5002 \n",
      "5003     pattern = (int *) malloc( pattern_dim * sizeof(int));\n",
      "5004 \n",
      "5005     for (Py_ssize_t i = 0; i < pattern_dim; i++)\n",
      "5006     {\n",
      "5007         PyObject * idx = PyLong_FromLong(i);\n",
      "5008 \n",
      "5009         if (idx == NULL)\n",
      "5010         {\n",
      "5011             PyErr_SetString(PyExc_Exception, \"Couldn't make long object to loop over list/tuple\");\n",
      "5012             goto CudaNdarray_dimshuffle_fail;\n",
      "5013         }\n",
      "5014 \n",
      "5015         long elem_value = 0;\n",
      "5016 \n",
      "5017         PyObject * elem = PyObject_GetItem(pattern_object, idx);\n",
      "5018 \n",
      "5019         if (elem == NULL)\n",
      "5020         {\n",
      "5021             Py_XDECREF( elem);\n",
      "5022             PyErr_SetString(PyExc_ValueError, \"Third argument to dimshuffle must be list or tuple of integers\");\n",
      "5023             goto CudaNdarray_dimshuffle_fail;\n",
      "5024         }\n",
      "5025 \n",
      "5026         elem_value = PyInt_AsLong(elem);\n",
      "5027 \n",
      "5028         if (elem_value == -1 && PyErr_Occurred() )\n",
      "5029         {\n",
      "5030             Py_XDECREF(elem);\n",
      "5031             PyErr_SetString(PyExc_ValueError, \"Third argument to dimshuffle must be list or tuple of integers\");\n",
      "5032             goto CudaNdarray_dimshuffle_fail;\n",
      "5033         }\n",
      "5034 \n",
      "5035         pattern[i] = elem_value;\n",
      "5036 \n",
      "5037         Py_XDECREF( elem );\n",
      "5038         Py_XDECREF( idx );\n",
      "5039     }\n",
      "5040 \n",
      "5041     //allocate rval\n",
      "5042     rval =  (PyObject *) CudaNdarray_View((CudaNdarray *) self);\n",
      "5043 \n",
      "5044     if (rval == NULL)\n",
      "5045     {\n",
      "5046         //CudaNdarray_New should have set the exception string\n",
      "5047         goto CudaNdarray_dimshuffle_fail;\n",
      "5048     }\n",
      "5049 \n",
      "5050 \n",
      "5051     //printf(\"pattern_dim: %d\\n\",pattern_dim);\n",
      "5052     //printf(\"pattern: %d %d\\n\",pattern[0],pattern[1]);\n",
      "5053     //dims = CudaNdarray_HOST_DIMS( (CudaNdarray *) self);\n",
      "5054     //printf(\"dims before: %d %d\\n\",dims[0],dims[1]);\n",
      "5055 \n",
      "5056     success = CudaNdarray_dimshuffle((CudaNdarray *) rval, pattern_dim, pattern);\n",
      "5057 \n",
      "5058     if (success != 0)\n",
      "5059     {\n",
      "5060         //Exception string should already be set by CudaNdarray_dimshuffle\n",
      "5061         goto CudaNdarray_dimshuffle_fail;\n",
      "5062     }\n",
      "5063 \n",
      "5064     free(pattern);\n",
      "5065 \n",
      "5066     return rval;\n",
      "5067 \n",
      "5068     CudaNdarray_dimshuffle_fail:\n",
      "5069 \n",
      "5070     if (pattern != NULL)\n",
      "5071         free(pattern);\n",
      "5072 \n",
      "5073     Py_XDECREF(rval);\n",
      "5074     return NULL;\n",
      "5075 }\n",
      "5076 \n",
      "5077 \n",
      "5078 int\n",
      "5079 cnda_structure_size(int nd)\n",
      "5080 {\n",
      "5081     // dim0, dim1, ...\n",
      "5082     // str0, str1, ...\n",
      "5083     // log2(dim0), log2(dim1), ...\n",
      "5084     return nd + nd + nd;\n",
      "5085 }\n",
      "5086 \n",
      "5087 const int *\n",
      "5088 CudaNdarray_HOST_DIMS(const CudaNdarray * self)\n",
      "5089 {\n",
      "5090     return self->host_structure;\n",
      "5091 }\n",
      "5092 \n",
      "5093 const int *\n",
      "5094 CudaNdarray_HOST_STRIDES(const CudaNdarray * self)\n",
      "5095 {\n",
      "5096     return self->host_structure + self->nd;\n",
      "5097 }\n",
      "5098 const int *\n",
      "5099 CudaNdarray_HOST_LOG2DIMS(const CudaNdarray * self)\n",
      "5100 {\n",
      "5101     return self->host_structure + 2*self->nd;\n",
      "5102 }\n",
      "5103 \n",
      "5104 int\n",
      "5105 CudaNdarray_EqualAndIgnore(CudaNdarray *cnda1, CudaNdarray *cnda2, int ignoreSync, int ignoreBase)\n",
      "5106 {\n",
      "5107     int verbose = 0;\n",
      "5108 \n",
      "5109     if (!ignoreSync && cnda1->dev_structure_fresh != cnda2->dev_structure_fresh)\n",
      "5110     {\n",
      "5111         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 1\\n\");\n",
      "5112         return 0;\n",
      "5113     }\n",
      "5114 \n",
      "5115     if (cnda1->nd != cnda2->nd)\n",
      "5116     {\n",
      "5117         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 2\\n\");\n",
      "5118         return 0;\n",
      "5119     }\n",
      "5120 \n",
      "5121     for (int i=0; i < 2*cnda1->nd; i++)\n",
      "5122     {\n",
      "5123         if (cnda1->host_structure[i] != cnda2->host_structure[i])\n",
      "5124         {\n",
      "5125             if(verbose)\n",
      "5126                 fprintf(stdout, \"CUDANDARRAY_EQUAL : host_structure : %d, %d, %d\\n\", i, cnda1->host_structure[i], cnda2->host_structure[i]);\n",
      "5127             return 0;\n",
      "5128         }\n",
      "5129     }\n",
      "5130 \n",
      "5131     if (!ignoreBase && cnda1->base != cnda2->base)\n",
      "5132     {\n",
      "5133         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 4\");\n",
      "5134         return 0;\n",
      "5135     }\n",
      "5136     else if (cnda1->data_allocated != cnda2->data_allocated)\n",
      "5137     {\n",
      "5138         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 5\");\n",
      "5139         return 0;\n",
      "5140     }\n",
      "5141     else if (cnda1->data_allocated && cnda1->devdata != cnda2->devdata)\n",
      "5142     {\n",
      "5143         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 6\");\n",
      "5144         // no need to check devdata if data is not allocated\n",
      "5145         return 0;\n",
      "5146     }\n",
      "5147 \n",
      "5148     return 1;\n",
      "5149 }\n",
      "5150 \n",
      "5151 \n",
      "5152 int\n",
      "5153 CudaNdarray_Equal(CudaNdarray *cnda1, CudaNdarray *cnda2)\n",
      "5154 {\n",
      "5155     return CudaNdarray_EqualAndIgnore(cnda1, cnda2, 0, 0);\n",
      "5156 }\n",
      "5157 \n",
      "5158 int\n",
      "5159 cnda_copy_structure_to_device(const CudaNdarray * self)\n",
      "5160 {\n",
      "5161     //If the device structure do not exists, create it.\n",
      "5162     //We allocate it here as we do not need it often.\n",
      "5163     //In fact, we need it so infrequently that we expect\n",
      "5164     //that most object won't need it. Not allocating it\n",
      "5165     //save a significant when creating object.\n",
      "5166     //This speed up a benchmark by 8% with the gc.\n",
      "5167     if (!self->dev_structure)\n",
      "5168     {\n",
      "5169         int struct_size = cnda_structure_size(self->nd);\n",
      "5170         if (struct_size)\n",
      "5171         {\n",
      "5172             self->dev_structure = (int*)device_malloc(struct_size* sizeof(int));\n",
      "5173             if (NULL == self->dev_structure)\n",
      "5174             {\n",
      "5175                 return -1;\n",
      "5176             }\n",
      "5177         }\n",
      "5178     }\n",
      "5179     if (cublasSetVector(cnda_structure_size(self->nd),\n",
      "5180                         sizeof(int),\n",
      "5181                         self->host_structure,\n",
      "5182                         1,\n",
      "5183                         self->dev_structure,\n",
      "5184                         1) != CUBLAS_STATUS_SUCCESS)\n",
      "5185     {\n",
      "5186         PyErr_SetString(PyExc_RuntimeError, \"error copying structure to device memory\");\n",
      "5187         return -1;\n",
      "5188     }\n",
      "5189     self->dev_structure_fresh = 1;\n",
      "5190     return 0;\n",
      "5191 }\n",
      "5192 \n",
      "5193 const int *\n",
      "5194 CudaNdarray_DEV_DIMS(const CudaNdarray * self)\n",
      "5195 {\n",
      "5196     if (!self->dev_structure_fresh)\n",
      "5197     {\n",
      "5198         if (cnda_copy_structure_to_device(self))\n",
      "5199             return NULL;\n",
      "5200     }\n",
      "5201     return self->dev_structure;\n",
      "5202 }\n",
      "5203 const int *\n",
      "5204 CudaNdarray_DEV_STRIDES(const CudaNdarray * self)\n",
      "5205 {\n",
      "5206     if (!self->dev_structure_fresh)\n",
      "5207     {\n",
      "5208         if (cnda_copy_structure_to_device(self))\n",
      "5209             return NULL;\n",
      "5210     }\n",
      "5211     return self->dev_structure + self->nd;\n",
      "5212 }\n",
      "5213 const int *\n",
      "5214 CudaNdarray_DEV_LOG2DIMS(const CudaNdarray * self)\n",
      "5215 {\n",
      "5216     if (!self->dev_structure_fresh)\n",
      "5217     {\n",
      "5218         if (cnda_copy_structure_to_device(self))\n",
      "5219             return NULL;\n",
      "5220     }\n",
      "5221     return self->dev_structure + 2*self->nd;\n",
      "5222 }\n",
      "5223 float *\n",
      "5224 CudaNdarray_DEV_DATA(const CudaNdarray * self)\n",
      "5225 {\n",
      "5226     return self->devdata;\n",
      "5227 }\n",
      "5228 \n",
      "5229 /**\n",
      "5230  * Return the number of elements in the ndarray (product of the dimensions)\n",
      "5231  */\n",
      "5232 size_t\n",
      "5233 CudaNdarray_SIZE(const CudaNdarray *self)\n",
      "5234 {\n",
      "5235     if (self->nd == -1) return 0;\n",
      "5236     size_t size = 1;\n",
      "5237     for (int i = 0; i < self->nd; ++i)\n",
      "5238     {\n",
      "5239         size *= CudaNdarray_HOST_DIMS(self)[i];\n",
      "5240     }\n",
      "5241     return size;\n",
      "5242 }\n",
      "5243 \n",
      "5244 PyObject *\n",
      "5245 CudaNdarray_SIZE_Object(const CudaNdarray *self, void *closure)\n",
      "5246 {\n",
      "5247     return PyInt_FromLong(CudaNdarray_SIZE(self));\n",
      "5248 }\n",
      "5249 \n",
      "5250 int CudaNdarray_set_device_data(CudaNdarray * self, float * data, const CudaNdarray * base)\n",
      "5251 {\n",
      "5252     return CudaNdarray_set_device_data(self, data, (PyObject *) base);\n",
      "5253 }\n",
      "5254 \n",
      "5255 PyObject * CudaNdarray_IS_C_Contiguous(CudaNdarray * self)\n",
      "5256 {\n",
      "5257     return PyBool_FromLong(CudaNdarray_is_c_contiguous(self));\n",
      "5258 }\n",
      "5259 \n",
      "5260 int fprint_CudaNdarray(FILE * fd, const CudaNdarray *self)\n",
      "5261 {\n",
      "5262     cudaError_t err = cudaGetLastError();\n",
      "5263     if( cudaSuccess != err)\n",
      "5264     {\n",
      "5265         PyErr_Format(PyExc_RuntimeError,\n",
      "5266                      \"Cuda error: %s: %s.\",\n",
      "5267                      \"fprint_CudaNdarray was called with an uncleared error\",\n",
      "5268                      cudaGetErrorString(err));\n",
      "5269         return -1;\n",
      "5270     }\n",
      "5271     fprintf(fd, \"CudaNdarray <%p, %p> nd=%i dev_structure_fresh=%d data_allocated=%d\\n\",\n",
      "5272             self, self->devdata, self->nd, self->dev_structure_fresh, self->data_allocated);\n",
      "5273     fprintf(fd, \"\\tHOST_DIMS:      \");\n",
      "5274     for (int i = 0; i < self->nd; ++i)\n",
      "5275     {\n",
      "5276         fprintf(fd, \"%i\\t\", CudaNdarray_HOST_DIMS(self)[i]);\n",
      "5277     }\n",
      "5278     fprintf(fd, \"\\n\\tHOST_STRIDES: \");\n",
      "5279     for (int i = 0; i < self->nd; ++i)\n",
      "5280     {\n",
      "5281         fprintf(fd, \"%i\\t\", CudaNdarray_HOST_STRIDES(self)[i]);\n",
      "5282     }\n",
      "5283 \n",
      "5284     if (self->dev_structure)\n",
      "5285     {\n",
      "5286         int data=0;\n",
      "5287         fprintf(fd, \"\\n\\tDEV_DIMS:      \");\n",
      "5288         for (int i = 0; i < self->nd; ++i)\n",
      "5289         {\n",
      "5290             cublasGetVector(1, sizeof(int),\n",
      "5291                             self->dev_structure+i, 1,\n",
      "5292                             &data, 1);\n",
      "5293             fprintf(fd, \"%i\\t\", data);\n",
      "5294         }\n",
      "5295         fprintf(fd, \"\\n\\tDEV_STRIDES: \");\n",
      "5296         for (int i = 0; i < self->nd; ++i)\n",
      "5297         {\n",
      "5298             cublasGetVector(1, sizeof(int),\n",
      "5299                             self->dev_structure + self->nd+i, 1,\n",
      "5300                             &data, 1);\n",
      "5301             fprintf(fd, \"%i \\t\", data);\n",
      "5302         }\n",
      "5303         fprintf(fd, \"\\n\");\n",
      "5304     }\n",
      "5305     else\n",
      "5306     {\n",
      "5307         fprintf(fd, \"\\n\\tdev_structure not allocated\\n\");\n",
      "5308     }\n",
      "5309 \n",
      "5310     err = cudaGetLastError();\n",
      "5311     if( cudaSuccess != err)\n",
      "5312     {\n",
      "5313         PyErr_Format(PyExc_RuntimeError,\n",
      "5314                      \"Cuda error: %s: %s.\",\n",
      "5315                      \"fprint_CudaNdarray\",\n",
      "5316                      cudaGetErrorString(err));\n",
      "5317         return -1;\n",
      "5318     }\n",
      "5319     return 0;\n",
      "5320 }\n",
      "5321 \n",
      "5322 \n",
      "5323 int CudaNdarray_prep_output(CudaNdarray ** arr, int nd,\n",
      "5324                             const int * dims, int fortran)\n",
      "5325 {\n",
      "5326     bool allocated = false;\n",
      "5327     if (*arr == NULL)\n",
      "5328     {\n",
      "5329         // This allocates the metadata but not the data\n",
      "5330         *arr = (CudaNdarray *) CudaNdarray_new_nd(nd);\n",
      "5331         if (*arr == NULL)\n",
      "5332             return -1;\n",
      "5333         allocated = true;\n",
      "5334     }\n",
      "5335 \n",
      "5336     if (CudaNdarray_alloc_contiguous(*arr, nd, dims, fortran))\n",
      "5337     {\n",
      "5338         if (allocated)\n",
      "5339         {\n",
      "5340             Py_DECREF(*arr);\n",
      "5341             *arr = NULL;\n",
      "5342         }\n",
      "5343         return -1;\n",
      "5344     }\n",
      "5345     return 0;\n",
      "5346 }\n",
      "5347 \n",
      "5348 \n",
      "5349 /*\n",
      "5350   Local Variables:\n",
      "5351   mode:c++\n",
      "5352   c-basic-offset:4\n",
      "5353   c-file-style:\"stroustrup\"\n",
      "5354   indent-tabs-mode:nil\n",
      "5355   fill-column:79\n",
      "5356   End:\n",
      "5357 */\n",
      "5358 // vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:textwidth=79 :\n",
      "5359 \n",
      "===============================\n",
      "nvcc fatal   : The version ('70300') of the host compiler ('Apple clang') is not supported\n",
      "ERROR (theano.sandbox.cuda): Failed to compile cuda_ndarray.cu: ('nvcc return status', 1, 'for cmd', 'nvcc -shared -O3 -m64 -Xcompiler -DCUDA_NDARRAY_CUH=c72d035fdf91890f3b36710688069b2e,-DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION,-fPIC,-fvisibility=hidden -Xlinker -rpath,/Users/esphinx/.theano/compiledir_Darwin-15.4.0-x86_64-i386-64bit-i386-2.7.11-64/cuda_ndarray -I/Users/esphinx/anaconda/lib/python2.7/site-packages/theano/sandbox/cuda -I/usr/local/lib/python2.7/site-packages/numpy/core/include -I/Users/esphinx/anaconda/include/python2.7 -I/Users/esphinx/anaconda/lib/python2.7/site-packages/theano/gof -o /Users/esphinx/.theano/compiledir_Darwin-15.4.0-x86_64-i386-64bit-i386-2.7.11-64/cuda_ndarray/cuda_ndarray.so mod.cu -L/Users/esphinx/anaconda/lib -lcublas -lcudart -Xcompiler -undefined,dynamic_lookup -Xlinker -pie')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['nvcc', '-shared', '-O3', '-m64', '-Xcompiler', '-DCUDA_NDARRAY_CUH=c72d035fdf91890f3b36710688069b2e,-DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION,-fPIC,-fvisibility=hidden', '-Xlinker', '-rpath,/Users/esphinx/.theano/compiledir_Darwin-15.4.0-x86_64-i386-64bit-i386-2.7.11-64/cuda_ndarray', '-I/Users/esphinx/anaconda/lib/python2.7/site-packages/theano/sandbox/cuda', '-I/usr/local/lib/python2.7/site-packages/numpy/core/include', '-I/Users/esphinx/anaconda/include/python2.7', '-I/Users/esphinx/anaconda/lib/python2.7/site-packages/theano/gof', '-o', '/Users/esphinx/.theano/compiledir_Darwin-15.4.0-x86_64-i386-64bit-i386-2.7.11-64/cuda_ndarray/cuda_ndarray.so', 'mod.cu', '-L/Users/esphinx/anaconda/lib', '-lcublas', '-lcudart', '-Xcompiler', '-undefined,dynamic_lookup', '-Xlinker', '-pie']\n"
     ]
    }
   ],
   "source": [
    "# Compute gradients using Theano\n",
    "# Hidden state\n",
    "#th_h = TT.tanh(TT.dot(th_x, th_W_xh) + TT.dot(th_h0, th_W_hh))\n",
    "#th_yhat = TT.tanh(TT.dot(th_h, th_W_hy))\n",
    "def step(th_x_t, th_h_tm1, th_W_xh, th_W_hh, th_W_hy):\n",
    "    th_h_t = TT.dot(th_x_t, th_W_xh) + TT.dot(th_h_tm1, th_W_hh)\n",
    "    th_yhat_t = TT.dot(th_h_t, th_W_hy)\n",
    "    return th_h_t, th_yhat_t\n",
    "\n",
    "[th_h, th_yhat], _ = theano.scan(step,\n",
    "                                 sequences=th_x,\n",
    "                                 outputs_info=[th_h0, None],\n",
    "                                 non_sequences=[th_W_xh, th_W_hh, \n",
    "                                                th_W_hy])\n",
    "\n",
    "# Gradients using Cost function\n",
    "th_gW_xh, th_gW_hh, th_gW_hy = \\\n",
    "        TT.grad(0.5 * ((th_yhat - th_y)**2).sum(),\n",
    "               [th_W_xh, th_W_hh, th_W_hy])\n",
    "th_fn = theano.function(\n",
    "        [th_x, th_h0, th_y, th_W_xh, th_W_hh, th_W_hy],\n",
    "        [th_gW_xh, th_gW_hh, th_gW_hy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Numerical examples\n",
    "T = 10\n",
    "np_h0 = np.random.uniform(size=(nh,))\n",
    "np_x = np.random.uniform(size=(T, nin))\n",
    "np_y = np.random.uniform(size=(T, nout))\n",
    "\n",
    "np_W_xh = np.random.uniform(size=(nin, nh))\n",
    "np_W_hh = np.random.uniform(size=(nh, nh))\n",
    "np_W_hy = np.random.uniform(size=(nh, nout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute gradients using Numpy\n",
    "#np_h = np.tanh(np.dot(np_x, np_W_xh) + np.dot(np_h0, np_W_hh))\n",
    "#np_y = np.tanh(np.dot(np_h, np_W_hy))\n",
    "\n",
    "# Forward propagation\n",
    "def forwardPropagation(np_x, np_h0, W_xh, W_hh, W_hy):\n",
    "    h_tm1 = np_h0\n",
    "    T = np_x.shape[0]\n",
    "    nin, nh = W_xh.shape\n",
    "    nout = W_hy.shape[1]\n",
    "    np_h = np.zeros((T+1, nh))\n",
    "    np_h[0,:] = np_h0\n",
    "    np_yhat = np.zeros((T+1, nout))\n",
    "    for t in xrange(1, T+1):\n",
    "        h_t = np.dot(np_x[t-1], W_xh) + np.dot(h_tm1, W_hh)\n",
    "        np_h[t, :] = h_t\n",
    "        h_tm1 = h_t\n",
    "        yhat = np.dot(h_t, W_hy)\n",
    "        np_yhat[t, :] = yhat\n",
    "    return np_h, np_yhat\n",
    "    \n",
    "np_h, np_yhat = forwardPropagation(np_x, np_h0, np_W_xh, np_W_hh, np_W_hy)\n",
    "loss = 0.5 * ((np_yhat[1:,:] - np_y)**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th_dE_xh:\n",
      "[[ 107124.3642154    66946.95200285   87496.67318075   56405.26129492]]\n",
      "th_dE_hh:\n",
      "[[ 869269.99195692  547150.25500308  721317.96648808  457142.57060317]\n",
      " [ 935828.73067963  589457.89111612  777747.07216035  492084.3838109 ]\n",
      " [ 283882.65488322  178320.86886155  234487.71733382  149348.00445349]\n",
      " [ 653380.1672786   411334.08543318  542387.71432151  343596.10833849]]\n",
      "th_dE_hy:\n",
      "[[ 191627.64265774]\n",
      " [ 225980.52998534]\n",
      " [  45158.0464313 ]\n",
      " [ 147792.27719617]]\n"
     ]
    }
   ],
   "source": [
    "# Theano gradients\n",
    "th_dE_xh, th_dE_hh, th_dE_hy = \\\n",
    "    th_fn(np_x, np_h0, np_y, np_W_xh, np_W_hh, np_W_hy)\n",
    "print \"th_dE_xh:\"\n",
    "print th_dE_xh\n",
    "print \"th_dE_hh:\"\n",
    "print th_dE_hh\n",
    "print \"th_dE_hy:\"\n",
    "print th_dE_hy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np_dE_hy:\n",
      "[[ 191627.64265774]\n",
      " [ 225980.52998534]\n",
      " [  45158.0464313 ]\n",
      " [ 147792.27719617]]\n"
     ]
    }
   ],
   "source": [
    "# Numpy gradients\n",
    "# np_h : T  x nh, np_yhat : T x nout\n",
    "np_dE_hy = np.dot(np_h[1:,:].T, (np_yhat[1:, :] - np_y))\n",
    "print \"np_dE_hy:\"\n",
    "print np_dE_hy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dE_hh:\n",
      "[[ 869269.99195692  547150.25500308  721317.96648808  457142.57060317]\n",
      " [ 935828.73067963  589457.89111612  777747.07216035  492084.3838109 ]\n",
      " [ 283882.65488322  178320.86886155  234487.71733382  149348.00445349]\n",
      " [ 653380.1672786   411334.08543318  542387.71432151  343596.10833849]]\n",
      "th_dE_hh:\n",
      "[[ 869269.99195692  547150.25500308  721317.96648808  457142.57060317]\n",
      " [ 935828.73067963  589457.89111612  777747.07216035  492084.3838109 ]\n",
      " [ 283882.65488322  178320.86886155  234487.71733382  149348.00445349]\n",
      " [ 653380.1672786   411334.08543318  542387.71432151  343596.10833849]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "dE_hh = np.zeros((nh, nh))\n",
    "\n",
    "dhh = np.zeros((T+1, nh))\n",
    "for t in xrange(T,0,-1):\n",
    "    # dE_t/dy_t * dy_t/dh_t\n",
    "    dhh[t] += np.dot(np_W_hy, (np_yhat[t,:] - np_y[t-1,:]))\n",
    "    dhh[t-1] += np.dot(np_W_hh, dhh[t])\n",
    "    dE_hh += np.outer(np_h[t-1,:], dhh[t])\n",
    "print \"dE_hh:\"\n",
    "print dE_hh\n",
    "print \"th_dE_hh:\"\n",
    "print th_dE_hh\n",
    "print np.allclose(dE_hh, th_dE_hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dE_xh:\n",
      "[[ 107124.3642154    66946.95200285   87496.67318075   56405.26129492]]\n",
      "th_dE_xh:\n",
      "[[ 107124.3642154    66946.95200285   87496.67318075   56405.26129492]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "dE_xh = np.zeros((nin, nh))\n",
    "\n",
    "dhh = np.zeros((T+1, nh))\n",
    "for t in xrange(T,0,-1):\n",
    "    # dE_t/dy_t * dy_t/dh_t\n",
    "    dhh[t] += np.dot(np_W_hy, (np_yhat[t,:] - np_y[t-1,:]))\n",
    "    dhh[t-1] += np.dot(np_W_hh, dhh[t])\n",
    "    dE_xh += np.outer(np_x[t-1,:], dhh[t])\n",
    "print \"dE_xh:\"\n",
    "print dE_xh\n",
    "print \"th_dE_xh:\"\n",
    "print th_dE_xh\n",
    "print np.allclose(dE_xh, th_dE_xh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients with tanh non-linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute gradients using Theano\n",
    "# Hidden state\n",
    "#th_h = TT.tanh(TT.dot(th_x, th_W_xh) + TT.dot(th_h0, th_W_hh))\n",
    "#th_yhat = TT.tanh(TT.dot(th_h, th_W_hy))\n",
    "def step(th_x_t, th_h_tm1, th_W_xh, th_W_hh, th_W_hy):\n",
    "    th_h_t = TT.tanh(TT.dot(th_x_t, th_W_xh) + TT.dot(th_h_tm1, th_W_hh))\n",
    "    th_yhat_t = TT.tanh(TT.dot(th_h_t, th_W_hy))\n",
    "    return th_h_t, th_yhat_t\n",
    "\n",
    "[th_h, th_yhat], _ = theano.scan(step,\n",
    "                                 sequences=th_x,\n",
    "                                 outputs_info=[th_h0, None],\n",
    "                                 non_sequences=[th_W_xh, th_W_hh, \n",
    "                                                th_W_hy])\n",
    "\n",
    "# Gradients using Cost function\n",
    "th_gW_xh, th_gW_hh, th_gW_hy = \\\n",
    "        TT.grad(0.5 * ((th_yhat - th_y)**2).sum(),\n",
    "               [th_W_xh, th_W_hh, th_W_hy])\n",
    "th_fn = theano.function(\n",
    "        [th_x, th_h0, th_y, th_W_xh, th_W_hh, th_W_hy],\n",
    "        [th_gW_xh, th_gW_hh, th_gW_hy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Numerical examples\n",
    "T = 10\n",
    "np_h0 = np.random.uniform(size=(nh,))\n",
    "np_x = np.random.uniform(size=(T, nin))\n",
    "np_y = np.random.uniform(size=(T, nout))\n",
    "\n",
    "np_W_xh = np.random.uniform(size=(nin, nh))\n",
    "np_W_hh = np.random.uniform(size=(nh, nh))\n",
    "np_W_hy = np.random.uniform(size=(nh, nout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute gradients using Numpy\n",
    "#np_h = np.tanh(np.dot(np_x, np_W_xh) + np.dot(np_h0, np_W_hh))\n",
    "#np_y = np.tanh(np.dot(np_h, np_W_hy))\n",
    "\n",
    "# Forward propagation\n",
    "def forwardPropagation(np_x, np_h0, W_xh, W_hh, W_hy):\n",
    "    h_tm1 = np_h0\n",
    "    T = np_x.shape[0]\n",
    "    nin, nh = W_xh.shape\n",
    "    nout = W_hy.shape[1]\n",
    "    np_h = np.zeros((T+1, nh))\n",
    "    np_h[0,:] = np_h0\n",
    "    np_yhat = np.zeros((T+1, nout))\n",
    "    for t in xrange(1, T+1):\n",
    "        h_t = np.tanh(np.dot(np_x[t-1], W_xh) + np.dot(h_tm1, W_hh))\n",
    "        np_h[t, :] = h_t\n",
    "        h_tm1 = h_t\n",
    "        yhat = np.tanh(np.dot(h_t, W_hy))\n",
    "        np_yhat[t, :] = yhat\n",
    "    return np_h, np_yhat\n",
    "    \n",
    "np_h, np_yhat = forwardPropagation(np_x, np_h0, np_W_xh, np_W_hh, np_W_hy)\n",
    "loss = 0.5 * ((np_yhat[1:,:] - np_y)**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th_dE_xh:\n",
      "[[ 0.00010295  0.00259489  0.04856052  0.00426662]]\n",
      "th_dE_hh:\n",
      "[[ 0.00029612  0.00626318  0.09047982  0.00933075]\n",
      " [ 0.00028583  0.00608747  0.08944656  0.00915609]\n",
      " [ 0.00025834  0.00546198  0.07707065  0.008035  ]\n",
      " [ 0.00023314  0.00517132  0.08216408  0.00814138]]\n",
      "th_dE_hy:\n",
      "[[ 0.43382863]\n",
      " [ 0.43207385]\n",
      " [ 0.36957455]\n",
      " [ 0.41207627]]\n"
     ]
    }
   ],
   "source": [
    "# Theano gradients\n",
    "th_dE_xh, th_dE_hh, th_dE_hy = \\\n",
    "    th_fn(np_x, np_h0, np_y, np_W_xh, np_W_hh, np_W_hy)\n",
    "print \"th_dE_xh:\"\n",
    "print th_dE_xh\n",
    "print \"th_dE_hh:\"\n",
    "print th_dE_hh\n",
    "print \"th_dE_hy:\"\n",
    "print th_dE_hy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np_dE_hy:\n",
      "[[ 0.43382863]\n",
      " [ 0.43207385]\n",
      " [ 0.36957455]\n",
      " [ 0.41207627]]\n",
      "th_dE_hy:\n",
      "[[ 0.43382863]\n",
      " [ 0.43207385]\n",
      " [ 0.36957455]\n",
      " [ 0.41207627]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Numpy gradients\n",
    "# np_h : T  x nh, np_yhat : T x nout\n",
    "np_dE_hy = np.dot(np_h[1:,:].T, \n",
    "          (np_yhat[1:, :] - np_y) * (1 - np_yhat[1:, :] ** 2))\n",
    "print \"np_dE_hy:\"\n",
    "print np_dE_hy\n",
    "print \"th_dE_hy:\"\n",
    "print th_dE_hy\n",
    "print np.allclose(np_dE_hy, th_dE_hy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dE_hh:\n",
      "[[ 0.00029612  0.00626318  0.09047982  0.00933075]\n",
      " [ 0.00028583  0.00608747  0.08944656  0.00915609]\n",
      " [ 0.00025834  0.00546198  0.07707065  0.008035  ]\n",
      " [ 0.00023314  0.00517132  0.08216408  0.00814138]]\n",
      "th_dE_hh:\n",
      "[[ 0.00029612  0.00626318  0.09047982  0.00933075]\n",
      " [ 0.00028583  0.00608747  0.08944656  0.00915609]\n",
      " [ 0.00025834  0.00546198  0.07707065  0.008035  ]\n",
      " [ 0.00023314  0.00517132  0.08216408  0.00814138]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "dE_hh = np.zeros((nh, nh))\n",
    "\n",
    "dhh = np.zeros((T+1, nh))\n",
    "for t in xrange(T,0,-1):\n",
    "    # dE_t/dy_t * dy_t/dh_t\n",
    "    dhh[t] += np.dot(np_W_hy, \n",
    "             (np_yhat[t,:] - np_y[t-1,:]) * (1 - np_yhat[t,:] ** 2))\n",
    "    r = dhh[t] * (1 - np_h[t,:] ** 2)\n",
    "    dhh[t-1] += np.dot(np_W_hh, r)\n",
    "    dE_hh += np.outer(np_h[t-1,:], r)\n",
    "print \"dE_hh:\"\n",
    "print dE_hh\n",
    "print \"th_dE_hh:\"\n",
    "print th_dE_hh\n",
    "print np.allclose(dE_hh, th_dE_hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dE_xh:\n",
      "[[ 0.00010295  0.00259489  0.04856052  0.00426662]]\n",
      "th_dE_xh:\n",
      "[[ 0.00010295  0.00259489  0.04856052  0.00426662]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "dE_xh = np.zeros((nin, nh))\n",
    "\n",
    "dhh = np.zeros((T+1, nh))\n",
    "for t in xrange(T,0,-1):\n",
    "    # dE_t/dy_t * dy_t/dh_t\n",
    "    dhh[t] += np.dot(np_W_hy, \n",
    "             (np_yhat[t,:] - np_y[t-1,:]) * (1 - np_yhat[t,:] ** 2))\n",
    "    r = dhh[t] * (1 - np_h[t,:] ** 2)\n",
    "    dhh[t-1] += np.dot(np_W_hh, r)\n",
    "    #dE_xh += np.outer(np_x[t-1,:], r)\n",
    "    dE_xh += np.reshape(np_x[t-1,:], (np_x.shape[1], 1)) * r.T\n",
    "print \"dE_xh:\"\n",
    "print dE_xh\n",
    "print \"th_dE_xh:\"\n",
    "print th_dE_xh\n",
    "print np.allclose(dE_xh, th_dE_xh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients for RNN with mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of input units\n",
    "nin = 3\n",
    "# Number of hidden units\n",
    "nh = 7\n",
    "# Number of output units\n",
    "nout = 1\n",
    "# Number of batches\n",
    "nbatches = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "th_W_xh = TT.matrix()\n",
    "th_W_hh = TT.matrix()\n",
    "th_W_hy = TT.matrix()\n",
    "\n",
    "th_x = TT.tensor3()\n",
    "th_y = TT.tensor3()\n",
    "th_h0 = TT.matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute gradients using Theano\n",
    "# Hidden state\n",
    "#th_h = TT.tanh(TT.dot(th_x, th_W_xh) + TT.dot(th_h0, th_W_hh))\n",
    "#th_yhat = TT.tanh(TT.dot(th_h, th_W_hy))\n",
    "def step(th_x_t, th_h_tm1, th_W_xh, th_W_hh, th_W_hy):\n",
    "    th_h_t = TT.tanh(TT.dot(th_x_t, th_W_xh) + TT.dot(th_h_tm1, th_W_hh))\n",
    "    th_yhat_t = TT.tanh(TT.dot(th_h_t, th_W_hy))\n",
    "    return th_h_t, th_yhat_t\n",
    "\n",
    "[th_h, th_yhat], _ = theano.scan(step,\n",
    "                                 sequences=th_x,\n",
    "                                 outputs_info=[th_h0, None],\n",
    "                                 non_sequences=[th_W_xh, th_W_hh, \n",
    "                                                th_W_hy])\n",
    "\n",
    "th_error = 0.5 * ((th_yhat - th_y) ** 2).sum().sum()\n",
    "# Gradients using Cost function\n",
    "th_gW_xh, th_gW_hh, th_gW_hy = \\\n",
    "                    TT.grad(th_error, [th_W_xh, th_W_hh, th_W_hy])\n",
    "th_fn = theano.function(\n",
    "        [th_x, th_h0, th_y, th_W_xh, th_W_hh, th_W_hy],\n",
    "        [th_gW_xh, th_gW_hh, th_gW_hy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Numerical examples\n",
    "T = 10\n",
    "np_h0 = np.random.uniform(size=(nbatches, nh))\n",
    "np_x = np.random.uniform(size=(T, nbatches, nin))\n",
    "np_y = np.random.uniform(size=(T, nbatches, nout))\n",
    "\n",
    "np_W_xh = np.random.uniform(size=(nin, nh))\n",
    "np_W_hh = np.random.uniform(size=(nh, nh))\n",
    "np_W_hy = np.random.uniform(size=(nh, nout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th_dE_xh:\n",
      "[[ 0.0039954   0.00416409  0.00063727  0.00010572  0.00279952  0.01228623\n",
      "   0.01305023]\n",
      " [ 0.012443    0.0111805   0.00122589  0.00054987  0.00638779  0.02178284\n",
      "   0.01558321]\n",
      " [ 0.00693897  0.00648802  0.00094858  0.00025519  0.00413016  0.01322138\n",
      "   0.01588981]]\n",
      "th_dE_hh:\n",
      "[[  6.63339083e-03   8.55404530e-03   1.31970141e-03   1.96182336e-04\n",
      "    5.42491988e-03   1.65474948e-02   2.34291788e-02]\n",
      " [  3.69880987e-03   5.96005918e-03   1.03906158e-03   8.01001790e-05\n",
      "    4.76751013e-03   1.10643960e-02   1.99483112e-02]\n",
      " [  6.01472254e-03   8.13967509e-03   1.28406162e-03   1.71797797e-04\n",
      "    5.50544715e-03   1.54187719e-02   2.25018397e-02]\n",
      " [  3.66507301e-03   5.96895792e-03   1.07327038e-03   6.25557291e-05\n",
      "    4.87360108e-03   1.05508715e-02   2.10190710e-02]\n",
      " [  6.35283641e-03   8.56755202e-03   1.32520573e-03   1.98669478e-04\n",
      "    5.92387373e-03   1.47818460e-02   2.14457156e-02]\n",
      " [  1.00808944e-02   1.07176140e-02   1.38939915e-03   4.23266838e-04\n",
      "    6.56216960e-03   1.87813611e-02   2.09511603e-02]\n",
      " [  8.01225237e-03   9.30569074e-03   1.23629506e-03   3.24521142e-04\n",
      "    6.13618933e-03   1.56907242e-02   1.92055574e-02]]\n",
      "th_dE_hy:\n",
      "[[ 6.27329127]\n",
      " [ 6.23557627]\n",
      " [ 6.25969545]\n",
      " [ 6.28069559]\n",
      " [ 6.19100554]\n",
      " [ 6.26275851]\n",
      " [ 6.25058935]]\n"
     ]
    }
   ],
   "source": [
    "# Theano gradients\n",
    "th_dE_xh, th_dE_hh, th_dE_hy = \\\n",
    "    th_fn(np_x, np_h0, np_y, np_W_xh, np_W_hh, np_W_hy)\n",
    "print \"th_dE_xh:\"\n",
    "print th_dE_xh\n",
    "print \"th_dE_hh:\"\n",
    "print th_dE_hh\n",
    "print \"th_dE_hy:\"\n",
    "print th_dE_hy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Compute gradients using Numpy\n",
    "# Forward propagation\n",
    "def forwardPropagationBatch(np_x, np_h0, W_xh, W_hh, W_hy):\n",
    "    h_tm1 = np_h0\n",
    "    T = np_x.shape[0]\n",
    "    nin, nh = W_xh.shape\n",
    "    nout = W_hy.shape[1]\n",
    "    np_h = np.zeros((T+1, nbatches, nh))\n",
    "    np_h[0,:, :] = np_h0\n",
    "    np_yhat = np.zeros((T+1, nbatches, nout))\n",
    "    for t in xrange(1, T+1):\n",
    "        h_t = np.tanh(np.dot(np_x[t-1], W_xh) + np.dot(h_tm1, W_hh))\n",
    "        np_h[t, :, :] = h_t\n",
    "        h_tm1 = h_t\n",
    "        yhat = np.tanh(np.dot(h_t, W_hy))\n",
    "        np_yhat[t, :, :] = yhat\n",
    "    return np_h, np_yhat\n",
    "    \n",
    "np_h, np_yhat = forwardPropagationBatch(np_x, np_h0, \n",
    "                                   np_W_xh, np_W_hh, np_W_hy)\n",
    "loss = 0.5 * ((np_yhat[1:,:,:] - np_y)**2).sum().sum()\n",
    "\"\"\"\n",
    "print \"np_h:\"\n",
    "print np_h\n",
    "print \"np_yhat:\"\n",
    "print np_yhat\n",
    "\"\"\"\n",
    "snp_h = np.zeros((T+1, nbatches, nh))\n",
    "snp_yhat = np.zeros((T+1, nbatches, nout))\n",
    "for nb in xrange(nbatches):\n",
    "    cur_h, cur_yhat = \\\n",
    "    forwardPropagation(np_x[:, nb, :], np_h0[nb, :], \n",
    "                       np_W_xh, np_W_hh, np_W_hy)\n",
    "    snp_h[:, nb, :] = cur_h\n",
    "    snp_yhat[:, nb, :] = cur_yhat\n",
    "\"\"\"\n",
    "print \"snp_h:\"\n",
    "print snp_h\n",
    "print \"snp_yhat:\"\n",
    "print snp_yhat\n",
    "\"\"\"\n",
    "print np.allclose(np_h, snp_h)\n",
    "print np.allclose(np_yhat, snp_yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10, 7)\n",
      "(10, 10, 1)\n",
      "np_dE_hy:\n",
      "[[ 6.27329127]\n",
      " [ 6.23557627]\n",
      " [ 6.25969545]\n",
      " [ 6.28069559]\n",
      " [ 6.19100554]\n",
      " [ 6.26275851]\n",
      " [ 6.25058935]]\n",
      "th_dE_hy:\n",
      "[[ 6.27329127]\n",
      " [ 6.23557627]\n",
      " [ 6.25969545]\n",
      " [ 6.28069559]\n",
      " [ 6.19100554]\n",
      " [ 6.26275851]\n",
      " [ 6.25058935]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Numpy gradients\n",
    "# np_h : T  x nh, np_yhat : T x nout\n",
    "\n",
    "h1 = np_h[1:,:,:]\n",
    "print h1.shape\n",
    "y1 = (np_yhat[1:, :, :] - np_y) * (1 - np_yhat[1:, :, :] ** 2)\n",
    "print y1.shape\n",
    "np_dE_hy = np.tensordot(h1, y1, axes=([0, 1], [0, 1]))\n",
    "print \"np_dE_hy:\"\n",
    "print np_dE_hy\n",
    "print \"th_dE_hy:\"\n",
    "print th_dE_hy\n",
    "print np.allclose(np_dE_hy, th_dE_hy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dE_hh:\n",
      "[[  6.63339083e-03   8.55404530e-03   1.31970141e-03   1.96182336e-04\n",
      "    5.42491988e-03   1.65474948e-02   2.34291788e-02]\n",
      " [  3.69880987e-03   5.96005918e-03   1.03906158e-03   8.01001790e-05\n",
      "    4.76751013e-03   1.10643960e-02   1.99483112e-02]\n",
      " [  6.01472254e-03   8.13967509e-03   1.28406162e-03   1.71797797e-04\n",
      "    5.50544715e-03   1.54187719e-02   2.25018397e-02]\n",
      " [  3.66507301e-03   5.96895792e-03   1.07327038e-03   6.25557291e-05\n",
      "    4.87360108e-03   1.05508715e-02   2.10190710e-02]\n",
      " [  6.35283641e-03   8.56755202e-03   1.32520573e-03   1.98669478e-04\n",
      "    5.92387373e-03   1.47818460e-02   2.14457156e-02]\n",
      " [  1.00808944e-02   1.07176140e-02   1.38939915e-03   4.23266838e-04\n",
      "    6.56216960e-03   1.87813611e-02   2.09511603e-02]\n",
      " [  8.01225237e-03   9.30569074e-03   1.23629506e-03   3.24521142e-04\n",
      "    6.13618933e-03   1.56907242e-02   1.92055574e-02]]\n",
      "th_dE_hh:\n",
      "[[  6.63339083e-03   8.55404530e-03   1.31970141e-03   1.96182336e-04\n",
      "    5.42491988e-03   1.65474948e-02   2.34291788e-02]\n",
      " [  3.69880987e-03   5.96005918e-03   1.03906158e-03   8.01001790e-05\n",
      "    4.76751013e-03   1.10643960e-02   1.99483112e-02]\n",
      " [  6.01472254e-03   8.13967509e-03   1.28406162e-03   1.71797797e-04\n",
      "    5.50544715e-03   1.54187719e-02   2.25018397e-02]\n",
      " [  3.66507301e-03   5.96895792e-03   1.07327038e-03   6.25557291e-05\n",
      "    4.87360108e-03   1.05508715e-02   2.10190710e-02]\n",
      " [  6.35283641e-03   8.56755202e-03   1.32520573e-03   1.98669478e-04\n",
      "    5.92387373e-03   1.47818460e-02   2.14457156e-02]\n",
      " [  1.00808944e-02   1.07176140e-02   1.38939915e-03   4.23266838e-04\n",
      "    6.56216960e-03   1.87813611e-02   2.09511603e-02]\n",
      " [  8.01225237e-03   9.30569074e-03   1.23629506e-03   3.24521142e-04\n",
      "    6.13618933e-03   1.56907242e-02   1.92055574e-02]]\n",
      "True\n",
      "dE_hh:\n",
      "[[  6.63339083e-03   8.55404530e-03   1.31970141e-03   1.96182336e-04\n",
      "    5.42491988e-03   1.65474948e-02   2.34291788e-02]\n",
      " [  3.69880987e-03   5.96005918e-03   1.03906158e-03   8.01001790e-05\n",
      "    4.76751013e-03   1.10643960e-02   1.99483112e-02]\n",
      " [  6.01472254e-03   8.13967509e-03   1.28406162e-03   1.71797797e-04\n",
      "    5.50544715e-03   1.54187719e-02   2.25018397e-02]\n",
      " [  3.66507301e-03   5.96895792e-03   1.07327038e-03   6.25557291e-05\n",
      "    4.87360108e-03   1.05508715e-02   2.10190710e-02]\n",
      " [  6.35283641e-03   8.56755202e-03   1.32520573e-03   1.98669478e-04\n",
      "    5.92387373e-03   1.47818460e-02   2.14457156e-02]\n",
      " [  1.00808944e-02   1.07176140e-02   1.38939915e-03   4.23266838e-04\n",
      "    6.56216960e-03   1.87813611e-02   2.09511603e-02]\n",
      " [  8.01225237e-03   9.30569074e-03   1.23629506e-03   3.24521142e-04\n",
      "    6.13618933e-03   1.56907242e-02   1.92055574e-02]]\n",
      "th_dE_hh:\n",
      "[[  6.63339083e-03   8.55404530e-03   1.31970141e-03   1.96182336e-04\n",
      "    5.42491988e-03   1.65474948e-02   2.34291788e-02]\n",
      " [  3.69880987e-03   5.96005918e-03   1.03906158e-03   8.01001790e-05\n",
      "    4.76751013e-03   1.10643960e-02   1.99483112e-02]\n",
      " [  6.01472254e-03   8.13967509e-03   1.28406162e-03   1.71797797e-04\n",
      "    5.50544715e-03   1.54187719e-02   2.25018397e-02]\n",
      " [  3.66507301e-03   5.96895792e-03   1.07327038e-03   6.25557291e-05\n",
      "    4.87360108e-03   1.05508715e-02   2.10190710e-02]\n",
      " [  6.35283641e-03   8.56755202e-03   1.32520573e-03   1.98669478e-04\n",
      "    5.92387373e-03   1.47818460e-02   2.14457156e-02]\n",
      " [  1.00808944e-02   1.07176140e-02   1.38939915e-03   4.23266838e-04\n",
      "    6.56216960e-03   1.87813611e-02   2.09511603e-02]\n",
      " [  8.01225237e-03   9.30569074e-03   1.23629506e-03   3.24521142e-04\n",
      "    6.13618933e-03   1.56907242e-02   1.92055574e-02]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "dE_hh = np.zeros((nh, nh))\n",
    "\n",
    "for nb in xrange(nbatches):\n",
    "    dhh = np.zeros((T+1, nh))\n",
    "    for t in xrange(T,0,-1):\n",
    "        # dE_t/dy_t * dy_t/dh_t\n",
    "        dhh[t] += np.dot(np_W_hy, \n",
    "                 (np_yhat[t,nb,:] - np_y[t-1,nb,:]) * (1 - np_yhat[t,nb,:] ** 2))\n",
    "        r = dhh[t] * (1 - np_h[t,nb,:] ** 2)\n",
    "        dhh[t-1] += np.dot(np_W_hh, r)\n",
    "        #dE_hh += np.outer(np_h[t-1,nb,:], r)\n",
    "        dE_hh += np.reshape(np_h[t - 1, nb, :], (nh, 1)) * r\n",
    "        \n",
    "print \"dE_hh:\"\n",
    "print dE_hh\n",
    "print \"th_dE_hh:\"\n",
    "print th_dE_hh\n",
    "print np.allclose(dE_hh, th_dE_hh)\n",
    "\n",
    "dE_hh = np.zeros((nh, nh))\n",
    "dhh = np.zeros((T+1, nbatches, nh))\n",
    "for t in xrange(T, 0, -1):\n",
    "    for nb in xrange(nbatches):\n",
    "        dhh[t, nb, :] += \\\n",
    "            np.dot(np_W_hy,\n",
    "            (np_yhat[t,nb,:] - np_y[t-1,nb,:]) * (1 - np_yhat[t,nb,:] ** 2))\n",
    "        r = dhh[t, nb, :] * (1 - np_h[t, nb, :] ** 2)\n",
    "        dhh[t-1, nb, :] += np.dot(np_W_hh, r)\n",
    "        dE_hh += np.reshape(np_h[t-1, nb, :], (nh,1)) * r\n",
    "\n",
    "print \"dE_hh:\"\n",
    "print dE_hh\n",
    "print \"th_dE_hh:\"\n",
    "print th_dE_hh\n",
    "print np.allclose(dE_hh, th_dE_hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dE_hh:\n",
      "[[  6.63339083e-03   8.55404530e-03   1.31970141e-03   1.96182336e-04\n",
      "    5.42491988e-03   1.65474948e-02   2.34291788e-02]\n",
      " [  3.69880987e-03   5.96005918e-03   1.03906158e-03   8.01001790e-05\n",
      "    4.76751013e-03   1.10643960e-02   1.99483112e-02]\n",
      " [  6.01472254e-03   8.13967509e-03   1.28406162e-03   1.71797797e-04\n",
      "    5.50544715e-03   1.54187719e-02   2.25018397e-02]\n",
      " [  3.66507301e-03   5.96895792e-03   1.07327038e-03   6.25557291e-05\n",
      "    4.87360108e-03   1.05508715e-02   2.10190710e-02]\n",
      " [  6.35283641e-03   8.56755202e-03   1.32520573e-03   1.98669478e-04\n",
      "    5.92387373e-03   1.47818460e-02   2.14457156e-02]\n",
      " [  1.00808944e-02   1.07176140e-02   1.38939915e-03   4.23266838e-04\n",
      "    6.56216960e-03   1.87813611e-02   2.09511603e-02]\n",
      " [  8.01225237e-03   9.30569074e-03   1.23629506e-03   3.24521142e-04\n",
      "    6.13618933e-03   1.56907242e-02   1.92055574e-02]]\n",
      "th_dE_hh:\n",
      "[[  6.63339083e-03   8.55404530e-03   1.31970141e-03   1.96182336e-04\n",
      "    5.42491988e-03   1.65474948e-02   2.34291788e-02]\n",
      " [  3.69880987e-03   5.96005918e-03   1.03906158e-03   8.01001790e-05\n",
      "    4.76751013e-03   1.10643960e-02   1.99483112e-02]\n",
      " [  6.01472254e-03   8.13967509e-03   1.28406162e-03   1.71797797e-04\n",
      "    5.50544715e-03   1.54187719e-02   2.25018397e-02]\n",
      " [  3.66507301e-03   5.96895792e-03   1.07327038e-03   6.25557291e-05\n",
      "    4.87360108e-03   1.05508715e-02   2.10190710e-02]\n",
      " [  6.35283641e-03   8.56755202e-03   1.32520573e-03   1.98669478e-04\n",
      "    5.92387373e-03   1.47818460e-02   2.14457156e-02]\n",
      " [  1.00808944e-02   1.07176140e-02   1.38939915e-03   4.23266838e-04\n",
      "    6.56216960e-03   1.87813611e-02   2.09511603e-02]\n",
      " [  8.01225237e-03   9.30569074e-03   1.23629506e-03   3.24521142e-04\n",
      "    6.13618933e-03   1.56907242e-02   1.92055574e-02]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "dE_hh = np.zeros((nh, nh))\n",
    "dhh = np.zeros((T+1, nbatches, nh))\n",
    "\n",
    "for t in xrange(T,0,-1):\n",
    "    # dE_t/dy_t * dy_t/dh_t\n",
    "    m = (np_yhat[t, :, :] - np_y[t-1, :, :]) * (1 - np_yhat[t, :, :] ** 2)\n",
    "    dhh[t, :, :] += np.dot(m, np_W_hy.T)\n",
    "    r = dhh[t, :, :] * (1 - np_h[t, :, :] ** 2)\n",
    "    \n",
    "    dhh[t-1, :, :] += np.dot(r, np_W_hh.T)\n",
    "    dE_hh += np.dot(np_h[t-1, :, :].T, r)\n",
    "    \n",
    "print \"dE_hh:\"\n",
    "print dE_hh\n",
    "print \"th_dE_hh:\"\n",
    "print th_dE_hh\n",
    "print np.allclose(dE_hh, th_dE_hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dE_xh:\n",
      "[[ 0.0039954   0.00416409  0.00063727  0.00010572  0.00279952  0.01228623\n",
      "   0.01305023]\n",
      " [ 0.012443    0.0111805   0.00122589  0.00054987  0.00638779  0.02178284\n",
      "   0.01558321]\n",
      " [ 0.00693897  0.00648802  0.00094858  0.00025519  0.00413016  0.01322138\n",
      "   0.01588981]]\n",
      "th_dE_xh:\n",
      "[[ 0.0039954   0.00416409  0.00063727  0.00010572  0.00279952  0.01228623\n",
      "   0.01305023]\n",
      " [ 0.012443    0.0111805   0.00122589  0.00054987  0.00638779  0.02178284\n",
      "   0.01558321]\n",
      " [ 0.00693897  0.00648802  0.00094858  0.00025519  0.00413016  0.01322138\n",
      "   0.01588981]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "dE_xh = np.zeros((nin, nh))\n",
    "dhh = np.zeros((T+1, nbatches, nh))\n",
    "\n",
    "for t in xrange(T,0,-1):\n",
    "    # dE_t/dy_t * dy_t/dh_t\n",
    "    m = (np_yhat[t, :, :] - np_y[t-1, :, :]) * (1 - np_yhat[t, :, :] ** 2)\n",
    "    dhh[t, :, :] += np.dot(m, np_W_hy.T)\n",
    "    r = dhh[t, :, :] * (1 - np_h[t, :, :] ** 2)\n",
    "    \n",
    "    dhh[t-1, :, :] += np.dot(r, np_W_hh.T)\n",
    "    dE_xh += np.dot(np_x[t-1, :, :].T, r)\n",
    "    \n",
    "print \"dE_xh:\"\n",
    "print dE_xh\n",
    "print \"th_dE_xh:\"\n",
    "print th_dE_xh\n",
    "print np.allclose(dE_xh, th_dE_xh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
